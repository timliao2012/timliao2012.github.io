<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>最大公约数（辗转相除法）</title>
    <url>/2017/04/09/2017-04-09-%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0%EF%BC%88%E8%BE%97%E8%BD%AC%E7%9B%B8%E9%99%A4%E6%B3%95%EF%BC%89/</url>
    <content><![CDATA[<p>辗转相除法，又称欧几里得算法，是求最大公约数(Greatest Common
Divisor)的算法。</p>
<span id="more"></span>
<h1 id="算法描述">算法描述</h1>
<p>设两数为<span class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span> <span
class="math inline">\((a&gt;b)\)</span>，求<span
class="math inline">\(a\)</span>和<span
class="math inline">\(b\)</span>最大公约数<span
class="math inline">\(gcd(a，b)\)</span>的步骤如下：</p>
<ol type="1">
<li>用<span class="math inline">\(b\)</span>除<span
class="math inline">\(a\)</span>，得<span
class="math inline">\(a÷b=q......r(0\leq r)\)</span>。</li>
<li>若<span class="math inline">\(r=0\)</span>，则<span
class="math inline">\(gcd(a,b)=b\)</span>；结束。</li>
<li>若<span class="math inline">\(r≠0\)</span>，取<span
class="math inline">\(a=b,b=r\)</span>，执行第1步。</li>
</ol>
<h1 id="原理证明">原理证明</h1>
<p>设两数为<span class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span> <span class="math inline">\((b\leq
a)\)</span>，用<span class="math inline">\(gcd(a,b)\)</span>表示<span
class="math inline">\(a\)</span>，<span
class="math inline">\(b\)</span>的最大公约数，$r=a mod b <span
class="math inline">\(为\)</span>a<span
class="math inline">\(除以\)</span>b<span
class="math inline">\(以后的余数，\)</span>k<span
class="math inline">\(为\)</span>a<span
class="math inline">\(除以\)</span>b<span
class="math inline">\(的商，即\)</span>a÷b=k.......r$。</p>
<p>辗转相除法即是要证明<span
class="math inline">\(gcd(a,b)=gcd(b,r)\)</span>。</p>
<ol type="1">
<li>令<span class="math inline">\(c=gcd(a,b)\)</span>，则设<span
class="math inline">\(a=mc\)</span>，<span
class="math inline">\(b=nc\)</span></li>
<li>则<span class="math inline">\(r =a-kb=mc-knc=(m-kn)c\)</span></li>
<li>即<span class="math inline">\(c\)</span>也是<span
class="math inline">\(r\)</span>的因数</li>
<li>可以断定<span class="math inline">\(m-kn\)</span>与<span
class="math inline">\(n\)</span>互素【否则，可设<span
class="math inline">\(m-kn=xd\)</span>，<span
class="math inline">\(n=yd\)</span>，<span
class="math inline">\((d&gt;1)\)</span>，则<span
class="math inline">\(m=kn+xd=kyd+xd=(ky+x)d\)</span>，则<span
class="math inline">\(a=mc=(ky+x)dc\)</span>，<span
class="math inline">\(b=nc=ycd\)</span>，故<span
class="math inline">\(a\)</span>与<span
class="math inline">\(b\)</span>最大公约数成为<span
class="math inline">\(cd\)</span>，而非<span
class="math inline">\(c\)</span>，与前面结论矛盾】</li>
</ol>
<p>从而可知<span class="math inline">\(gcd(b,r)=c\)</span>，继而<span
class="math inline">\(gcd(a,b)=gcd(b,r)\)</span>。</p>
<h1 id="算法实现c">算法实现(c++)</h1>
<p>递归方式： <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(b == <span class="number">0</span>)</span><br><span class="line">    	<span class="keyword">return</span> b;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    	<span class="keyword">return</span> <span class="built_in">gcd</span>(b, a % b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>迭代方式： <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(b != <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> r = a % b;</span><br><span class="line">        a = b;</span><br><span class="line">        b = r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>gcd</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 2.7.3 安装</title>
    <url>/2017/04/14/2017-04-14-Hadoop-2-7-3-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>在此记录hadoop 2.7.3版本的安装过程以及基本配置过程。</p>
<span id="more"></span>
<h1 id="安装环境">安装环境</h1>
<ol type="1">
<li>jdk1.8</li>
<li>hadoop 2.7.3</li>
<li>CentOS release 6.7 (Final) * 3</li>
</ol>
<table>
<thead>
<tr class="header">
<th>hostname</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>master</td>
<td>172.168.170.84</td>
</tr>
<tr class="even">
<td>slave1</td>
<td>172.168.170.88</td>
</tr>
<tr class="odd">
<td>slave2</td>
<td>172.168.170.89</td>
</tr>
</tbody>
</table>
<h1 id="必需软件">必需软件</h1>
<ol type="1">
<li>JDK安装(<a
href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">下载地址</a>)</li>
<li>ssh安装
hadoop中使用ssh来实现cluster中各个node的登陆认证，同时需要进行ssh免密登陆。
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure></li>
<li>rsync安装 Ubuntu 12.10已经自带rsync。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install rsync</span><br></pre></td></tr></table></figure></li>
<li>hadoop下载 从官方<a
href="http://www.apache.org/mirrors/#cn">mirrors</a>下载对应版本的hadoop。</li>
</ol>
<h1 id="安装hadoop">安装Hadoop</h1>
<ol type="1">
<li>创建hadoop用户组以及用户 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo addgroup hadoop</span><br><span class="line">sudo adduser --ingroup hadoop hadoop</span><br></pre></td></tr></table></figure>
重新用hadoop用户登陆到Linux中。</li>
<li>将hadoop解压到目录<code>/home/hadoop/local/opt</code>中</li>
<li>配置hadoop环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HOME</span>/<span class="built_in">local</span>/opt/hadoop-2.7.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></li>
<li>进入<code>hadoop-2.7.3/etc/hadoop</code>文件夹修改<code>core-site.xml</code>文件
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/local/var/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改<code>hdfs-site.xml</code>文件 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span># 通过web界面来查看HDFS状态 <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/local/var/hadoop/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/local/var/hadoop/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span># 每个Block有1个备份<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改<code>mapred-site.xml</code>
这个是mapreduce任务的配置，由于hadoop2.x使用了yarn框架，所以要实现分布式部署，必须在<code>mapreduce.framework.name</code>属性下配置为yarn。<code>mapred.map.tasks</code>和<code>mapred.reduce.tasks</code>分别为map和reduce的任务数。
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改<code>yarn-site.xml</code> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改<code>slaves</code>文件 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></li>
<li>修改<code>hosts</code>文件，命名各个节点的名称。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">172.168.170.84 master</span><br><span class="line">172.168.170.88 slave1</span><br><span class="line">172.168.170.89 slave2</span><br></pre></td></tr></table></figure></li>
<li>节点之间ssh免密登陆
在<code>master</code>节点中生成密钥，并添加到<code>.ssh/authorized_keys</code>文件中。
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat id_rsa.pub&gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure>
将<code>master</code>中的<code>/etc/hosts</code>文件和<code>.ssh/authorized_keys</code>文件发送到slave1和slave2文件中。
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp /etc/hosts hadoop@slave1:/etc/hosts</span><br><span class="line">scp /home/hadoop/.ssh/authorized_keys hadoop@slave1:/home/hadoop/.ssh/authorized_keys</span><br><span class="line">scp /home/hadoop/.ssh/authorized_keys hadoop@slave2:/home/hadoop/.ssh/authorized_keys</span><br></pre></td></tr></table></figure> 完成之后可以利用以下语句测试免密登陆。
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh slave1</span><br><span class="line">ssh slave2</span><br></pre></td></tr></table></figure></li>
<li>将<code>hadoop-2.7.3</code>文件拷贝至slave1和slave2
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -r /home/hadoop/<span class="built_in">local</span>/opt/hadoop-2.7.3 hadoop@slave1:/home/hadoop/<span class="built_in">local</span>/opt/</span><br><span class="line">scp -r /home/hadoop/<span class="built_in">local</span>/opt/hadoop-2.7.3 hadoop@slave2:/home/hadoop/<span class="built_in">local</span>/opt/</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="启动hadoop">启动Hadoop</h1>
<ol type="1">
<li>在master节点使用hadoop用户初始化NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode –format</span><br><span class="line"><span class="comment">#执行后控制台输出，看到 Exiting with status 0 表示格式化成功。</span></span><br><span class="line"><span class="comment">#如有错误，先删除var目录下的临时文件，然后重新运行该命令</span></span><br></pre></td></tr></table></figure></li>
<li>启动hadoop <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#启动hdfs</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="comment">#启动yarn分布式计算框架</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>用jps命令查看hadoop集群运行情况 master节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Jps</span><br><span class="line">NameNode</span><br><span class="line">ResourceManager</span><br><span class="line">SecondaryNameNode</span><br><span class="line">JobHistoryServer</span><br></pre></td></tr></table></figure> slave节点
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Jps</span><br><span class="line">DataNode</span><br><span class="line">NodeManager</span><br></pre></td></tr></table></figure></li>
<li>通过以下网址查看集群状态 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://172.168.170.84:50070</span><br><span class="line">http://172.168.170.84:8088</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之HDFS架构</title>
    <url>/2017/04/14/2017-04-14-Hadoop%E4%B9%8BHDFS%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<p>HDFS即Hadoop Distributed File
System分布式文件系统，它的设计目标是把超大数据集存储到分布在网络中的多台普通商用计算机上，并且能够提供高可靠性和高吞吐量的服务。</p>
<span id="more"></span>
<h1 id="hdfs架构原理">HDFS架构原理</h1>
<p>HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode
运行在master节点，DataNode运行slave节点。</p>
<h2 id="数据块">数据块</h2>
<p>磁盘数据块是磁盘读写的基本单位，与普通文件系统类似，hdfs也会把文件分块来存储。hdfs默认数据块大小为64MB，磁盘块一般为512B，HDFS块为何如此之大呢？块增大可以减少寻址时间与文件传输时间的比例，若寻址时间为10ms，磁盘传输速率为100MB/s，那么寻址与传输比仅为1%。当然，磁盘块太大也不好，因为一个MapReduce通常以一个块作为输入，块过大会导致整体任务数量过小，降低作业处理速度。</p>
<p>hdfs按块存储还有如下好处：</p>
<ol type="1">
<li>文件可以任意大，也不用担心单个结点磁盘容量小于文件的情况</li>
<li>简化了文件子系统的设计，子系统只存储文件块数据，而文件元数据则交由其它系统（NameNode）管理</li>
<li>有利于备份和提高系统可用性，因为可以以块为单位进行备份，hdfs默认备份数量为3。</li>
<li>有利于负载均衡</li>
</ol>
<h2 id="namenode">NameNode</h2>
<h3 id="namenode中的元信息">NameNode中的元信息</h3>
<p>当一个客户端请求一个文件或者存储一个文件时，它需要先知道具体到哪个DataNode上存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。</p>
<p>NameNode管理着文件系统命名空间，它维护这文件系统树及树中的所有文件和目录。NameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件数据的保存与操作，都是由DataNode负责。当一个客户端请求数据时，它仅仅是从NameNode中获取文件的元信息，而具体的数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互。</p>
<p>NameNode保存元信息的种类有：</p>
<ul>
<li>文件名目录名及它们之间的层级关系</li>
<li>文件目录的所有者及其权限</li>
<li>每个文件块的名及文件有哪些块组成</li>
</ul>
<p>需要注意的是，<strong>NameNode元信息并不包含每个块的位置信息</strong>，这些信息会在NameNode启动时从各个DataNode获取并保存在内存中，因为这些信息会在系统启动时由数据节点重建。把块位置信息放在内存中，在读取数据时会减少查询时间，增加读取效率。</p>
<h3 id="元信息的持久化">元信息的持久化</h3>
<p>在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个文件edits中。并且edits文件和fsimage文件会被SecondaryNameNode周期性的合并（合并过程会在SecondaryNameNode中详细介绍）。</p>
<h3 id="其它问题">其它问题</h3>
<p>运行NameNode会占用大量内存和I/O资源，一般NameNode不会存储用户数据或执行MapReduce任务。</p>
<p>为了简化系统的设计，Hadoop只有一个NameNode，这也就导致了hadoop集群的单点故障问题。因此，对NameNode节点的容错尤其重要，hadoop提供了如下两种机制来解决：</p>
<ul>
<li>将hadoop元数据写入到本地文件系统的同时再实时同步到一个远程挂载的网络文件系统（NFS）。</li>
<li>运行一个secondaryNameNode，它的作用是与NameNode进行交互，定期通过编辑日志文件合并命名空间镜像，当NameNode发生故障时它会通过自己合并的命名空间镜像副本来恢复。需要注意的是secondaryNameNode保存的状态总是滞后于NameNode，所以这种方式难免会导致丢失部分数据（后面会详细介绍）。</li>
</ul>
<h2 id="secondarynamenode">SecondaryNameNode</h2>
<p>需要注意，SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS文件的元信息都保存在NameNode的内存中。在NameNode启动时，它首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在了内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。</p>
<p>Edits文件存在的目的是为了提高系统的操作效率，NameNode在更新内存中的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题而诞生的。</p>
<p>SecondaryNameNode的角色就是定期的合并edits和fsimage文件，我们来看一下合并的步骤：</p>
<figure>
<img src="/imgs/Hadoop/hdfs/copyfsimage.jpg"
alt="Secondary NameNode处理流程" />
<figcaption aria-hidden="true">Secondary NameNode处理流程</figcaption>
</figure>
<ol type="1">
<li>合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</li>
<li>SecondaryNameNode从NameNode请求fsimage和edits文件</li>
<li>SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件</li>
<li>NameNode从SecondaryNameNode获取合并好的新的fsimage并将旧的替换掉，并把edits用第一步创建的edits.new文件替换掉</li>
<li>更新fstime文件中的检查点</li>
</ol>
<p>最后再总结一下整个过程中涉及到NameNode中的相关文件</p>
<ul>
<li>fsimage ：保存的是上个检查点的HDFS的元信息</li>
<li>edits ：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</li>
<li>fstime：保存了最后一个检查点的时间戳</li>
</ul>
<h2 id="datanode">DataNode</h2>
<p>DataNode是HDFS中的worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除、和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。当对hdfs文件系统进行读写时，NameNode告知客户端每个数据驻留在哪个DataNode，客户端直接与DataNode进行通信，DataNode还会与其它DataNode通信，复制这些块以实现冗余。</p>
<h2 id="数据备份">数据备份</h2>
<p>HDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其它所有数据块大小都是一样的。NameNode负责各个数据块的备份，DataNode会通过心跳的方式定期的向NameNode发送自己节点上的Block报告，这个报告中包含了DataNode节点上的所有数据块的列表。</p>
<p>一个大型的HDFS文件系统一般都是需要跨很多机架的，不同机架之间的数据传输需要经过网关，并且，同一个机架中机器之间的带宽要大于不同机架机器之间的带宽。如果把所有的副本都放在不同的机架中，这样既可以防止机架失败导致数据块不可用，又可以在读数据时利用到多个机架的带宽，并且也可以很容易的实现负载均衡。但是，如果是写数据，各个数据块需要同步到不同的机架，会影响到写数据的效率。而在Hadoop中，如果副本数量是3的情况下，Hadoop默认是这么存放的，把第一个副本放到机架的一个节点上，另一个副本放到同一个机架的另一个节点上，把最后一个节点放到不同的机架上。这种策略减少了跨机架副本的个数提高了写的性能，也能够允许一个机架失败的情况，算是一个很好的权衡。</p>
<figure>
<img src="/imgs/Hadoop/hdfs/copylocation.jpg" alt="副本摆放策略" />
<figcaption aria-hidden="true">副本摆放策略</figcaption>
</figure>
<h2 id="安全模式">安全模式</h2>
<p>关于安全模式，当
Hadoop的NameNode节点启动时，会进入安全模式阶段。在此阶段，DataNode会向NameNode上传它们数据块的列表，让
NameNode得到块的位置信息，并对每个文件对应的数据块副本进行统计。当最小副本条件满足时，即一定比例的数据块都达到最小副本数，系统就会退出安全模式，而这需要一定的延迟时间。当最小副本条件未达到要求时，就会对副本数不足的数据块安排DataNode进行复制，直至达到最小副本数。而在安全模式下，系统会处于只读状态，NameNode不会处理任何块的复制和删除命令。</p>
<h2 id="hdfs负载均衡">HDFS负载均衡</h2>
<p>HDFS的数据也许并不是非常均匀的分布在各个DataNode中。一个常见的原因是在现有的集群上经常会增添新的DataNode节点。当新增一个数据块（一个文件的数据被保存在一系列的块中）时，NameNode在选择DataNode接收这个数据块之前，会考虑到很多因素。其中的一些考虑的是：</p>
<ul>
<li>将数据块的一个副本放在正在写这个数据块的节点上。</li>
<li>尽量将数据块的不同副本分布在不同的机架上，这样集群可在完全失去某一机架的情况下还能存活。</li>
<li>一个副本通常被放置在和写文件的节点同一机架的某个节点上，这样可以减少跨越机架的网络I/O。</li>
<li>尽量均匀地将HDFS数据分布在集群的DataNode中。</li>
</ul>
<h1 id="hdfs健壮性">HDFS健壮性</h1>
<p>HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：
Namenode出错 , Datanode出错和网络割裂(network partitions)。</p>
<h2
id="磁盘数据错误心跳检测和重新复制">磁盘数据错误，心跳检测和重新复制</h2>
<p>每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟
Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号
Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。</p>
<h2 id="数据完整性">数据完整性</h2>
<p>从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode
获取该数据块的副本。</p>
<h2 id="元数据磁盘错误">元数据磁盘错误</h2>
<p>FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS
实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事
务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当
Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。</p>
<p>Namenode是HDFS集群中的单点故障(single point of
failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。</p>
<h1 id="hdfs网络">HDFS网络</h1>
<h2 id="hdfs中的沟通协议">HDFS中的沟通协议</h2>
<p>所有的HDFS中的沟通协议都是基于tcp/ip协议，一个客户端通过指定的tcp端口与NameNode机器建立连接，并通过ClientProtocol协议与NameNode交互。而DataNode则通过DataNode
Protocol协议与NameNode进行沟通。HDFS的RCP(远程过程调用)对ClientProtocol和DataNode
Protocol做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。</p>
<h2 id="hdfs机架感知与网络拓扑">HDFS机架感知与网络拓扑</h2>
<p>通常，大型Hadoop集群是以机架的形式来组织的，而HDFS不能够自动判断集群中各个datanode的网络拓扑情况，因此Hadoop允许集群的管理员通过配置<code>dfs.network.script</code>参数来确定节点所处的机架。
文件提供了IP-&gt;rackid的翻译。NameNode通过这个得到集群中各个datanode机器的rackid。如果<code>topology.script.file.name</code>没有设定，则每个IP都会翻译
成<code>/default-rack</code>。</p>
<p>Hadoop把网络拓扑看成是一棵树，两个节点的距离=它们到最近共同祖先距离的总和，而树的层次可以这么划分：</p>
<ul>
<li>同一节点中的进程</li>
<li>同一机架上的不同节点</li>
<li>同一数据中心不同机架</li>
<li>不同数据中心的节点</li>
</ul>
<figure>
<img src="/imgs/Hadoop/hdfs/datanodetop.jpg"
alt="datanode 网络拓扑图" />
<figcaption aria-hidden="true">datanode 网络拓扑图</figcaption>
</figure>
<p>若数据中心<span class="math inline">\(d_1\)</span>中一个机架<span
class="math inline">\(r_1\)</span>中一个节点<span
class="math inline">\(n_1\)</span>表示为<span
class="math inline">\(d_1/r_1/n_1\)</span>,则：</p>
<p><span class="math display">\[
distance(d_1/r_1/n_1,d_1/r_1/n_1)=0; 相同的datanode\\\\
distance(d_1/r_1/n_1,d_1/r_1/n_2)=2; 同一rack下的不同datanode\\\\
distance(d_1/r_1/n_1,d_1/r_2/n_3)=4; 同一IDC下的不同datanode\\\\
distance(d_1/r_1/n_1,d_2/r_3/n_4)=6; 不同IDC下的datanode
\]</span></p>
<h1 id="hdfs文件读写过程剖析">hdfs文件读写过程剖析</h1>
<figure>
<img src="/imgs/Hadoop/hdfs/hdfs-architecture.gif"
alt="NameNode和DataNode架构图" />
<figcaption aria-hidden="true">NameNode和DataNode架构图</figcaption>
</figure>
<h2 id="hdfs文件读取过程">hdfs文件读取过程</h2>
<p>HDFS有一个FileSystem实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。HDFS通过rpc调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回含有该块副本的DataNode的节点地址，另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。</p>
<p>HDFS会返回一个FSDataInputStream对象，FSDataInputStream类转而封装成DFSDataInputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：</p>
<ol type="1">
<li>客户端发起读请求</li>
<li>客户端与NameNode得到文件的块及位置信息列表</li>
<li>客户端直接和DataNode交互读取数据</li>
<li>读取完成关闭连接</li>
</ol>
<p>当FSDataInputStream与DataNode通信时遇到错误，它会选取另一个较近的DataNode，并为出故障的DataNode做标记以免重复向其读取数据。FSDataInputStream还会对读取的数据块进行校验和确认，发现块损坏时也会重新读取并通知NameNode。</p>
<p>这样设计的巧妙之处：</p>
<ol type="1">
<li>让客户端直接联系DataNode检索数据，可以使HDFS扩展到大量的并发客户端，因为数据流就是分散在集群的每个节点上的，在运行MapReduce任务时，每个客户端就是一个DataNode节点。</li>
<li>NameNode仅需相应块的位置信息请求（位置信息在内存中，速度极快），否则随着客户端的增加，NameNode会很快成为瓶颈。</li>
</ol>
<h2 id="hdfs文件写入过程">HDFS文件写入过程</h2>
<p>HDFS有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件。DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。</p>
<p>同读文件过程一样，FSDataOutputStream类转而封装成DFSDataOutputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：</p>
<ol type="1">
<li>客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件</li>
<li>待临时文件达到块大小时开始向NameNode请求DataNode信息</li>
<li>NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）</li>
<li>客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode</li>
<li>当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见</li>
</ol>
<p>上面第四步描述的flush过程实际处理过程比较负杂，现在单独描述一下：</p>
<ol type="1">
<li>首先，第一个DataNode是以数据包(数据包一般4KB)的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</li>
<li>在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包</li>
<li>第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点</li>
<li>传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK,最终，第一个DataNode会向客户端发回一个ACK</li>
<li>当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点。然后，客户端会向NameNode发送一个确认</li>
<li>如果管道中的任何一个DataNode失败，管道会被关闭。数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点</li>
<li>数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在hdfs中，供读取时进行完整性校验</li>
</ol>
<h2 id="hdfs文件删除过程">hdfs文件删除过程</h2>
<p>hdfs文件删除过程一般需要如下几步：</p>
<ol type="1">
<li>一开始删除文件，NameNode只是重命名被删除的文件到/trash目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在/trash中文件会被保留一定间隔的时间（可配置，默认是6小时），在这期间，文件可以很容易的恢复，恢复只需要将文件从/trash移出即可。</li>
<li>当指定的时间到达，NameNode将会把文件从命名空间中删除</li>
<li>标记删除的文件块释放空间，HDFS文件系统显示空间增加</li>
</ol>
<h1 id="hdfs缺点">HDFS缺点</h1>
<p>一般来说，一条元信息记录会占用200byte内存空间。假设块大小为64MB，备份数量是3
，那么一个1GB大小的文件将占用1GB/64MB*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。我们可以发现，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>
<p>还有一个问题就是，因为 Map task 的数量是由 splits 来决定的，所以 用
MR 处理大量的小文件时，就会产生过多的 Maptask
，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M
，那就会有10000个Maptasks，会有很大的线程开销；若每个split为
100M，则只有100个Maptasks，每个Maptask
将会有更多的事情做，而线程的管理开销也将减小很多。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a href="https://my.oschina.net/leejun2005/blog/151872">HDFS
原理、架构与特性介绍</a> <a
href="http://blog.csdn.net/suifeng3051/article/details/48548341">Hadoop核心之HDFS
架构设计</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之MapReduce原理</title>
    <url>/2017/04/17/2017-04-17-Hadoop%E4%B9%8BMapReduce%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>Hadoop
Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>
<span id="more"></span>
<h1 id="概述">概述</h1>
<p>一个Map/Reduce作业（job）通常会把输入的数据集切分为若干独立的数据块，由map任务（task）以完全并行的方式处理它们。框架会对map的输出先进行排序，然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p>通常，Map/Reduce框架和分布式文件系统是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。</p>
<p>Map/Reduce框架由一个单独的master JobTracker和每个集群节点一个slave
TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p>
<h1 id="mapreduce的架构">MapReduce的架构</h1>
<p>和HDFS一样，MapReduce也是采用Master/Slave的架构，其架构图如下所示。</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/architecture.jpg"
alt="MapReduce架构图" />
<figcaption aria-hidden="true">MapReduce架构图</figcaption>
</figure>
<p>MapReduce包含四个组成部分，分别为Client、JobTracker、TaskTracker和Task，下面我们详细介绍这四个组成部分。</p>
<h2 id="client-客户端">Client 客户端</h2>
<p>每一个Job
都会在用户端通过Client类将应用程序以及配置参数Configuration打包成JAR文件存储在HDFS，并把路径提交到JobTracker的master服务，然后由master创建每一个Task（即MapTask
和ReduceTask）将它们分发到各个TaskTracker服务中去执行。</p>
<h2 id="jobtracker">JobTracker</h2>
<p>JobTracke负责资源监控和作业调度。JobTracker监控所有TaskTracker与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop
中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。</p>
<h2 id="tasktracker">TaskTracker</h2>
<p>TaskTracker会周期性地通过Heartbeat将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker
使用"slot"等量划分本节点上的资源量。"slot"代表计算资源（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map
slot和Reduce slot两种，分别供Map Task和Reduce
Task使用。TaskTracker通过slot数目（可配置参数）限定Task的并发度。</p>
<h2 id="task">Task</h2>
<p>Task 分为Map Task和Reduce
Task两种，均由TaskTracker启动。HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单位是split。</p>
<p>Map Task执行过程如下图所示：由该图可知，Map
Task先将对应的split迭代解析成一个个key/value
对，依次调用用户自定义的map()函数进行处理，最终将临时结果存放到本地磁盘上,
其中临时数据被分成若干个partition，每个partition将被一个Reduce
Task处理。</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/maptask.jpg" alt="Map Task执行过程" />
<figcaption aria-hidden="true">Map Task执行过程</figcaption>
</figure>
<p>Reduce Task执行过程下图所示。该过程分为三个阶段：</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/reducetask.jpg"
alt="Reduce Task执行过程" />
<figcaption aria-hidden="true">Reduce Task执行过程</figcaption>
</figure>
<ol type="1">
<li>从远程节点上读取Map Task 中间结果（称为“Shuffle 阶段”）；</li>
<li>按照key 对key/value 对进行排序（称为“Sort 阶段”）；</li>
<li>依次读取&lt; key, value list&gt;，调用用户自定义的reduce()
函数处理，并将最终结果存到HDFS 上（称为“Reduce 阶段”）。</li>
</ol>
<h1 id="mapreduce运行机制">MapReduce运行机制</h1>
<p>下面从逻辑实体的角度介绍mapreduce运行机制，这些按照时间顺序包括：输入分片（input
split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/mrjob.png"
alt="MapReduce作业运行流程" />
<figcaption aria-hidden="true">MapReduce作业运行流程</figcaption>
</figure>
<p>在MapReduce运行过程中，最重要的是Map，Shuffle，Reduce三个阶段，各个阶段的作用简述如下：</p>
<ul>
<li>Map:数据输入,做初步的处理,输出形式的中间结果；</li>
<li>Shuffle:按照partition、key对中间结果进行排序合并,输出给reduce线程；</li>
<li>Reduce:对相同key的输入进行最终的处理,并将结果写入到文件中。</li>
</ul>
<figure>
<img src="/imgs/Hadoop/mapreduce/shuffle.png"
alt="MapReduce Shuffle过程" />
<figcaption aria-hidden="true">MapReduce Shuffle过程</figcaption>
</figure>
<p>上图是把MapReduce过程分为两个部分，而实际上从两边的Map和Reduce到中间的那一大块都属于Shuffle过程，也就是说，Shuffle过程有一部分是在Map端，有一部分是在Reduce端。</p>
<h2 id="输入分片input-split">输入分片（input split）</h2>
<h3 id="inputformat">InputFormat</h3>
<p>InputFormat为Map/Reduce作业描述输入的细节规范。Map/Reduce框架根据作业的InputFormat来进行以下操作：</p>
<ul>
<li>检查作业输入的有效性。</li>
<li>把输入文件切分成多个逻辑InputSplit实例，并把每一实例分别分发给一个Mapper。</li>
<li>提供RecordReader的实现，这个RecordReader从逻辑InputSplit中获得输入记录，这些记录将由Mapper处理。</li>
</ul>
<p>InputFormat只包含了两个接口函数:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits) <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">RecordReader &lt; K, V&gt; getRecordReader(InputSplit split, JobConf job, Reporter reporter) <span class="keyword">throws</span> IOException;</span><br></pre></td></tr></table></figure>
<p>getSplits就是现在要使用的划分函数。job参数是任务的配置集合，从中可以取到用户在启动MapReduce时指定的输入文件路径。而numSplits参数是一个Split数目的建议值，是否考虑这个值，由具体的InputFormat实现来决定。返回的是InputSplit数组，它描述了所有的Split信息，一个InputSplit描述一个Split。</p>
<p>getRecordReader方法返回一个RecordReader对象，该对象将输入的InputSplit解析成若干个key/value对，MapReduce框架在Map
Task执行过程中，会不断的调用RecordReader对象中的方法，迭代获取key/value对并交给map函数处理。</p>
<h3 id="inputsplit">InputSplit</h3>
<p>InputSplit是一个单独的Mapper要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。InputSplit也只有两个接口函数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">getLength</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line">String[] getLocations() <span class="keyword">throws</span> IOException;</span><br></pre></td></tr></table></figure>
<p>这个interface仅仅描述了Split有多长，以及存放这个Split的Location信息（也就是这个Split在HDFS上存放的机器。它可能有多个replication，存在于多台机器上）。除此之外，就再没有任何直接描述Split的信息了。</p>
<p>而Split中真正重要的描述信息还是只有InputFormat会关心。在需要读取一个Split的时候，其对应的InputSplit会被传递到InputFormat的第二个接口函数getRecordReader，然后被用于初始化一个RecordReader，以解析输入数据。</p>
<p>在分配Map任务时，Split的Location信息就要发挥作用了。JobTracker会根据TaskTracker的地址来选择一个Location与之最接近的Split所对应的Map任务（注意一个Split可以有多个Location）。这样一来，输入文件中Block的Location信息经过一系列的整合（by
InputFormat）和传递，最终就影响到了Map任务的分配。其结果是Map任务倾向于处理存放在本地的数据，以保证效率。</p>
<h3 id="recordreader">RecordReader</h3>
<p>RecordReader从InputSlit读入&lt;key, value&gt;对。</p>
<p>一般的，RecordReader 把由InputSplit
提供的字节样式的输入文件，转化成由Mapper处理的记录样式的文件。因此RecordReader负责处理记录的边界情况和把数据表示成&lt;keys,
values&gt;对形式。</p>
<h2 id="map阶段">Map阶段</h2>
<p>在进行海量数据处理时，外存文件数据I/O访问会成为一个制约系统性能的瓶颈，因此，Hadoop的Map过程实现的一个重要原则就是：计算靠近数据，这里主要指两个方面：</p>
<ol type="1">
<li>代码靠近数据：
<ul>
<li>原则：本地化数据处理（locality），即一个计算节点尽可能处理本地磁盘上所存储的数据；</li>
<li>尽量选择数据所在DataNode启动Map任务；</li>
<li>这样可以减少数据通信，提高计算效率；</li>
</ul></li>
<li>数据靠近代码：
<ul>
<li>当本地没有数据处理时，尽可能从同一机架或最近其他节点传输数据进行处理（host选择算法）。</li>
</ul></li>
</ol>
<p>map的经典流程图如下：</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/map-shuffle.png"
alt="Map Shuffle过程" />
<figcaption aria-hidden="true">Map Shuffle过程</figcaption>
</figure>
<h3 id="输入">输入</h3>
<ol type="1">
<li>map
task只读取split分片，split与block（HDFS的最小存储单位，默认为64MB）可能是一对一也能是一对多，但是对于一个split只会对应一个文件的一个block或多个block，不允许一个split对应多个文件的多个block；</li>
<li>这里切分和输入数据的时会涉及到InputFormat的文件切分算法和host选择算法。</li>
</ol>
<p>文件切分算法，主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。FileInputFormat以文件为单位切分生成InputSplit，对于每个文件，由以下三个属性值决定其对应的InputSplit的个数：</p>
<ul>
<li>goalSize：
它是根据用户期望的InputSplit数目计算出来的，即totalSize/numSplits。其中，totalSize为文件的总大小；numSplits为用户设定的Map
Task个数，默认情况下是1；</li>
<li>minSize：InputSplit的最小值，由配置参数mapred.min.split.size确定，默认是1；</li>
<li>blockSize：文件在hdfs中存储的block大小，不同文件可能不同，默认是64MB。</li>
</ul>
<p>这三个参数共同决定InputSplit的最终大小，计算方法如下： <span
class="math display">\[
splitSize=\max(minSize, \min(gogalSize,blockSize))
\]</span></p>
<h3 id="partitioner">Partitioner</h3>
<p>作用：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。</p>
<p>实现功能：</p>
<ol type="1">
<li>map输出的是&lt;key,value&gt;对，决定于当前的mapper的partition交给哪个reduce的方法是：mapreduce提供的Partitioner接口，对key进行hash后，再以reducetask数量取模，然后到指定的job上（HashPartitioner，可以通过<code>job.setPartitionerClass(MyPartition.class)</code>自定义）。</li>
<li>然后将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。&lt;key,value&gt;对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组。</li>
</ol>
<p>要求：负载均衡，效率；</p>
<h3 id="spill溢写">spill（溢写）</h3>
<p>作用：把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick
sort）；</p>
<p>注意：</p>
<ol type="1">
<li>这个spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区（两个指针的方向不会变，下面会详述）；</li>
<li>在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，这个就是sort操作，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
</ol>
<h3 id="combiner">Combiner</h3>
<p>combine：执行combine操作要求开发者必须在程序中设置了combine（程序中通过<code>job.setCombinerClass(myCombine.class)</code>自定义combine操作）。</p>
<p>程序中有两个阶段可能会执行combine操作：</p>
<ol type="1">
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性<code>min.num.spills.for.combine</code>配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
<p>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</p>
<p>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：</p>
<ol type="1">
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ol>
<h3 id="merge">merge</h3>
<p>当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map
task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map
Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map
Task最终只生成一个中间数据文件。</p>
<p>注意：</p>
<ol type="1">
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性<code>min.num.spills.for.combine</code>配置；</li>
<li>多个溢出文件合并时，会进行一次排序，排序算法是<strong>多路归并排序</strong>；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做<code>file.out.index</code>。</li>
</ol>
<h3 id="内存缓冲区">内存缓冲区</h3>
<p>在Map
Task任务的业务处理方法map()中，最后一步通过<code>OutputCollector.collect(key,value)</code>或<code>context.write(key,value)</code>输出Map
Task的中间处理结果，在相关的<code>collect(key,value)</code>方法中，会调用<code>Partitioner.getPartition(K2 key, V2 value, int numPartitions)</code>方法获得输出的&lt;key,value&gt;对应的分区号(分区号可以认为对应着一个要执行Reduce
Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数<code>io.sort.mb</code>来调整其大小。</p>
<p>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的Linux本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件。</p>
<p>缓存有一个阀值比例配置，当达到整个缓存的这个比例时，会触发spill操作；触发时，map输出还会接着往剩下的空间写入，但是写满的空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区；</p>
<h2 id="reduce阶段">Reduce阶段</h2>
<p>Reduce过程的经典流程图如下：</p>
<figure>
<img src="/imgs/Hadoop/mapreduce/reduce-shuffle.png"
alt="Redece过程流程图" />
<figcaption aria-hidden="true">Redece过程流程图</figcaption>
</figure>
<h3 id="copy">copy</h3>
<p>作用：拉取数据；</p>
<p>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map
task所在的TaskTracker获取map task的输出文件。因为这时map
task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</p>
<p>默认情况下，当整个MapReduce作业的所有已执行完成的Map
Task任务数超过Map Task总数的5%后，JobTracker便会开始调度执行Reduce
Task任务。然后Reduce
Task任务默认启动<code>mapred.reduce.parallel.copies</code>(默认为5）个MapOutputCopier线程到已完成的Map
Task任务节点上分别copy一份属于自己的数据。
这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p>
<h3 id="merge-1">merge</h3>
<p>Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap
size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。</p>
<p>这里需要强调的是，merge有三种形式：1)内存到内存 2)内存到磁盘
3)磁盘到磁盘。默认情况下第一种形式是不启用的。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge）
。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有
map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge
（图中的第二个merge）方式生成最终的那个文件。</p>
<p>在远程copy数据的同时，Reduce
Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</p>
<h3 id="reduce">Reduce</h3>
<p>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer
输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer
执行，把结果放到 HDFS 上。</p>
<p>Reduce的数目建议是0.95或1.75乘以
(<code>&lt;no. of nodes&gt; * mapred.tasktracker.reduce.tasks.maximum</code>)。</p>
<p>用0.95，所有reduce可以在maps一完成时就立刻启动，开始传输map的输出结果。用1.75，速度快的节点可以在完成第一轮reduce任务后，可以开始第二轮，这样可以得到比较好的负载均衡的效果。</p>
<p>如果没有归约要进行，那么设置reduce任务的数目为零是合法的。这种情况下，map任务的输出会直接被写入由
setOutputPath(Path)指定的输出路径。框架在把它们写入FileSystem之前没有对它们进行排序。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a
href="http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html">Hadoop
Map/Reduce教程</a> <a
href="http://blog.csdn.net/u010330043/article/details/51200712">深入理解MapReduce的架构及原理</a>
<a href="http://blog.csdn.net/hsuxu/article/details/7673171/">Hadoop
InputFormat浅析--hadoop如何分配输入</a> <a
href="http://wangzzu.github.io/2016/03/02/hadoop-shuffle/">MapReduce之Shuffle过程详述</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之Yarn原理</title>
    <url>/2017/04/17/2017-04-17-Hadoop%E4%B9%8BYarn%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>YARN（Yet Another Resource
Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。</p>
<span id="more"></span>
<h1 id="hadoop-mapreduce-框架的问题">Hadoop MapReduce 框架的问题</h1>
<p>随着分布式系统集群的规模和其工作负荷的增长，MapReduce
框架的问题逐渐浮出水面，主要的问题集中如下：</p>
<ol type="1">
<li>JobTracker 是 Map-reduce 的集中处理点，存在单点故障。</li>
<li>JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job
非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker fail
的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000
节点主机的上限。</li>
<li>在 TaskTracker 端，以 map/reduce task
的数目作为资源的表示过于简单，没有考虑到 cpu/
内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现
OOM(Out-Of-Memory)。</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task
slot, 如果当系统中只有 map task 或者只有 reduce task
的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
<li>源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class
做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug
修复和版本维护的难度。</li>
<li>从操作的角度来看，现在的 Hadoop MapReduce
框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 )
时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的
Hadoop 版本而浪费大量时间。</li>
</ol>
<h1 id="yarn介绍">Yarn介绍</h1>
<p>Yarn的核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。</p>
<p>YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等)
部署到一个公共集群中，共享集群的资源。并提供如下功能：</p>
<ol type="1">
<li>资源的统一管理和调度：
集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container，
YARN按照特定的策略对资源进行调度进行Container的分配。</li>
<li>资源隔离：
YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</li>
</ol>
<p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。</p>
<p>YARN可以看成一个云操作系统，由一个ResourceManager和多个NodeManager组成，
它负责管理所有NodeManger上多维度资源，
并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑)
或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。</p>
<h1 id="yarn体系架构">YARN体系架构</h1>
<p>YARN架构如下图所示：</p>
<figure>
<img src="/imgs/Hadoop/yarn/architecture.png" alt="YARN架构图" />
<figcaption aria-hidden="true">YARN架构图</figcaption>
</figure>
<p>YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、
ApplicationMaster和Container等几个组件构成。</p>
<ul>
<li><p><strong>ResourceManager(RM)</strong>
负责对各NodeManager上的资源进行统一管理和调度。将ApplicationMaster分配空闲的Container运行并监控其运行状态。对ApplicationMaster申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器：</p>
<ol type="1">
<li><strong>调度器(Scheduler)</strong>：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Scheduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。</li>
<li><strong>应用程序管理器(Applications
Manager)</strong>：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li>
</ol></li>
<li><p><strong>NodeManager (NM)</strong>
NodeManager是每个节点上的资源和任务管理器。它会定时地向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自ApplicationMaster的Container
启动/停止等请求。</p></li>
<li><p><strong>ApplicationMaster (AM)</strong>
用户提交的应用程序均包含一个ApplicationMaster，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。</p></li>
<li><p><strong>Container</strong>
Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster返回的资源便是用Container
表示的。
YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</p></li>
</ul>
<h1 id="yarn应用工作流程">YARN应用工作流程</h1>
<p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p>
<ol type="1">
<li>启动ApplicationMaster，如下步骤1~3；</li>
<li>由ApplicationMaster创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li>
</ol>
<figure>
<img src="/imgs/Hadoop/yarn/yarn.png" alt="YARN应用工作流程图" />
<figcaption aria-hidden="true">YARN应用工作流程图</figcaption>
</figure>
<ol type="1">
<li>用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</li>
<li>ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动ApplicationMaster；</li>
<li>ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager査看应用程序的运行状态，运行状态通过
<code>AMRMClientAsync.CallbackHandler</code>的<code>getProgress()</code>
方法来传递给ResourceManager。
然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</li>
<li>ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源；资源的协调通过<code>AMRMClientAsync</code>异步完成,相应的处理方法封装在<code>AMRMClientAsync.CallbackHandler</code>中。</li>
<li>一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务；通常需要指定一个<code>ContainerLaunchContext</code>，提供Container启动时需要的信息。</li>
<li>NodeManager为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NodeManager的通信通过<code>NMClientAsync object</code>来完成，容器的所有事件通过<code>NMClientAsync.CallbackHandler</code>来处理。例如启动、状态更新、停止等。</li>
<li>应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</li>
</ol>
<h1 id="yarn资源调度模型">YARN资源调度模型</h1>
<p>YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。</p>
<p>YARN采用了双层资源调度模型：</p>
<ul>
<li>ResourceManager中的资源调度器将资源分配给各个ApplicationMaster：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的ApplicationMaster，而是暂时放到一个缓冲区中，等待ApplicationMaster通过周期性的心跳主动来取；</li>
<li>ApplicationMaster领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。</li>
</ul>
<p>也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。</p>
<p>YARN目前采用的资源分配算法有三种。但真实的调度器实现中还对算法做了一定程度的优化。</p>
<ol type="1">
<li>Capacity
Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。
该调度器保证共享集群的各个组织能够得到容量的保证，同时可以超额使用集群中暂时没有人使用的资源。Capacity
Scheduler为了实现这些目标，抽象了queue的概念，queue通常由管理员配置。为了进一步细分容量的使用，调度器支持层级化的queue（hierarchical
queues），使得在特定组织内部，可以进一步有效利用集群资源。
Capacity调度器支持的一些特性如下：
<ul>
<li>层级队列（Hierarchical Queues）</li>
<li>容量保证</li>
<li>安全性：每个队列都有队列的访问权限控制（ACL）</li>
<li>弹性： 空闲资源可以额外分配给任何需要的队列</li>
<li>多租户</li>
<li>基于资源的调度（resouce-based scheduling):
对资源敏感的应用程序，可以有效地控制资源情况</li>
<li>支持用户（组）到queue的映射：基于用户组提交作业到对应queue。</li>
<li>运营支持：支持运行时配置队列的容量，ACL等。也可以在运行时停止queue阻止进一步往queue提交作业。</li>
</ul></li>
<li>Fair
Scheduler：公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。
这种策略使得运行时间短的应用能够尽快结束，而不至于在等待资源时被饿死。另外，也可以为应用程序配置优先级，优先级用于决定资源使用量的占比。</li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a
href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/">Hadoop
新MapReduce 框架Yarn 详解</a> <a
href="http://blog.csdn.net/bingduanlbd/article/details/51880019">理解Hadoop
YARN架构</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>宇龙计算机通信科技（深圳）有限公司</title>
    <url>/2020/08/11/%E5%AE%87%E9%BE%99%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%80%9A%E4%BF%A1%E7%A7%91%E6%8A%80%EF%BC%88%E6%B7%B1%E5%9C%B3%EF%BC%89%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="ffb5b5641a5e022baa2786976a280c71fb3f377b3a999a6cdfc22f899dceb99d">5d20b81ac1f5d0b70e9113cacf134e1e15826083e6b6367bf1e68fe6118c8f33a272ce2d0e6bcfeeda4cbdbd7a02d9c9c7de930988ecdeee0b836b6626acc05292246098cd90e4a352207e089580ce1a27367db90d4adbdb5f4cdbeb949d8c9c584d005e75b2a3a0a439ca76fd49237643853e066dc4184be3ef9446555097b627860e8711b594da3d593c348ad8c62d2c559f53ced606e9cc7368e2899828f36e3d74c7dc9758735f3ee715390800665d39a6a15717012a6b22ab7a64fac917958c6e176b34b49a6566ccc2d7bcd89563a2dcb49671d09f1e99f8a5b4eb19633d9fd3e617346e42abe303929dd53097c19ffecec7ce2a78163e0b9615ff79c0f3c359e5b687a7089b13d13beac8680039e7b53b31cab5948cbac65385795e2322ff1564994d3221e3fa35852bf9dae07335578c76bd09737345155a281891dcd63b9835119fc457427ead3ae0e2a951d1ab3e010552a7b4ab9d44d0b9ea6e84d294f59c0fdfc7a3f5b0f1a7e98a9fb9cb4fb38dd56789e18ceba3a454757bde804f044cff01846e3b0cebd7e91be3e700f450aae193c5c67ad29bfb68eea1ded083eff697f6e66230030e3d9ec7f6e31e225acada3675cf73e3eac2cb31268f3ee0c02a404c3f82b513c2c9d4382a5606c1453053cde6dae60ab09ee8670bd0d6f28b7747b8e88e78380783458d94d5d3a2e2b2e8ce605dd8a222313a9e44bbf8d40befffbcc24ea6a9178117d489128dc97f14122a0c08351d39ef4a4c8252dafcdcf4d42de945d66715734a4044b6d02eb7e7e5f972c7a9f4f5cb26ee3d9ad332b59bbdebd0f7f008f8e317d44300bde7e5b26a511b8a83afb48cc008c84cd3fd9ec6ad2425c6c73cf2626d717a940f3074ba88a890fe6ededb464d3bcd08d0376b9094603fef60bf94abe78abda57498ccd38cbc73f3bf6a2792c7ee833a74a7e7e180648e2d7c845a247a5a84114b343f5a0dd46938f87eb4de73d8924be96a7d19e2dae327b9c10888363eb9e34a9bd081722ee0db2b1c2c1e41ed9cc4915804d8068ad0c7ad8c97e2307082b95be91f4156580b02bdc8cec4ccf6bda656f90d9b2c7f8c9de483009dd0930441b58b20eac36a563f10eb3a15134179487d977a4ddeccf0588172cf4e06d7b6239e88449679e9ef264aee5f1740ebe0b1c823eb3985d051225204e5e1b238db1b10dc457a3816634e346106c17048f009483e65f5207caf6d8d7597f2e94995f40d8434674648c257dbbc827c4c5fa2fe4c427151046b3f26328ea79f761a22f6e15f78f141845d3ba7c595282e6a2f68de0c01294a4825a0012ce03dec5e5cc939f8a26854af19ae2699b1fab107fbef814504ab43fe2367798851328edf7b91dacc0a611c43d2475e62f870231aa181581de1a77a528c91a271d269a8d73a21378599e9af9c2aa937c9690fca05e11e373484199147f1826c19ab42e1756ddf08eff25e28d3b944856d0c589c79e4f158738fcfe7db066260a09765c242f070eedf5c2de2996606a65372f30cf2cb9b00ab6f0b19659b90de6c1c889344d06eedf46abac7180b0b870d137e2c35520b79c7202133319b210fddaea7284ba86397fd67ad33cff7eb5e0542f86f4014c2303896c325216217dd20245f4d157c3f120f187edb2d458fe7a097b07c7ede371adb00263995ce29d6a15b1d1b17e982ace7ee362950c7a3785f02394d35cebaa109df15cdf55e1a3c41437b7bb9a258b0a87cc83bbee3d6b53fbe3ab9a1ceb346e045004da15045540cc5cc449b23a6cb7cb10170ae7e6abb29bef3b00c68d25110d5183acaf5ee72ba5e3a9c6d544293be3dc67f335e3005e33a4f7a1951f2b5c24df23e2dceed352d5d180dcdd6bcfab9067052a5aeef827df8cdd097789d2d90d9cc9d3928cc558352d67f4b6155684a7da8f73578cd0b810b52bd1f3309bcd27fad0ec492dbc12a7b110e37c42cd15bea0ae355ffcce753c3acf8ca4473ddd0b86f358dfeee2c8fa3a103beeb7d1e6cda61167e2a620b93eed90c88beb9</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>rs</tag>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>oppo广东移动通信有限公司</title>
    <url>/2020/08/12/oppo%E5%B9%BF%E4%B8%9C%E7%A7%BB%E5%8A%A8%E9%80%9A%E4%BF%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="65eb9e5160b127f63a1fb86838eea0cabe99619f4f008b9c06f23ea7eea0c9e4">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳追一科技有限公司</title>
    <url>/2020/08/03/%E6%B7%B1%E5%9C%B3%E8%BF%BD%E4%B8%80%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="9b08cdd55ca4709e1cbb7829f138493acf16bd4b02e877a0fa76337b8b29ac4c">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>rs todo</tag>
        <tag>nlp todo</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳乐信软件技术有限公司</title>
    <url>/2020/08/05/%E6%B7%B1%E5%9C%B3%E4%B9%90%E4%BF%A1%E8%BD%AF%E4%BB%B6%E6%8A%80%E6%9C%AF%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="b77d16bf28a0d55d34590061aa9b42931bebea99afc92e3ffd6f74ec05fc4079">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>维沃移动通信有限公司</title>
    <url>/2020/08/12/%E7%BB%B4%E6%B2%83%E7%A7%BB%E5%8A%A8%E9%80%9A%E4%BF%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="43478cb0bee6481793867625a82e80be56b354d4480e4e6d4276e00f6b7db96a">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>粤港澳大湾区数字经济研究院</title>
    <url>/2020/08/08/%E7%B2%A4%E6%B8%AF%E6%BE%B3%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%95%B0%E5%AD%97%E7%BB%8F%E6%B5%8E%E7%A0%94%E7%A9%B6%E9%99%A2/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="03931f645bc0345aac30609d67c5ca5e792bea236b636e5e5a4d330230bd7cad">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>纵表、横表互转</title>
    <url>/2017/12/03/2017-12-03-%E7%BA%B5%E8%A1%A8%E3%80%81%E6%A8%AA%E8%A1%A8%E4%BA%92%E8%BD%AC/</url>
    <content><![CDATA[<p>mysql中横表和纵表的互换。</p>
<span id="more"></span>
<p>将纵表中的多行记录转化成横表中的一条记录。</p>
<h2 id="建表">1、 建表</h2>
<p>纵表结构：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Table_A</span><br><span class="line">(</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    course <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    score <span class="type">int</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;chinese&#x27;</span>,<span class="number">60</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;math&#x27;</span>,<span class="number">70</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;english&#x27;</span>,<span class="number">80</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;chinese&#x27;</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;math&#x27;</span>,<span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<p>横表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Table_B</span><br><span class="line">(</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    chinese <span class="type">int</span>,</span><br><span class="line">    math <span class="type">int</span>,</span><br><span class="line">    english <span class="type">int</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_B(name,chinese,math,english) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="number">60</span>,<span class="number">70</span>,<span class="number">80</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Table_B(name,chinese,math,english) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="number">90</span>,<span class="number">100</span>,<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h2 id="纵表变横表">2、纵表变横表</h2>
<p>方法一：聚合函数[max或sum]配合case语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;chinese&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> chinese,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;math&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> math,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;english&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> english</span><br><span class="line"><span class="keyword">from</span> Table_A</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure>
<p>方法二：使用pivot <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Table_A pivot (<span class="built_in">max</span>(score)<span class="keyword">for</span> course <span class="keyword">in</span>(chinese,math,english)) tmp_table;</span><br></pre></td></tr></table></figure></p>
<h2 id="横表变纵表">3、横表变纵表</h2>
<p>方法一：union all</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;chinese&#x27;</span> <span class="keyword">as</span> course,chinese <span class="keyword">as</span> score <span class="keyword">from</span> Table_B <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;math&#x27;</span> <span class="keyword">as</span> course,math <span class="keyword">as</span> score <span class="keyword">from</span> Table_B <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;english&#x27;</span> <span class="keyword">as</span> course,english <span class="keyword">as</span> score <span class="keyword">from</span> Table_B</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name,course <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<p>方法二：使用unpivot</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,course,score <span class="keyword">from</span> Table_B</span><br><span class="line">unpivot</span><br><span class="line">(score <span class="keyword">for</span> course <span class="keyword">in</span> ([chinese],[math],english)) tmp_table;</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<blockquote>
<p><a
href="http://www.cnblogs.com/liushen/p/3333936.html">纵表、横表互转的SQL</a></p>
</blockquote>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql入门</title>
    <url>/2017/12/04/2017-12-04-mysql%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>本文记录mysql的一些常用命令。</p>
<span id="more"></span>
<ol type="1">
<li><p>启动与停止mysql</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">net <span class="keyword">start</span> mysql</span><br><span class="line">net stop mysql</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建一个名称为mydb1的数据库</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb1;</span><br></pre></td></tr></table></figure></p></li>
<li><p>显示所有数据库</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建一个使用utf-8字符集的mydb2数据库。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb2 <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建一个使用utf-8字符集，并带校对规则的mydb3数据库</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb3 <span class="type">character</span> <span class="keyword">set</span> utf8 <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查看前面创建的mydb2数据库的定义信息</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> database mydb2;</span><br></pre></td></tr></table></figure></p></li>
<li><p>删除前面创建的mydb1数据库</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database mydb1;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查看服务器中的数据库，并把mydb2库的字符集修改为gb2312;</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> database mydb2 <span class="type">character</span> <span class="keyword">set</span> gb2312;</span><br></pre></td></tr></table></figure></p></li>
<li><p>备份mydb3库中的数据，并恢复</p>
<p>备份（退到window命令行窗口）：</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysqldump <span class="operator">-</span>u root <span class="operator">-</span>p mydb3<span class="operator">&gt;</span>c:\test.sql</span><br></pre></td></tr></table></figure></p>
<p>恢复：</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb3;</span><br><span class="line">use mydb3;</span><br><span class="line">source c:\test.sql</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建一个员工表</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> employee</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    sex <span class="type">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    birthday <span class="type">date</span>,</span><br><span class="line">    entry_date <span class="type">date</span>,</span><br><span class="line">    job <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span>,</span><br><span class="line">    resume text</span><br><span class="line">)<span class="type">character</span> <span class="keyword">set</span> utf8 <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure></p></li>
<li><p>在上面员工表的基本上增加一个image列。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employee <span class="keyword">add</span> image <span class="type">blob</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查看表</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> employee;</span><br></pre></td></tr></table></figure></p></li>
<li><p>修改job列，使其长度为60。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employee modify job <span class="type">varchar</span>(<span class="number">60</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>删除sex列</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employee <span class="keyword">drop</span> sex;</span><br></pre></td></tr></table></figure></p></li>
<li><p>表名改为user</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rename <span class="keyword">table</span> employee <span class="keyword">to</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>修改表的字符集为utf-8</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">user</span> <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查看表的字符集</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">user</span>;(表的创建语句)</span><br></pre></td></tr></table></figure></p></li>
<li><p>列名name修改为username</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">user</span> change <span class="keyword">column</span> name username <span class="type">varchar</span>(<span class="number">20</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>使用insert语句向表中插入三个员工的信息。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rename <span class="keyword">table</span> <span class="keyword">user</span> <span class="keyword">to</span> employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employee(id,username,birthday,entry_date,job,salary,resume) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;aaa&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="number">90</span>,<span class="string">&#x27;aaaa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employee(id,username,birthday,entry_date,job,salary,resume) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="number">90</span>,<span class="string">&#x27;aaaa&#x27;</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>插入中文数据</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employee(id,username) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;小李子&#x27;</span>);</span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;chara%&#x27;</span>;</span><br><span class="line"><span class="keyword">set</span> character_set_cilent<span class="operator">=</span>gb2312;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employee(id,username) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;小李子&#x27;</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询时如果发生乱码：</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> character_set_results<span class="operator">=</span>gb2312;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employee;</span><br></pre></td></tr></table></figure></p></li>
<li><p>将所有员工薪水修改为5000元。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">update employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">5000</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>将姓名为’aaa’的员工薪水修改为3000元。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">update employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">3000</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>将姓名为’aaa’的员工薪水修改为4000元,job改为ccc。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">update employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">4000</span>,job<span class="operator">=</span><span class="string">&#x27;ccc&#x27;</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>将aaa的薪水在原有基础上增加1000元。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">update employee <span class="keyword">set</span> salary<span class="operator">=</span>salary<span class="operator">+</span><span class="number">1000</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>删除表中名称为’zs’的记录</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> employee <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>删除表中所有记录。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> employee;</span><br></pre></td></tr></table></figure></p></li>
<li><p>使用truncate删除表中记录</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> employee</span><br></pre></td></tr></table></figure></p></li>
<li><p>执行sql脚本</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">source c:\student.sql</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询表中所有学生的信息。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询表中所有学生的姓名和对应的英语成绩。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,english <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>过滤表中重复数据。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> english <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>在所有学生总分上加10分特长分。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>使用别名表示学生分数。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">as</span> 姓名,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> <span class="keyword">as</span> 总分 <span class="keyword">from</span> student;</span><br><span class="line"><span class="keyword">select</span> name 姓名,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> 总分 <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询姓名为wu的学生成绩</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;王五&#x27;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p>查询英语成绩大于90分的同学</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english<span class="operator">&gt;</span><span class="number">90</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询总分大于200分的所有同学</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> (english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">&gt;</span><span class="number">200</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询英语分数在 80－90之间的同学</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english<span class="operator">&gt;</span><span class="number">80</span> <span class="keyword">and</span> english<span class="operator">&lt;</span><span class="number">90</span>;</span><br><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english <span class="keyword">between</span> <span class="number">80</span> <span class="keyword">and</span> <span class="number">90</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询数学分数为89,90,91的同学</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">=</span><span class="number">80</span> <span class="keyword">or</span> math<span class="operator">=</span><span class="number">90</span> <span class="keyword">or</span> math<span class="operator">=</span><span class="number">91</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> math <span class="keyword">in</span>(<span class="number">80</span>,<span class="number">90</span>,<span class="number">91</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询所有姓李的学生成绩。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="keyword">like</span> <span class="string">&#x27;李%&#x27;</span></span><br></pre></td></tr></table></figure></p></li>
<li><p>查询数学分&gt;80，语文分&gt;80的同学。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">&gt;</span><span class="number">80</span> <span class="keyword">and</span> chinese<span class="operator">&gt;</span><span class="number">80</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>对数学成绩排序后输出</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">order</span> <span class="keyword">by</span> math;</span><br></pre></td></tr></table></figure></p></li>
<li><p>对总分排序后输出，然后再按从高到低的顺序输出</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">order</span> <span class="keyword">by</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>对姓李的学生成绩排序输出</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="keyword">like</span> <span class="string">&#x27;李%&#x27;</span> <span class="keyword">order</span> <span class="keyword">by</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese);</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计一个班级共有多少学生？</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计数学成绩大于90的学生有多少个？</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">&gt;</span><span class="number">90</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计总分大于250的人数有多少？</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student <span class="keyword">where</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese)<span class="operator">&gt;</span><span class="number">250</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计一个班级数学总成绩？</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计一个班级语文、英语、数学各科的总成绩</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese),<span class="built_in">sum</span>(english),<span class="built_in">sum</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计一个班级语文、英语、数学的成绩总和</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese<span class="operator">+</span>math<span class="operator">+</span>english) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>统计一个班级语文成绩平均分</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese)<span class="operator">/</span><span class="built_in">count</span>(chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>求一个班级数学平均分？</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>求一个班级总分平均分</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>求班级最高分和最低分</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">max</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese),<span class="built_in">min</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></p></li>
<li><p>第6种形式的select:</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orders(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    product <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    price <span class="type">float</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;电视&#x27;</span>,<span class="number">900</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;洗衣机&#x27;</span>,<span class="number">100</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">&#x27;洗衣粉&#x27;</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">&#x27;桔子&#x27;</span>,<span class="number">9</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">&#x27;洗衣粉&#x27;</span>,<span class="number">90</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>对订单表中商品归类后，显示每一类商品的总价</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product,<span class="built_in">sum</span>(price) <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product;</span><br></pre></td></tr></table></figure></p></li>
<li><p>查询购买了几类商品，并且每类总价大于100的商品</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product <span class="keyword">where</span> <span class="built_in">sum</span>(price)<span class="operator">&gt;</span><span class="number">100</span>;×</span><br><span class="line"><span class="keyword">select</span> product <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product <span class="keyword">having</span> <span class="built_in">sum</span>(price)<span class="operator">&gt;</span><span class="number">100</span>;</span><br></pre></td></tr></table></figure></p></li>
<li><p>主键约束</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">	<span class="keyword">create</span> <span class="keyword">table</span> test1</span><br><span class="line">	(</span><br><span class="line">	    id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">	    name <span class="type">varchar</span>(<span class="number">20</span>)</span><br><span class="line">	);</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 主键自动增长：</span><br><span class="line"></span><br><span class="line">	```<span class="keyword">sql</span></span><br><span class="line">	<span class="keyword">create</span> <span class="keyword">table</span> test2</span><br><span class="line">	(</span><br><span class="line">	    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">	    name <span class="type">varchar</span>(<span class="number">20</span>)</span><br><span class="line">	 );</span><br></pre></td></tr></table></figure></p></li>
<li><p>唯一约束和非空约束</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">user</span></span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    username <span class="type">varchar</span>(<span class="number">40</span>) <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">unique</span>,</span><br><span class="line">    password <span class="type">varchar</span>(<span class="number">40</span>) <span class="keyword">not</span> <span class="keyword">null</span>,</span><br><span class="line">    email <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">unique</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></p></li>
<li><p>外键约束</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> male</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">40</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> female</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">40</span>),</span><br><span class="line">    male_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">constraint</span> male_id_FK <span class="keyword">foreign</span> key(male_id) <span class="keyword">references</span> male(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建部门表和员工表(一对多或多对一)</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> department</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> employee</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span>,</span><br><span class="line">    department_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">constraint</span> department_id_FK <span class="keyword">foreign</span> key(department_id)<span class="keyword">references</span> department(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建学生、老师表（多对多）</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> teacher</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> teacher_student</span><br><span class="line">(</span><br><span class="line">    teacher_id <span class="type">int</span>,</span><br><span class="line">    student_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">primary</span> key(teacher_id,student_id),</span><br><span class="line">    <span class="keyword">constraint</span> teacher_id_FK <span class="keyword">foreign</span> key(teacher_id) <span class="keyword">references</span> teacher(id),</span><br><span class="line">    <span class="keyword">constraint</span> student_id_FK <span class="keyword">foreign</span> key(student_id) <span class="keyword">references</span> student(id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher(name,salary) <span class="keyword">values</span>(<span class="string">&#x27;老王&#x27;</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher(name,salary) <span class="keyword">values</span>(<span class="string">&#x27;老李&#x27;</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;aaa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;bbb&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;ccc&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure></p></li>
<li><p>已知老师的id为1，查询出1号老师所有的学生</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> s.<span class="operator">*</span> <span class="keyword">from</span> teacher_student t_s,student s <span class="keyword">where</span> teacher_id<span class="operator">=</span><span class="number">1</span> <span class="keyword">and</span> t_s.student_id<span class="operator">=</span>s.id;</span><br></pre></td></tr></table></figure></p></li>
<li><p>1号学生有几个老师</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> teacher.<span class="operator">*</span> <span class="keyword">from</span> teacher_student,teacher <span class="keyword">where</span> student_id<span class="operator">=</span><span class="number">1</span> <span class="keyword">and</span> teacher_student.teacher_id<span class="operator">=</span>teacher.id;</span><br></pre></td></tr></table></figure></p></li>
<li><p>创建人、身份证表（一对一）</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">30</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> idcard</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">    num <span class="type">varchar</span>(<span class="number">30</span>) <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">unique</span>,</span><br><span class="line">    <span class="keyword">constraint</span> person_id_FK <span class="keyword">foreign</span> key(id) <span class="keyword">references</span> person(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p></li>
<li><p>数据分页</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> news limits <span class="number">0</span>,<span class="number">20</span>;第<span class="number">0</span>个位置取<span class="number">20</span>条</span><br></pre></td></tr></table></figure></p></li>
<li><p>当数据库是自动生成主键时，可以使用如下的语句：</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">st <span class="operator">=</span> con.prepareStatement(<span class="keyword">sql</span>,Statement.RETURN_GENERATED_KEYS);</span><br><span class="line">st.executeUpdate();</span><br><span class="line">rs <span class="operator">=</span> st.getGeneratedKeys();</span><br></pre></td></tr></table></figure></p></li>
<li><p>事物：逻辑上的一组操作，要么全部成功，要么全部都不成功。</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">	<span class="keyword">start</span> transaction 开启事物</span><br><span class="line">	<span class="keyword">Rollback</span> 回滚事物</span><br><span class="line">	<span class="keyword">Commit</span>提交事物</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> JDBC控制事物语句</span><br><span class="line"></span><br><span class="line">	```<span class="keyword">sql</span></span><br><span class="line">	conn.setAutoCommit(<span class="literal">false</span>);</span><br><span class="line">	conn.commit();</span><br><span class="line">	conn.rollback();</span><br></pre></td></tr></table></figure></p></li>
<li><p>事物设置回滚点</p>
<p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sp <span class="operator">=</span> conn.setSavepoint();</span><br><span class="line">conn.rollback(sp);</span><br><span class="line">conn.commit(); 回滚了也要提交事物</span><br></pre></td></tr></table></figure></p></li>
</ol>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库索引</title>
    <url>/2017/12/05/2017-12-05-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<p>本文介绍数据库索隐的基本结构、聚集索引与非聚集索引等问题。</p>
<span id="more"></span>
<h1 id="b-tree">B-Tree</h1>
<p>我们常见的数据库系统，其索引使用的数据结构多是B-Tree或者B+Tree。例如，MsSql使用的是B+Tree，Oracle及Sysbase使用的是B-Tree。所以在最开始，简单地介绍一下B-Tree。</p>
<p>B-Tree不同于Binary
Tree（二叉树，最多有两个子树），一棵M阶的B-Tree满足以下条件：</p>
<pre><code>1）每个结点至多有M个孩子；
2）除根结点和叶结点外，其它每个结点至少有M/2个孩子；
3）根结点至少有两个孩子（除非该树仅包含一个结点）；
4）所有叶结点在同一层，叶结点不包含任何关键字信息；
5）有K个关键字的非叶结点恰好包含K+1个孩子；</code></pre>
<p>另外，对于一个结点，其内部的关键字是从小到大排序的。以下是B-Tree（M=4）的样例：</p>
<p><img title="B-Tree样例" src="/imgs/sql/index/b-tree.png" style="display:block;margin:auto" /></p>
<p>对于每个结点，主要包含一个关键字数组Key[]，一个指针数组（指向儿子）Son[]。在B-Tree内，查找的流程是：使用顺序查找（数组长度较短时）或折半查找方法查找Key[]数组，若找到关键字K，则返回该结点的地址及K在Key[]中的位置；否则，可确定K在某个Key[i]和Key[i+1]之间，则从Son[i]所指的子结点继续查找，直到在某结点中查找成功；或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。</p>
<p>接着，我们使用以下图片演示如何生成B-Tree（M=4，依次插入1~6）：</p>
<p>从图可见，当我们插入关键字4时，由于原结点已经满了，故进行分裂，基本按一半的原则进行分裂，然后取出中间的关键字2，升级（这里是成为根结点）。其它的依类推，就是这样一个大概的过程。</p>
<p><img title="B-Tree样例" src="/imgs/sql/index/b-tree2.png" style="display:block;margin:auto" /></p>
<h1 id="数据库索引">数据库索引</h1>
<h2 id="什么是索引">什么是索引</h2>
<p>在数据库中，索引的含义与日常意义上的“索引”一词并无多大区别（想想小时候查字典），它是用于提高数据库表数据访问速度的数据库对象。</p>
<pre><code>A）索引可以避免全表扫描。多数查询可以仅扫描少量索引页及数据页，而不是遍历所有数据页。
B）对于非聚集索引，有些查询甚至可以不访问数据页。
C）聚集索引可以避免数据插入操作集中于表的最后一个数据页。
D）一些情况下，索引还可用于避免排序操作。</code></pre>
<p>当然，众所周知，虽然索引可以提高查询速度，但是它们也会导致数据库系统更新数据的性能下降，因为大部分数据更新需要同时更新索引。</p>
<h2 id="索引的存储">索引的存储</h2>
<p>一条索引记录中包含的基本信息包括：键值（即你定义索引时指定的所有字段的值）+逻辑指针（指向数据页或者另一索引页）。</p>
<p><img title="索引" src="/imgs/sql/index/index.png" style="display:block;margin:auto" /></p>
<p>当你为一张空表创建索引时，数据库系统将为你分配一个索引页，该索引页在你插入数据前一直是空的。此页此时既是根结点，也是叶结点。每当你往表中插入一行数据，数据库系统即向此根结点中插入一行索引记录。当根结点满时，数据库系统大抵按以下步骤进行分裂：</p>
<pre><code>A）创建两个儿子结点
B）将原根结点中的数据近似地拆成两半，分别写入新的两个儿子结点
C）根结点中加上指向两个儿子结点的指针</code></pre>
<p>通常状况下，由于索引记录仅包含索引字段值（以及4-9字节的指针），索引实体比真实的数据行要小许多，索引页相较数据页来说要密集许多。一个索引页可以存储数量更多的索引记录，这意味着在索引中查找时在I/O上占很大的优势，理解这一点有助于从本质上了解使用索引的优势。</p>
<h2 id="索引的类型">索引的类型</h2>
<p>索引有聚集索引和非聚集索引两种类型。</p>
<pre><code>A）聚集索引，表数据按照索引的顺序来存储的。对于聚集索引，叶子结点即存储了真实的数据行，不再有另外单独的数据页。
B）非聚集索引，表数据存储顺序与索引顺序无关。对于非聚集索引，叶结点包含索引字段值及指向数据页数据行的逻辑指针，该层紧邻数据页，其行数量与数据表行数据量一致。</code></pre>
<p>在一张表上只能创建一个聚集索引，因为真实数据的物理顺序只可能是一种。如果一张表没有聚集索引，那么它被称为“堆集”（Heap）。这样的表中的数据行没有特定的顺序，所有的新行将被添加的表的末尾位置。</p>
<h2 id="聚集索引">聚集索引</h2>
<p>在聚集索引中，叶结点也即数据结点，所有数据行的存储顺序与索引的存储顺序一致。</p>
<p><img title="索引" src="/imgs/sql/index/index2.png" style="display:block;margin:auto" /></p>
<h3 id="聚集索引与查询操作">聚集索引与查询操作</h3>
<p>如上图，我们在名字字段上建立聚集索引，当需要在根据此字段查找特定的记录时，数据库系统会根据特定的系统表查找的此索引的根，然后根据指针查找下一个，直到找到。例如我们要查询“Green”，由于它介于[Bennet,Karsen]，据此我们找到了索引页1007，在该页中“Green”介于[Greane,
Hunter]间，据此我们找到叶结点1133（也即数据结点），并最终在此页中找以了目标数据行。</p>
<p>此次查询的IO包括3个索引页的查询（其中最后一次实际上是在数据页中查询）。这里的查找可能是从磁盘读取(Physical
Read)或是从缓存中读取(Logical
Read)，如果此表访问频率较高，那么索引树中较高层的索引很可能在缓存中被找到。所以真正的IO可能小于上面的情况。</p>
<h3 id="聚集索引与插入操作">聚集索引与插入操作</h3>
<p>最简单的情况下，插入操作根据索引找到对应的数据页，然后通过挪动已有的记录为新数据腾出空间，最后插入数据。</p>
<p>如果数据页已满，则需要拆分数据页（页拆分是一种耗费资源的操作，一般数据库系统中会有相应的机制要尽量减少页拆分的次数，通常是通过为每页预留空间来实现）：</p>
<pre><code>A）在该使用的数据段（extent）上分配新的数据页，如果数据段已满，则需要分配新段。
B）调整索引指针，这需要将相应的索引页读入内存并加锁。
C）大约有一半的数据行被归入新的数据页中。
D）如果表还有非聚集索引，则需要更新这些索引指向新的数据页。</code></pre>
<p>特殊情况：</p>
<pre><code>A）如果新插入的一条记录包含很大的数据，可能会分配两个新数据页，其中之一用来存储新记录，另一存储从原页中拆分出来的数据。
B）通常数据库系统中会将重复的数据记录存储于相同的页中。
C）类似于自增列为聚集索引的，数据库系统可能并不拆分数据页，页只是简单的新添数据页。</code></pre>
<h3 id="聚集索引与删除操作">聚集索引与删除操作</h3>
<p>删除行将导致其下方的数据行向上移动以填充删除记录造成的空白。
如果删除的行是该数据页中的最后一行，那么该数据页将被回收，相应的索引页中的记录将被删除。如果回收的数据页位于跟该表的其它数据页相同的段上，那么它可能在随后的时间内被利用。如果该数据页是该段的唯一一个数据页，则该段也被回收。</p>
<p>对于数据的删除操作，可能导致索引页中仅有一条记录，这时，该记录可能会被移至邻近的索引页中，原索引页将被回收，即所谓的“索引合并”。</p>
<h2 id="非聚集索引">非聚集索引</h2>
<p>非聚集索引与聚集索引相比：</p>
<pre><code>A）叶子结点并非数据结点
B）叶子结点为每一真正的数据行存储一个“键-指针”对
C）叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行。
D）类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的。</code></pre>
<p>聚集索引是一种稀疏索引，数据页上一级的索引页存储的是页指针，而不是行指针。而对于非聚集索引，则是密集索引，在数据页的上一级索引页它为每一个数据行存储一条索引记录。</p>
<p>对于根与中间级的索引记录，它的结构包括：</p>
<pre><code>A）索引字段值
B）RowId（即对应数据页的页指针+指针偏移量）。在高层的索引页中包含RowId是为了当索引允许重复值时，当更改数据时精确定位数据行。
C）下一级索引页的指针</code></pre>
<p>对于叶子层的索引对象，它的结构包括：</p>
<pre><code>A）索引字段值
B）RowId</code></pre>
<p><img title="索引" src="/imgs/sql/index/index3.png" style="display:block;margin:auto" /></p>
<h3 id="非聚集索引与查询操作">非聚集索引与查询操作</h3>
<p>针对上图，如果我们同样查找“Green”，那么一次查询操作将包含以下IO：3个索引页的读取+1个数据页的读取。同样，由于缓存的关系，真实的IO实际可能要小于上面列出的。</p>
<h3 id="非聚集索引与插入操作">非聚集索引与插入操作</h3>
<p>如果一张表包含一个非聚集索引但没有聚集索引，则新的数据将被插入到最末一个数据页中，然后非聚集索引将被更新。如果也包含聚集索引，该聚集索引将被用于查找新行将要处于什么位置，随后，聚集索引、以及非聚集索引将被更新。</p>
<h3 id="非聚集索引与删除操作">非聚集索引与删除操作</h3>
<p>如果在删除命令的Where子句中包含的列上，建有非聚集索引，那么该非聚集索引将被用于查找数据行的位置，数据删除之后，位于索引叶子上的对应记录也将被删除。如果该表上有其它非聚集索引，则它们叶子结点上的相应数据也要删除。</p>
<p>如果删除的数据是该数所页中的唯一一条，则该页也被回收，同时需要更新各个索引树上的指针。</p>
<p>由于没有自动的合并功能，如果应用程序中有频繁的随机删除操作，最后可能导致表包含多个数据页，但每个页中只有少量数据。</p>
<h2 id="索引覆盖">索引覆盖</h2>
<p>索引覆盖是这样一种索引策略：当某一查询中包含的所需字段皆包含于一个索引中，此时索引将大大提高查询性能。</p>
<p>包含多个字段的索引，称为复合索引。索引最多可以包含31个字段，索引记录最大长度为600B。如果你在若干个字段上创建了一个复合的非聚集索引，且你的查询中所需Select字段及Where,Order
By,Group
By,Having子句中所涉及的字段都包含在索引中，则只搜索索引页即可满足查询，而不需要访问数据页。由于非聚集索引的叶结点包含所有数据行中的索引列值，使用这些结点即可返回真正的数据，这种情况称之为“索引覆盖”。</p>
<p>在索引覆盖的情况下，包含两种索引扫描：</p>
<pre><code>A）匹配索引扫描
B）非匹配索引扫描</code></pre>
<h3 id="匹配索引扫描">匹配索引扫描</h3>
<p>此类索引扫描可以让我们省去访问数据页的步骤，当查询仅返回一行数据时，性能提高是有限的，但在范围查询的情况下，性能提高将随结果集数量的增长而增长。</p>
<p>针对此类扫描，索引必须包含查询中涉及的的所有字段，另外，还需要满足：Where子句中包含索引中的“引导列”（Leading
Column），例如一个复合索引包含A,B,C,D四列，则A为“引导列”。如果Where子句中所包含列是BCD或者BD等情况，则只能使用非匹配索引扫描。</p>
<h3 id="非配置索引扫描">非配置索引扫描</h3>
<p>正如上述，如果Where子句中不包含索引的导引列，那么将使用非配置索引扫描。这最终导致扫描索引树上的所有叶子结点，当然，它的性能通常仍强于扫描所有的数据页。</p>
<h1 id="参考">参考</h1>
<blockquote>
<p><a
href="http://www.cnblogs.com/morvenhuang/archive/2009/03/30/1425534.html">数据库进阶系列之一：漫谈数据库索引</a></p>
</blockquote>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>上海天壤智能科技有限公司</title>
    <url>/2020/07/19/%E4%B8%8A%E6%B5%B7%E5%A4%A9%E5%A3%A4%E6%99%BA%E8%83%BD%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="a95a409e4d66b600f4cf4f78b6019a79aeb8dc81e69bf19b13870df97edf63b0">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
        <tag>数据倾斜</tag>
        <tag>XGBoost</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳极光联盟</title>
    <url>/2020/07/22/%E6%B7%B1%E5%9C%B3%E6%9E%81%E5%85%89%E8%81%94%E7%9B%9F/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="9ba5c979679304a5c4a87b3967aabee90551e835a4ce47d7e0ca8d49d0c78c75">5d20b81ac1f5d0b70e9113cacf134e1e0631fbdeb95d2e18e69242aada9459a8c4c9d289ad03a9e0d8b9f5577febb77a1289a33b1c1aa2858f144d451fa114f747046125a80d333e7f74639abb23edd296e3a35fa6fdff280e60a8019e1454fb126c13f07ac42adff90d0c5bebd0a04eeb91fef6f80753bd7b0b5ddc92f75ee0b3eb94ef17633917e2e7b7df33a15624c9c91c2820c584149baa1c0bbeec1a8c1eb6639ccf6eb658ee5e93c4d82ce4936cbcaf7824dad203b6f24fafff576184268e163eb69a51b01bdcf2b9f732a3627fb33e5080e176c63bf468caab406537a48248899a1ee8f4457ba6a09be8785df7bedfca65ec30177f9bb78854638935778b96ed96eaa409fa4e7754d0672dd7d9573282931b2a7b80bd245f08a0125b5189d1b2bdf254fbc288d2068cc8ec5f64cd5f860e531cca273dcbd213eca5831473af62e4018c2c8898c4f9ecdc1d5659da5dd3bca52e35bd54edea71da6e493c1e77d760938b09cce6757556ee46b2779c59f202433ead4cdc91678775d5e167d4d0d6af9c112b42c1b4999e1a3a9e98501f6212aaf524de5bac65baaed45d25db110a9003f41bada1a611463fc48e44e9988b10d4bbb4a6345c02c537ec515323fa5eaa360455b7ceb2edce4d658095c5bc5c1032bf4d454a6e7cfbf3c361997aaa1be8676a2c5ea704bcb590a829bd748a41390241de4c9f2d7f0b6be2c2c829719ac96bb1ab930dc7dbdced9445bc14af0a40c8af2ad81464bc312a271d0df327062bd8d6f67cca9aa4da520716b6680f75a03a4ea148897a96f18d8055c2384db23749da098e04bbf26c47c80fdd61e1fb16e32501f364b7bc461b4acea6ad5b807eee975db08a047d5cce5c0fbec0ed82c3f808186bb19afd31a3ce33c7a10a6fd383712504f9568e2a7b0ef2fdcd987f3dbada915e3f652d9a786380b9d6fec210a899ad02f8f13170357f82dda43052dbc22ee3d95f7cac33363899e8960f49ff491d9b4b196a251a8605d3593203310cc155024798f65d2ba29cc949b74e6982370d286e0b4cff579a9d0a7d8c14d61ea5ecb24076bf93a21a139e549f8319a491807b4a6951512c42b9868577f4239dfa09d4c9b765985ede8bb79eeccc0905baf69d868009d570b0c049b9e1307dbcdebb7087c5ec16f4660db09ed44eea549dfb74170547a745c645fae897e41292fa4c06e19cc3d30ddb66dfbec25876e343110f408a1b6d524b903f02d1d4ca41af5b74d4132a02e67808d39c8ff1b9a9045d8422c506e6dcbaf1feeac064bc5ff21393d7aca84cff5568724d1ac2b7bc6594f40ea7e23a491769cb9f02045effd1c6a61c936c281055846987ca63c04279534cf6d79ac3738b5320b3342b5ef65be0b14baca9300fb3d1516399a7bd3815d045b01b2ffa318ff42bcd5a927e35cbd9f3bf29811aade097095089970ac76cd268cffde2128f4d36f4e13e1724d16d08af0c318bf2142755b893f125aec974ca6bf7fa3e4e2dde563174bd962366176717b49291f10878da2be5237e7a22e3bb05f6becc9ded8cd9dc771d7af8ea0095617b6080bf324389992752d138b6bcf75924267445925ca9d31502c340dea65929d71376e60f3b34b666b1d11c31c88eee7c7f58594b37d6da6fd1d620b7cfd994e375701dc7a4aa3ee59569cf9903813ebf337957361678799266958329c7b71f314b30245ca93214c725f58c278fe64437d1ec42a04dab7b7ca65bc4ce1471f6a65d102470697b3e80d22580db34403f7d2b81aeade612356b73c68818bb340051e3d75ab430dd176fce068eba644fad489f059c0ad9e342863a78d26706c8d9a2979594fa85566f604800413a2d3db3df14468c29dd2753f011121fc71f0a78fa01982e34597800d9da5306538a7b2e5af807ec53aef552d95d13d1927618c9bc1c80a450b1366394f176faf3cacda772f3dea62c0576684b13b2d5bfce0ea800e2c791ff5b77b6c3ae0fdd74d675c3e303fa898899ea68962700fd50b63ae13e47c6e899a45498d6ae3c946e434551c3ac861d8d1d7272b0801327299351d694be1e304c567a0718c8fa8258b981dbf2e08877ce474fd34d195cf6a04c09c1a6855638fd39e488dd53e290d7d006b824e713ff9fc2c6d80accf910fb7a9694c7419a4485079fc45cadc8b56c9a5178530e0790d4bc96bdc9478e06ee097d48ee31b6b3d42b959d74b74036a56e99e285dcdc08a25b1106dcbb51b3fcedd59a5cd6f71f118ed70f8a767da6da6ff0f0532dbf9bd89eb572240ea6077fcc72e66896124bbdc389e6886dc8924f1afd7f1976bc09f88af28524673f75feb8d205d5a55a9ddbcbeeace2f183723d831ded38fc20da3cb006802de1f2d51bacb62952ff926ed64e1f0a9f7f97d81c15d6de5d44094e63cf37a65af042458bbbec87294790fa97eeb85c09edd855b817fb687d811a6e4c725598f38722b4ebea01d09a75bd9aab8dc139988db82bb4ab8b115eb64a4f0e777270f40b74444f9f4136f0cb81008285c0fdaf9a65ddc461dc6fa8c09f9b70dcfa2a0773ffd6a203145a0e54fe2fe06c2179efe16582a8644613bcc074adc7793f1e7ee0e7fff7ed21a3cc7d4090b78a7c1a10c74913fb8bcfd8298181af89a3e21b581b3fb935576a361e636feb975f0496af900594e875335faeb938f2b2da960937459bbbaf60eaceedf35e20fce764e5831222d21f6398f45455ffe5f887143989dbeefadc31fada02ba9cfcb6e63ea28ee69f296e022aea0c4dc3e9c0c39744e750128b4f6cc033d2d7660e380a1cdf9971ff0450d2a467bdfd0b8415714fe3d2b841659fc7e10cbbb086c279b3fef15293beb4ab37f06d4ac25352ed780fc0be5c9a229dbc2517b5a87c20b16818124435460812ed6ae37c5dc480b036636b36ba8a58fbda45c7a21468c5d2cd3d8b65f0f926d85e5259e81be5ac177ce30f8a57d4f81ea12d2ad9d58f5c4c674e8c790b3b9c39ff69df36b4f130f2832823efbaf951e0896e6f62431a5344f3f4cf39970c202fb3a5e471dd55972ef7d32681949b71b4b27c7aa9a618e8ff8eb9dbd238116fcc684706549093670dc91a5c3c6e69ecc59556ad7d4c077df6dd31e128ebb92b0dfdcb8771776626c031bb2c2c8ff9b8adc84a27ba444e227ac01552818d75c08ecbf0a7d9dc754edddc2ddf36a61ebff5b6e566719e88c4e794a88fede96a90d2e2aa8c002918d22c912652f98953189af58b58a7f5620a38fa06f53c49a738c80625f9caa99a64c3fad0e1c07ebbd151dec4e1a907d9b52ca8c9a854d425de9e4b8f87446ba2235efb452ff0e0d2423a2e043308eeff41b0ccc3179e24c21b9e596f809eeccd37a8e02408ad8019509cc5e7638e186ed0e1e5af70f9e95da24e7feced896ef87f2880b1952fc436ffd2a62ceea0d87baaf7940ececac033afc731e4056c572cc2257afc025dee2b84c74954e641f8e2322416f92d505c8822c700f85b56c3f64ae3da333741ec705cf3face127d65e5422fd65d9c7b59e4c8c8fd94e11319899ce1e71ff209db1a3eb9bf205f1b786de3f39f9987a6e210a800c18c942a1ad33f5baa1c54b3166e044b9be77311938d094b9471d821cfdbf81f418072bf1b67ffbc91305725f707ad425d9efb79180654bc4c8675d8b93c1758b85e8c39412b3c16339ff8429f1d11a25bd6b70a689e7abae3d006bb96bba58f0c268947d403c16937b4162e75687505a5222e4f4e92f724e9dbb523245e7d377e60662b8ced88a7e635900dcb919d1cd56c7a896885408aa974061d98abd4c1d44cd7b540030ef95328a1101fa5a588296869620ff29856fbc8f91bf8c5777675b59b3fc03cec9ccba0ea293ef6aeeacf559e4682e9a686b97c79ee79223c43bd5051e2e9becb02aa1dbcb325ec30c17e427acf8a37659b8eb0b42fd62a153de7796e79b05ae4e44d38165b9ab1e9ff9bc9cca9604b58dca666d4acba4862e071cec56c0895a9cd3757ce3a9a075ba5a372cc52e07339e0fc1aa3564fa1de29dd523bebd6f7ccba02ead5cdb9ec3831f2ce4f145bb90250d7df4d90229d2575d375e4d51c28cd8f006a6a5a6bf0e9e781a6bfbb6a21457460de5a0beb3a890e071b23879730b8ac4a3f6c6349b2f70988af4291220e63f329c2afd924c59cb93b03d335418b26d1b3dd846af7e88d6a12eff1075a6f4f313275f91f0f489cc1f1ae4cbecd79ec4ce4f5e2f5f7924c946a7c78b96739862912a4b1855c2d24642df8542b75ed4c125c90425d8f477b3f5a7f2834bcc64f2afb48292a8b4c4c424ab4594846be0dd74734ba5c1f568f0e9a74a7b083ccf07ab5c08c7bfb7df2c1ac938531bf1aed8caf3d2a1d34b2fea89014386f4400974ae542a0b668143bcb983e8f1ed08be1e29157dd001598337aa77a245c868684de2767e808b722538628eaa20c14504009555eb40c9fbe02674673ccc767178b9024f02b25946e03a0b5e7c2e453951adc5d620565c08fba9908899970649e4b4d04e9cbfe53ae00cf0ef7990d1324f5bca411f87751149729348c79702d68dc36ae0177255cf2f6585185270536c61341509a1c411cefb12856a2081825c90eb01fe8892167ecfcf3047be23a72d9a9650d8d157033512ba5031343c8d7c5a8d2ad1a12af37defa15e7c3e9c4c443f94a6879b9a216d902a0134ee244446315e5d005db40cdc09296d5514e657a25d67db10c22b206a26de542c3421173939dadb31b43646a78fb4f4c513cfeab943271d3d72321f9498b4cb9664ad43bd3e565a43e59536bad6c7de3f4d195cc5e36116e76f105674789ade79af1892f7f73a634ad3c2a80b40202a6a4dc16d12927b7f3174778c910865d50c7c2d65950f060e5025c03fddffa7e35eda6fcd88415b8683ae14aab2f1d9b3cc4188de2b25c9b52185a54caa454f833f2996ca92a8e74b6d36527e725e73ed0a13fce39b8d8d4f4779fe2c756f0caf0901c13ccaac403d25ed4b2a2898a506f16df0a341250313e3cf76b8cdcc59ff63687d1ff433106f56eb6a6b9ad59bc1210da2ee3a6477cadabeb169964b31f0735ea206e7a3b093cdb80c280513da1d8fde9c9e08afe41d2ea1f408cf788426dd2f2ffcc8209e1bf5737da3e377f46b55472d0265c7487b9af553a606bdd76c154e22b2393c2cc53852c52d507c615d516927a5b9f035e28c4e4714e439ca75ff5edbb380c98c3cdd07599c4370f2c5ad6271354083a8cd3627171d9e1fa17d2c9c1772f5b83bc24f72941e9fbae3fce454f890405f1b6ed0c7fe94fca686829e2b83dde8d771affa4d57ea7b3458f3c151f43a406ef9be0aab93eeca1ffd073d494ee9e6d8060e7d8cbf2f9caac55ad1cc5ab4812a48bfc0f640ca7e0b895c870c13b679d38206a1cc1da3a7f851d680e6a733eca3d5ee72b64e9298fa5039497b0d68fe93a7ff950144487d072137fe81f756943733771bf85e70d6cdaffd4a6c0d6e53d26280a7d898db1aaa64dfefeb5a289447b7e0db14952c774b335a24cb56e986df9831cb3a5b1404dc0e27d940247241fb0b828c91366e7508e1ef1b37623379ba1eddd282526026cff56f4bdbfed70eccdbf04cc09e28a8ea9f02c2d1809a1e9bb7208923d14498eea4fe447ac626b02697315fc9f4afc6112e05380b9b91da29ade913bd0e9649fa3a8d93d503a02c29ce1e8c6a7355274291a9b617aea07d4af72c40f621ebd03d463c8ed2d24021e2eff0feee56f90bdfcf6a3f7616adc96890b5c8084e8ad8b2879f7fbd537399c99326ecd647061efee92eb37b46e55bfb3399ebebde945664e522f41dc483bc26c09664d2f22c4cd8969f78a1cf82b1bad4a727053633a83c99ddca57bc407f37ee1df73798ffe303657203a856dfc08f3ed8fd93b31dce47b0d8006c8b748a038ab866ac5a7959123d5bed1b12f29ddb5d2724dc57ae33687f49195b44efe46672bc41ecfd5ce4f88091ae03a15bae6fa713206801fb124621bd8879ea0adc418566216fdb406782523498400aa90f54771100894472b8b42287d1525943f8ac081826e54ef872709badb3db47afb5a128a240f034355c5a730c0e826c10dc4a8c8a242908aecf895d30289e4502b0695cc60afd603b2bc68d536931a92343e77c6d1fbd66b0cb51651a82ff724d713ea8a893483e8958c74e22d8441fce8798194744d359d5d4dc51dab692038ddcf85622f607e035f9fd06f3980f6206c5a6ae7a91232c9442f35c597313c45a9a686fbd7b9aa0872892da3f3951844be677fc1cd4c3610b39563d47f44fda8021a46a32aced4f8039d5db5441005947e0d0029f7057c3577aee71a197f73188688eee1c83be5ebcb3ad169b91f0ff06484f9724113a0c838ae70d3403ac8cb5b5d2463ad32ce7f45980ecccf2263cd284c4b48c0b467e5c4be8395836a6706de0e5d2a462fd7d510f57f219972bf79bb2a0097887ed6359ad2e0efe4db1fc1de2126ff7a6f58d0f02dd1a7a55bccb3a7c1e00f75dff13a38784c4725cdf9f847f0f345f123fdd347f142863652c4dcfe65d4ecd3e624405a5b39c2fdeacd7e95bd1437275027906352730f44308cda7a758b264ddb51cbba12adf25c470772c928a64a8ccc1fccddec4c2dc3d51d9c78c2c5e0da2d7a8e2ccfe6ff80e41784e6deb85b6c31a3c1b9e285eb90ecdff0d8be05fd4d618cb4abe5a2b0dc500b7d03915bb081d02257617ebbb511642c055d1f5b851d2a00d9c6bf95ae17d621e6519ebeca8d53cf3ade29a0222fb18bbd0ccc5f5ec266ff62f10346f15f46b599d044dd269e860c0657cc56f0ab18373cdeacca5b565ddeed14d485e23b965b3f516068afdbe79531a772debb0ccdadb74f64d1040a804b62ec7724f8a6620688f4eff2d083af6d763664209aa9540fa4082c513c2505913a4d25f2744109e0cb71d94426e4649f97c86ae6058938883a650e010ac38c93bfed818cf007aea1196fbac000e357483d9c6508e0c037c1436f21f75a5bd09a91c6989b334b1ecc56ee86cc0b94d54ab07cd507159cdd3d762a4b633f3df24e0d07b9007372bb24dfa2b65fa2073610a2c32147be72904ab32c749eade2c81bd8b6d316e3861a7a0c01c0de85c64714644c11d5a15e242f68482efc3a32e12fbd82ab8c29ad04b2a508a422cad3b8c7b61b70bb602d7b58280b70c30e2aa00d2059d24767ccd0c6d3233afc6a38652cdafcb312430da86786ad78a7178926a6250b56d69fd8d9e00786b6961fec8c3f754b3f00dda332928098e99433faa85479644bd8fd608bbd6260f1cd1a8983cdd774b7646e2c7dcc5f83e942b057da28048ffad999fdca4a75cd55aa6bf59a8e842fcb4692111eee0338f5437daecff8185464f857a0fbd17a3fc6720313ae04c81ae49e365c14f27b4f13fa507c38b6d834d8329c3ed42906a8762ab7b6dd375cff16e2e0fbe32aa2af5c4dc51c39c8d9de06317b5a3e78b2023333e3b8d9dbcdd7fb7c3c584879f3e27a6637f0ea5fc2fd2f9c037b64989f614ddf72998aa957eb8756f4f68ad1900f769fc0880877c30bc38a09ac214cca7fd3f7689f7b9d9f27881a566461939d5797cae7a4fc10c23047bb7b1af8062048bde565b31209d8f6c29d84ca8785f21096a6b4cefc246af4adff9ab53170832762548db856b3beccd4f25885d97a78835ab65429b3904aaf50eacadcc65f5ebf8725b3e7c8d7f9204668cf071841e7e9a831717782001dd61571df3c0ad6b9d90c6af406837d6700b3b8c218761055ac8291fd3db52938e6a52dcf5d3a0d694ffc3a216abd7ffafaf3a1090890e051a3fe0b0fb395fe1bec5e79ae8a185e040705a6e4ed2ae3233bee05411d569e32a49397fdfc1c4252b532ecdc5f4162ab3b7efce6867d8aff6c7ca12b81f9b93909e861b3f45c14781a359ffbc9331c8d16a9957cfef1d4531900523dff1a01c329dc4bb326a53efe3eb2e524872be495d285b34d9a229e46e3854bd9347eb074265e63eaccd07c6c6b0505ace8bcf7f4328b511e9516e040452f13d9494c543f2d9ff9c7c557d86c68d52aa71a487ddb6ce4bb12b2ea54511255fea08b44db0f852333455cab0935e8651492e22654cfcbf99d596d844ab4b86e02177e09b770fb3178ba7bad4d7ca9222640f391491f6682e7bd3b5f4891955e732afebe6938e76f5f53354698b2838cee8d031bd3a6a2d6cdc102bbf9b9efc5e8d85fb75ecece129ec2dbf4e316fe3880b8a27fca40a77bfe1e10ff88aa42497b10cec3d9f0f2878b0480a64036078ae19ab97cab89e2dad8079dcd32af25b683896d34d092e27387888d1b466c0a762db1ab3fb227ffc22dbd5e9fbc95f39fae1a9cc255d5e7bb85089deff4a737469cab76834292da0136b5ab8e15d7b90694febb356d6a851bdc9fa889e4a3635aaac6bb433632c53f83df6baf2b839b58910262ef089cad36c464e0bd3ccac216270da0be121652f0953622f5919d43d62da85bc0387d0f0aae5d6712a599971fe5eeb3111b155a389204074b9b9b09ef266e1f684cb698e6d7b0b7c40e6c21aa65d92fc865a440916e1b569978e73110de28c01776f3470415705ba9fbdee202c2113ff24e3206cf2e7a1d37dc5c3505af3fe9d6aa2982588251daf88578d413dd819ddacaf804a023f84214b06bc2f5a6085cdb81ed2ad0ce01333fa0e062516d50881e5935b04fd8d24a6ef1dd43c889cdd69d7ad932b81bb2d5f6409a59c0b2333e7d2b8d6f663299394a5226b2808814cb539b915b1c4fc8af4570f77df6a5da094da05c06a8d10edcf10dd2604d00c76e3a60b60d0b23e6c54f1d45a6e258686095de41edd5b16d998d856ed30fe4fff50b01a12fc2669b28e181cf44316a944b7cb8ea9f360ef679543219b36e023b7c5d3a324bed56339236b5acc456e27a95aaf493bbc09bdc33d421e6a6e86cfbb57e86bb4ed6d7f57d5154442de62cdf4bc4d94d331533f81a1372af533b006f0e831277fa3a4df67cf738131616d6aeac6cb0d7d107b7cd901e64fbd28637912bf3615921052196d0b788fc3b95e3de46d8c8b49e6fb5fb45c7815e656d6f1a3b482d4629fd83a8769dcb37250321da21c21c5837918dd66a5b8e07e11c8a9b5629e23d94c585df7765c98ea795cf1c60712db53931f38ba73360f87815acadbddb0dcab297d4785eadc5c037567a3f1d72409c0b321f35874bf8d29feca2393e0507591396cb060a890def034b2ad38be811d96bd78d92deedb0d329aec66439f582023dcfc3553d3df46e41cf6afe28698461e95d7ef9dacca335a65a25b29b7f377b2755f1b2d9c853eaf6d470a6201a32a87ff032bf15b50acb2931feb2ce07bf89d2f75a5c0f2e2b544dc3caaa5bcc54e9a97878a828db568c2415738b30c2f1df0ee15e6c001bba60c4f2982ccd534327a15d5406d27ae897a9d7b56088bc15dc0ed7a67393e5a6aaa2b821a09aac956881100708c09c4cf543195a84b684e28a29b6e46aa925482dbe1ab3c085793d03fcb06bb73752d53f4bb560d31a24a8c0323c0687f1f72752b4edd64654fb1720e7404e490872f813dfde7dc9e9f9c1c8c14e15c15fb52c062500c3a6b6b12ca02dad7e6e7bde92e229541ec5624fe51611c976b2a82f5879cc73835c48bb1b91b11bbb61ac31623b38f409624d6d2b051c694d2df4fc09a7350235da5f5f8762b41ee6b11de06c184b5fbf6f05b617011960a710db48cab8540900d3b3e4ad4db8e6c499867954172a00c64f2c9a19deff903aa229abb0838e07f5801b2983a9bfd51b9d2b443805f6c0acbaf28dc143dd0543c5a41497aa51885ba91590cb3e334a681edaa03f3a7dd1e7c2c6513718f20c4fa2fd7bd85a4c8d025cc3bca3130bb2bd06a5d7b10e7924eca026f452c250d660abca14f3f5638f9e8dc2639e1aa2c24d36c39a77f67e206959321ddc93ea567db7f022452f17863fa758666d57525bac2354c0cb5818b741b2c5b1a74f03b9b589f298187dc81d0b432a4ac0c89be028cebb53c5b864ac4ebf8b0030c2aad2e2dfc8a2e3c62a794453123bb6d71e8a945619f02de0ef67333e77e3311e353f0b314aebe12b0555cd1c1b2503d7edf33055a3f29b04ce198a4062e9b6f672f4cd57b4a9c4682d8b7e97b0affbb699c43a30f704efbf43867f0d1af777485051a26997b0a0f58b7d5020fc3ae6585036d1d43c454aaeb04e2f5d9a1f35e0393d58795c52624fc824a73995503fa11dc4155afad04ba5225e7da955e6f98dd3a85dd2db68ceb68caeae5c836e27e116cfd80f9794326a7cf51c7447a41395dd5cd85a72c6b2f0f7c7a97ff0739cf2233be6fcaae6ba9c77aba635b0a4da350f43a63687b3ae2d7a429a3282adf7319552cb89aabe28e185f806e6dbe344352aed9b25d60668a712e77a7aa69b6669d396c6af9fd265fec8208d0eb7869447b76d709c68828380b9e90c11f1e28cc566c8568943c21ca836c734cad3d0eb655710f4a1cccd9cb6ed663ec2e2ab7752cdd43f550ba0878e8f327eb3d6e04271e177e4d515f0858f142f9058b99468c62a487c2caf6a9201fd5f84ce53fc9184ece6d94b126ddb8c41efedbe285bf46f4c13c3372c4a7b00d56dea271aaf45c142425a0d044fb6bf6c88ae4cb86c8a4e11c1a0aae2863d8e8219151c74191440e15cf914f2c44875320d3f7977f9c0669a273214717a246fc01e7f8d77348f73ac634818b25f20c522307dc36b0d4b4b1b99315ccc2f06f7c1b60cd50d36a77e9debf210ef1def3895278bb9fa722f51a53c3c9d04d678d95ec6f3e40b0891af1994845d25b9d1d406e9016f6c62532540305157b824c1b5da23a97a6abc6d5b2c66e7c3354101baf420dc14721a6a9d15dfde696363f768bafca9661fbf49cd00807dc898cfcb219acf9f8d72f053c579625597e6340643378665979f2dea7ca35ad2e5a027baf4b136acc38deec20367b7b5e79e71390e709eb0be3775700e94f51f707381be9fb15f8af087a5428dc36a36037e381df4e0790b80f99b95c28a847ab61fa3e9e063cf8733d69d7741d5b6fc94180674bf6a0152dd610812ca4d936e221836ea456fa2a811bbf7fd3c907999f5365d3c96347e04c8414515b2ee4371dfdfb425</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>lstm</tag>
        <tag>时间复杂度</tag>
        <tag>时序特征</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>python import导包路径</title>
    <url>/2022/02/10/python-import%E5%AF%BC%E5%8C%85%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>本文主要分析python中import文件路径的问题。</p>
<span id="more"></span>
<h1 id="基本概念">基本概念</h1>
<ul>
<li><code>module</code>：一个<code>.py</code>文件</li>
<li><code>package</code>：一个包含<code>.py</code>文件的文件夹</li>
</ul>
<h1 id="搜索路径">搜索路径</h1>
<p>python导入其他包或者模块时，会根据<code>sys.path</code>中的路径进行搜索，这些路径主要为一下三种:</p>
<ol type="1">
<li>当前执行脚本所在路径</li>
<li>python内置标准库路径，PYTHONPATH</li>
<li>安装的第三方模块路径。</li>
</ol>
<h1 id="import步骤">import步骤</h1>
<p>python所有加载的module信息都存放在<code>sys.modules</code>结构中，当import一个模块时:</p>
<ol type="1">
<li>如果是<code>import A</code>，检查<code>sys.modules</code>中是否有A，如果有则不加载，如果没有，则为A创建module对象，并加载A</li>
<li>如果是 <code>from A import B</code>，先为 A 创建 module
对象，再解析A，从中寻找B并填充到 A 的 <code>__dict__</code>中</li>
</ol>
<h1 id="相对导入与绝对导入">相对导入与绝对导入</h1>
<p><strong>绝对导入与相对导入的形式：</strong></p>
<ul>
<li><p>绝对导入的格式为 <code>import A.B</code> 或
<code>from A import B</code></p></li>
<li><p>相对导入格式为 <code>from . import B</code> 或
<code>from ..A import B</code>，.代表当前模块，..代表上层模块，...代表上上层模块，依次类推。</p></li>
</ul>
<p><strong>顶层结构：</strong></p>
<p>在没有明确指定包结构的情况下，Python 是根据
<code>__name__</code>来决定一个模块在包中的结构的:</p>
<ul>
<li>如果是 <code>__main__</code>则它本身是顶层模块，没有包结构</li>
<li>如果是A.B.C 结构，那么顶层模块是 A。</li>
</ul>
<blockquote>
<p><strong><code>__name__</code>变量</strong>作为python的内置变量，是每个python模块必备的属性，可以取两种值:</p>
<ul>
<li>当你直接执行一段脚本的时候，这段脚本的
<strong><strong>name</strong></strong>变量等于
<strong>'<strong>main</strong>'</strong></li>
<li>当这段脚本被导入其他程序的时候，<strong><strong>name</strong></strong>
变量等于脚本本身的名字。</li>
</ul>
</blockquote>
<p><strong>导包原则：</strong></p>
<ol type="1">
<li>如果是绝对导入，一个模块只能导入自身的子模块或和它的顶层模块同级别的模块及其子模块</li>
<li>如果是相对导入，一个模块必须有包结构且只能导入它的顶层模块内部的模块</li>
</ol>
<p>如果一个模块被直接运行，则它自己为顶层模块，不存在层次结构，找不到其他的相对路径，因此存在相对导入语句的模块，不能直接运行。</p>
<p>Python2.x 缺省为相对路径导入，Python3.x
缺省为绝对路径导入。绝对导入可以避免导入子包覆盖掉标准库模块（由于名字相同，发生冲突）。如果在
Python2.x 中要默认使用绝对导入，可以在文件开头加入如下语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br></pre></td></tr></table></figure>
<p>**from __future__ import absolute_import**</p>
<p>这句 import 并不是指将所有的导入视为绝对导入，而是指禁用
<code>implicit relative import</code>（隐式相对导入）, 但并不会禁掉
<code>explicit relative import</code>（显示相对导入）。</p>
<p><strong>关于隐式相对导入、显示相对导入和绝对导入</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">thing</span><br><span class="line">├── books</span><br><span class="line">│ ├── adventure.py</span><br><span class="line">│ ├── history.py</span><br><span class="line">│ ├── horror.py</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ └── lovestory.py</span><br><span class="line">├── furniture</span><br><span class="line">│ ├── armchair.py</span><br><span class="line">│ ├── bench.py</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── screen.py</span><br><span class="line">│ └── stool.py</span><br><span class="line">└── __init__.py</span><br></pre></td></tr></table></figure>
<p>如果在 stool 中引用 bench，则有如下几种方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bench <span class="comment"># 此为 implicit relative import</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> bench <span class="comment"># 此为 explicit relative import</span></span><br><span class="line"><span class="keyword">from</span> furniture <span class="keyword">import</span> bench <span class="comment"># 此为 absolute import</span></span><br></pre></td></tr></table></figure>
<ul>
<li>隐式相对就是没有告诉解释器相对于谁，但默认相对与当前模块；</li>
<li>显示相对则明确告诉解释器相对于谁来导入</li>
</ul>
<p>以上导入方式的第三种，才是官方推荐的，第一种是官方强烈不推荐的</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>manjaro配置</title>
    <url>/2022/03/19/manjaro%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录manjaro使用过程中的一些配置！</p>
<span id="more"></span>
<h1 id="zsh-配置">zsh 配置</h1>
<h2 id="安装">安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh</span><br><span class="line">sh install.sh</span><br></pre></td></tr></table></figure>
<h2 id="themes">themes:</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.zshrc</span><br><span class="line">ZSH_THEME=<span class="string">&quot;ys2&quot;</span></span><br></pre></td></tr></table></figure>
<p>其中<code>~/.oh-my-zsh/themes/ys2.zsh-theme</code>的部分内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Conda info</span></span><br><span class="line"><span class="built_in">local</span> conda_info=<span class="string">&#x27;$(conda_prompt_info)&#x27;</span></span><br><span class="line"><span class="function"><span class="title">conda_prompt_info</span></span>() &#123;</span><br><span class="line">  <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$CONDA_DEFAULT_ENV</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -n <span class="string">&quot;(<span class="variable">$CONDA_DEFAULT_ENV</span>) &quot;</span></span><br><span class="line">  <span class="keyword">else</span> </span><br><span class="line">    <span class="built_in">echo</span> -n <span class="string">&quot;(base) &quot;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PROMPT=<span class="string">&quot;</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[blue]%&#125;#%&#123;<span class="variable">$reset_color</span>%&#125; \</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[yellow]%&#125;%~%&#123;<span class="variable">$reset_color</span>%&#125;\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;hg_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;git_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;svn_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;venv_info&#125;</span>\</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$fg</span>[yellow]%&#125; <span class="variable">$&#123;conda_info&#125;</span>\</span></span><br><span class="line"><span class="string">\</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[red]%&#125;$ %&#123;<span class="variable">$reset_color</span>%&#125;&quot;</span></span><br></pre></td></tr></table></figure>
<p><code>~/.condarc</code>中增加：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">changeps1: False</span><br></pre></td></tr></table></figure>
<h2 id="所有可读目录颜色">所有可读目录颜色</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.dir_colors</span><br><span class="line">OTHER_WRITABLE 01;34</span><br><span class="line"></span><br><span class="line">vim ~/.zshrc</span><br><span class="line"><span class="built_in">eval</span> `dircolors <span class="variable">$HOME</span>/.dir_colors`</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>manjaro</tag>
      </tags>
  </entry>
  <entry>
    <title>jupyter notebook配置</title>
    <url>/2022/04/02/jupyter-notebook%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录使用jupyter notebook过程中的配置问题！</p>
<span id="more"></span>
<h1 id="jupyter-notebook好用的插件">Jupyter notebook好用的插件</h1>
<ol type="1">
<li><p>安装<code>jupyter notebook</code>插件工具栏
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure></p></li>
<li><p>添加插件选项工具栏到<code>jupyter notebook</code>页面中
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter contrib nbextension install</span><br></pre></td></tr></table></figure></p></li>
<li><p>选择好用的插件</p>
<ul>
<li>Hinterland：代码自动填充</li>
<li>Skip-Traceback：省略繁杂的错误提示信息</li>
<li>Live Markdown Preview：Markdown实时渲染</li>
<li>Highlighter：摘选高亮</li>
<li>Spell Checker：拼写错误检查</li>
<li>Code prettify：格式化代码</li>
<li>Codefolding：代码折叠</li>
<li>ExecuteTime：代码执行时长</li>
<li>Table of Contents：自动生成目录</li>
<li>Variable Inspector ：这是一个查看变量的插件</li>
</ul></li>
</ol>
<h1 id="更改code字体">更改Code字体</h1>
<ol type="1">
<li><p>找到控制字体的css文件： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/home/huangyedi2012/.conda/envs/pytorch/lib/python3.9/site-packages/notebook/static/components/codemirror/lib</span><br></pre></td></tr></table></figure></p></li>
<li><p>修改css文件：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.CodeMirror</span> pre<span class="selector-class">.CodeMirror-line</span>,</span><br><span class="line"><span class="selector-class">.CodeMirror</span> pre<span class="selector-class">.CodeMirror-line-like</span> &#123;</span><br><span class="line">  <span class="comment">/* Reset some styles that the rest of the page might have set */</span></span><br><span class="line">  -moz-<span class="attribute">border-radius</span>: <span class="number">0</span>; -webkit-<span class="attribute">border-radius</span>: <span class="number">0</span>; <span class="attribute">border-radius</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">border-width</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">background</span>: transparent;</span><br><span class="line">  <span class="attribute">font-family</span>: <span class="string">&#x27;Courier New&#x27;</span>;</span><br><span class="line">  <span class="attribute">font-size</span>: inherit;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">white-space</span>: pre;</span><br><span class="line">  <span class="attribute">word-wrap</span>: normal;</span><br><span class="line">  <span class="attribute">line-height</span>: <span class="number">1.5</span>;</span><br><span class="line">  <span class="attribute">color</span>: inherit;</span><br><span class="line">  <span class="attribute">z-index</span>: <span class="number">2</span>;</span><br><span class="line">  <span class="attribute">position</span>: relative;</span><br><span class="line">  <span class="attribute">overflow</span>: visible;</span><br><span class="line">  -webkit-tap-highlight-<span class="attribute">color</span>: transparent;</span><br><span class="line">  -webkit-<span class="attribute">font-variant-ligatures</span>: contextual;</span><br><span class="line">  <span class="attribute">font-variant-ligatures</span>: contextual;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
        <tag>notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>recurrent neural network</title>
    <url>/2022/04/26/recurrent-neural-network/</url>
    <content><![CDATA[<h1 id="gru">GRU</h1>
]]></content>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2022/04/26/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[
]]></content>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>多标签分类</title>
    <url>/2022/09/27/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>多标签分类相关算法</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>mlc</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow常见问题汇总</title>
    <url>/2022/08/31/tensorflow%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>tensorflow 常见问题汇总</p>
<span id="more"></span>
<h2 id="save-flags">save flags</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_configuration</span>(<span class="params">FLAGS, filepath</span>):</span></span><br><span class="line">    flag_dict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name,value <span class="keyword">in</span> FLAGS.__flags.items():</span><br><span class="line">        value = value.value</span><br><span class="line">        flag_dict[name] = value</span><br><span class="line">    json_str = json.dumps(flag_dict, indent=<span class="number">4</span>)</span><br><span class="line">    tf.logging.info(json_str)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        fw.write(json_str)</span><br></pre></td></tr></table></figure>
<h2 id="logging-twice">logging twice</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logger = tf.get_logger()</span><br><span class="line">logger.propagate=<span class="literal">False</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>logging</tag>
        <tag>config</tag>
      </tags>
  </entry>
  <entry>
    <title>面试问题2</title>
    <url>/2022/07/19/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-cj/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="892f7be8b091d68ca5a7367b08df2327da9808e66a848dc746cfd737f7f97e87">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>面试问题1</title>
    <url>/2022/07/18/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-qw/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="411962825e01a849a78e4afe3ed9a0ef9653a15009b2045d1e5079ca29be84f7">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>active learning 主动学习</title>
    <url>/2022/10/18/active-learning-%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><strong>Deep Active Learning：</strong> Pool-based AL selects most
informative data iteratively from a large pool of unlabeled i.i.d. data
samples until either the basic learner(s) reaches a certain level of
performance or a fixed budget is exhausted.</p>
<span id="more"></span>
<h1 id="querying-strategies">Querying Strategies</h1>
<h2 id="uncertainty-based">uncertainty-based</h2>
<p>Uncertainty-based DAL selects data samples with <strong>high
aleatoric uncertainty or epistemic uncertainty</strong>.</p>
<ul>
<li><strong>Aleatoric uncertainty</strong> refers to the natural
uncertainty in data due to influences on data generation processes that
are inherently random.</li>
<li><strong>Epistemic uncertainty</strong> comes from the
modeling/learning process and is caused by a lack of knowledge.</li>
</ul>
<p><strong>Typical methods：</strong></p>
<ol type="1">
<li><strong>Maximum Entropy (Entropy)</strong> selects data x that
maximize the predictive entropy.</li>
<li><strong>Margin</strong> selects data x whose two most likely labels
have smallest difference in posterior probabilities.</li>
<li><strong>Least Confidence (LeastConf)</strong> selects data x whose
most likely label ŷ has lowest posterior probability</li>
<li><font color='red'><strong>Bayesian Active Learning by Disagreements
(BALD) </strong> chooses data points that are expected to maximize the
information gained from the model parameters ω, i.e. the mu- tual
information between predictions and model posterior: αBALD (x, M) = HM
[y|x] − Ep(ω|Dl) [HM[y|x, ω]].</font></li>
<li><font color='red'><strong>Mean Standard Deviation (MeanSTD)</strong>
maximizes the mean standard deviation of the predicted probabilities
over all k classes: αMeanSTD (x, M) = 1 k Pk pVarq(ω)[p(y = k|x,
ω)].</font></li>
<li>DeepFool Active Learning method (AdvDeepFool)</li>
<li>Generative Adversarial Active Learning (GAAL)</li>
<li>Bayesian Generative Active Deep Learning (BGADL)</li>
<li>Batch Active learning by Diverse Gradient Embeddings (BADGE)</li>
<li>Loss Prediction Loss (LPL)</li>
</ol>
<h2
id="representativenessdiversity-based">representativeness/diversity-based</h2>
<p>Representative/diversity-based strategies select batches of samples
representative of the unlabeled set and are based on the intuition that
the selected representative examples, once labeled, can act as a
surrogate for the entire dataset.</p>
<p>Typical methods：</p>
<ol type="1">
<li>KMeans</li>
<li>CoreSet</li>
<li>Cluster-Margin</li>
<li>Active-DPP</li>
</ol>
<h2 id="combined-strategies">combined strategies</h2>
<p>Due to the demand for larger batch size (representative/diversity)
and more precise decision boundaries for higher model performance
(uncertainty) in DAL, combined strategies have become the dominant
approaches to DAL. It aims to achieve a trade-off between uncertainty
and representativeness/diversity in query selection.</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a href="https://arxiv.org/pdf/2203.13450.pdf">Zhan, Xueying, et al.
"A comparative survey of deep active learning." arXiv preprint
arXiv:2203.13450 (2022).</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>active learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic Regression</title>
    <url>/2021/06/25/Logistic-Regression/</url>
    <content><![CDATA[<p>本文主要参考Andrew Ng老师的Machine
Learning公开课，并用《机器学习实战》中的源码实现。</p>
<span id="more"></span>
<h1 id="logistic-regression基本原理">Logistic Regression基本原理</h1>
<h2 id="logistic分布">Logistic分布</h2>
<p>Logistic Distribution的密度函数和概率分布函数如下：</p>
<p><span class="math display">\[\begin{equation}
f(x)=F&#39;(x) = \frac{e^{-(x-\mu)/\gamma}} { \gamma
(1+e^{-(x-\mu)/\gamma})^2 }
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
F(x) = P(X \leqslant x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}
\end{equation}\]</span></p>
<p>上式中$ <span class="math inline">\(是位置参数，\)</span> &gt; 0
$是形状参数。</p>
<p>下图是不同参数对logistic分布的影响，从图中可以看到可以看到 $ $
影响的是中心对称点的位置，$ <span
class="math inline">\(越小中心点附近增长的速度越快。而常常在深度学习中用到的非线性变换\)</span>
sigmoid <span class="math inline">\(函数是逻辑斯蒂分布的\)</span> ,
$的特殊形式。</p>
<p><img title="不同参数对logistic分布的影响（图片来源维基百科）" src="lr-distribution.png" style="display:block;margin:auto" /></p>
<h2 id="二项logistic-regression模型">二项Logistic Regression模型</h2>
<p><img title="分类数据示例" src="lr-classify-data.png"  style="display:block;margin:auto" /></p>
<p>逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。对于上图中的数据，逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。
&gt;分离边界的维数与空间的维数相关。如果是二维平面，分离边界就是一条线（一维）。如果是三维空间，分离边界就是一个空间中的面（二维）。如果是一维直线，分离边界就是直线上的某一点。</p>
<p>假设输入的特征向量为$ x R^n <span class="math inline">\(，\)</span> Y
<span class="math inline">\(取值为\)</span> 0，1 <span
class="math inline">\(。对于二维的空间，决策边界可以表示为\)</span>
w_1x_1+w_2x_2+b=0 <span
class="math inline">\(，假如存在一个例子使得\)</span>
h_w(x)=w_1x_1+w_2x_2+b&gt;0 <span
class="math inline">\(，那么可以判断它类别为\)</span> 1
$，这个过程实际上是<strong>感知机</strong>，即只通过决策函数的符号来判断属于哪一类。</p>
<p>而逻辑回归需要再进一步，它要找到分类概率$ P(Y=1)<span
class="math inline">\(与输入向量\)</span> x <span
class="math inline">\(的直接关系，然后通过比较概率值来判断类别，而刚好上文中的`logistic
function`能满足这样的要求，它令决策函数的输出值\)</span> w^Tx+b = log
<span class="math inline">\(，求解这个式子得到了输入向量\)</span> x
$下导致产生两类的概率为：</p>
<p><span class="math display">\[\begin{equation}
P(Y=1|x)=\frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}}
\label{eq:logistic1}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(Y=0|x)=\frac{1}{1+e^{w\cdot x+b}}
\end{equation}\]</span></p>
<p>其中$ w <span class="math inline">\(称为权重，\)</span> b <span
class="math inline">\(称为偏置，其中的\)</span> w⋅x+b <span
class="math inline">\(看成对\)</span> x <span
class="math inline">\(的线性函数，有时候为了书写方便，会将\)</span> b
<span class="math inline">\(写入\)</span> w $，即 $ w=(b,w_1,…,w_n) $
，并取$ x=(1,x_1,…,x_n) <span
class="math inline">\(。然后对比上面两个概率值，概率值大的就是\)</span>
x $对应的类。</p>
<p>又已知一个事件发生的几率<code>odds</code>是指该事件发生与不发生的概率比值，二分类情况下即$
= <span class="math inline">\(。取`odds`的对数就是上面提到的`logistic
function`，\)</span> logistic(P(Y=1|x))=log=w⋅x
$。从而可以得到一种对逻辑回归的定义，</p>
<p><strong>输出$ Y=1 <span
class="math inline">\(的对数几率是由输入\)</span> x <span
class="math inline">\(的线性函数表示的模型，即逻辑斯蒂回归模型(李航.《统计机器学习》)。**而直接考察公式\)</span><span
class="math inline">\(\eqref{eq:logistic1}\)</span>$可以得到另一种对逻辑回归的定义，</strong>线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型(李航.《统计机器学习》)。**因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
<p>有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$
P(Y=1|x)=h_w(x) <span class="math inline">\(，似然函数为\)</span>
<sup>{y</sup>{(i)}}[1-h_w(x^{(i)})]<sup>{(1-y</sup>{(i)})}
$，对数似然函数为</p>
<p><span class="math display">\[\begin{eqnarray}
L(w) &amp; = &amp; \sum\_{i=1}^{N}\log P(y^{(i)}|x^{(i)};w) \\\\
&amp; = &amp; \sum\_{i=1}^{N}[y^{(i)}\log
h\_w(x^{(i)})+(1-y^{(i)})\log(1-h\_w(x^{(i)}))]
\end{eqnarray}\]</span></p>
<h1 id="优化方法">优化方法</h1>
<p>优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是</p>
<p><span class="math display">\[\begin{eqnarray}
min J(w) &amp;=&amp; \min \frac{1}{m}
\sum_{j=1}^{m}Cost(h\_w(x^{(i)}),y^{(i)}) \\\\
&amp;=&amp; \min {-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}\log
h\_w(x^{(i)})+(1-y^{(i)})\log(1-h\_w(x^{(i)}))]}
\end{eqnarray}\]</span></p>
<h2 id="梯度下降法">梯度下降法</h2>
<blockquote>
<p>最大似然估计就是要求得使$ J(θ) <span
class="math inline">\(取最大值时的\)</span> θ <span
class="math inline">\(，但因此处的\)</span>
Cost(h_w(x<sup>{(i)}),y</sup>{(i)}) <span
class="math inline">\(添加了一个负号，所以必须用梯度下降法求解最佳参数。但若此处的\)</span>
Cost(h_w(x<sup>{(i)}),y</sup>{(i)})
$没有添加负号，则需要用梯度上升法求解最佳参数。</p>
</blockquote>
<p>先把$ J(w) <span class="math inline">\(对\)</span> w_j <span
class="math inline">\(的一阶偏导求出来，且用\)</span> g <span
class="math inline">\(表示。\)</span> g $是梯度向量。</p>
<p><span class="math display">\[\begin{eqnarray}
g_j &amp;=&amp; \frac{\partial J(w)}{\partial w\_j}\\\\
&amp;=&amp; -\frac{1}{m}\sum_{i=1}^{m}(\frac{y^{(i)}}{h_w(x^{(i)})}
h_w(x^{(i)}) (1-h\_w(x^{(i)}))(-x_{j}^{(i)}) + (1-y^{(i)})\frac
{1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x\_j^{(i)}) \\\\
&amp;=&amp;
-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h\_w(x^{(i)}))x\_{j}^{(i)})
\end{eqnarray}\]</span></p>
<p>梯度下降是通过$ J(w) <span class="math inline">\(对\)</span> w
$的一阶导数来找下降方向（负梯度），并且以迭代的方式来更新参数，更新方式为</p>
<p><span class="math display">\[\begin{eqnarray}
w^{k+1}_j &amp;=&amp; w^k_j+α(-g_j)\\\\
&amp;=&amp;w^k\_j+α
\frac{1}{m}\sum\_{i=1}^{m}(y^{(i)}-h\_w(x^{(i)}))x\_{j}^{(i)}
\label{eq:lr-gd}
\end{eqnarray}\]</span></p>
<p>$ k <span
class="math inline">\(为迭代次数。每次更新参数后，可以通过比较\)</span>||J(w<sup>{k+1})−J(w</sup>k)||<span
class="math inline">\(或者\)</span> ||w<sup>{k+1}−w</sup>k ||<span
class="math inline">\(与某个阈值\)</span>
$大小的方式来停止迭代，即比阈值小就停止。</p>
<blockquote>
<p>如果采用梯度上升法来推到参数的更新方式，会发现式子与公式<span
class="math inline">\(\eqref{eq:lr-gd}\)</span>完全一样，所以采用梯度上升发和梯度下降法是一样的。</p>
</blockquote>
<h2 id="随机梯度下降法">随机梯度下降法</h2>
<p>从上面梯度下降法中的公式<span
class="math inline">\(\eqref{eq:lr-gd}\)</span>中可以看到，每次更新回归系数时都需要遍历整个数据集，如果有数十亿样本和成千上万个特征，则梯度下降法的计算复杂度就太高了。随机梯度下降法一次仅用一个样本点来更新回归系数：</p>
<p><span class="math display">\[\begin{equation}
w^{k+1}_j = w^k\_j+α (y^{(i)}-h\_w(x^{(i)}))x\_{j}^{(i)}
\end{equation}\]</span></p>
<h2 id="梯度下降过程向量化">梯度下降过程向量化</h2>
<p>约定训练数据的矩阵形式如下，$ x
$的每一行为一条训练样本，而每一列为不同的特称取值：</p>
<p><span class="math display">\[\begin{equation}
x=
\left[
\begin{matrix}
x^{(1)}\\\\
x^{(2)}\\\\
\ldots\\\\
x^{(m)}
\end{matrix}
\right]
=
\left[
\begin{matrix}
x_0^{(1)} &amp; x_1^{(1)} &amp; \ldots &amp; x_n^{(1)}\\\\
x_0^{(2)} &amp; x_1^{(2)} &amp; \ldots &amp; x_n^{(2)}\\\\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\\\
x_0^{(m)} &amp; x_1^{(m)} &amp; \ldots &amp; x_n^{(m)}
\end{matrix}
\right]
,
y=
\left[
\begin{matrix}
y^{(1)}\\\\
y^{(2)}\\\\
\ldots\\\\
y^{(m)}
\end{matrix}
\right]
\end{equation}\]</span></p>
<p>约定待求的参数θ的矩阵形式为：</p>
<p><span class="math display">\[\begin{equation}
\theta =
\left[
\begin{matrix}
\theta_1\\\\
\theta_2\\\\
\ldots\\\\
\theta_n
\end{matrix}
\right]
\end{equation}\]</span></p>
<p>先求$ x <span class="math inline">\(并记为\)</span> A $：</p>
<p><span class="math display">\[\begin{equation}
A=x \cdot \theta
=
\left[
\begin{matrix}
x_0^{(1)} &amp; x_1^{(1)} &amp; \ldots &amp; x_n^{(1)}\\\\
x_0^{(2)} &amp; x_1^{(2)} &amp; \ldots &amp; x_n^{(2)}\\\\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\\\
x_0^{(m)} &amp; x_1^{(m)} &amp; \ldots &amp; x_n^{(m)}
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
\theta_0\\\\
\theta_1\\\\
\ldots\\\\
\theta_n
\end{matrix}
\right]
=
\left[
\begin{matrix}
\theta_0x_0^{(1)} + \theta_1x_1^{(1)} + \ldots + \theta_nx_n^{(1)}\\\\
\theta_0x_0^{(2)} + \theta_1x_1^{(2)} + \ldots + \theta_nx_n^{(2)}\\\\
\ldots \\\\
\theta_0x_0^{(m)} + \theta_1x_1^{(m)} + \ldots + \theta_nx_n^{(m)}
\end{matrix}
\right]
\end{equation}\]</span></p>
<p>求$ h_(x)-y <span class="math inline">\(并记为\)</span> E $：</p>
<p><span class="math display">\[\begin{equation}
E=h_\theta(x)-y=
\left[
\begin{matrix}
g(A^{(1)})-y^{(1)}\\\\
g(A^{(2)})-y^{(2)}\\\\
\ldots \\\\
g(A^{(m)})-y^{(m)}
\end{matrix}
\right]
=
\left[
\begin{matrix}
e^{(1)}\\\\
e^{(2)}\\\\
\ldots\\\\
e^{(m)}
\end{matrix}
\right]
=g(A)-y
\end{equation}\]</span></p>
<p>由上式可知$ h_(x)-y <span class="math inline">\(可以由\)</span>
g(A)-y $一次计算求得。</p>
<p>再来看一下公式<span
class="math inline">\(\eqref{eq:lr-gd}\)</span>的<span
class="math inline">\(\theta\)</span>更新过程：</p>
<p><span class="math display">\[\begin{eqnarray}
\theta_j &amp;=&amp; \theta\_j + \alpha
\sum\_{i=1}^{m}(-e^{(i)})x\_j^{(i)}\\\\
&amp;=&amp;
\theta\_j-\alpha\cdot(x\_j^{(1)},x\_j^{(2)},\ldots,x\_j^{(m)})\cdot E
\end{eqnarray}\]</span></p>
<p>综合上面的式子有：</p>
<p><span class="math display">\[\begin{equation}
\theta = \theta - \alpha\cdot\frac{1}{m}\cdot
x^T\cdot(g(x\cdot\theta)-y)
\end{equation}\]</span></p>
<h1 id="正则化">正则化</h1>
<p>由于模型的参数个数一般是由人为指定和调节的，所以正则化常常是用来限制模型参数值不要过大，也被称为惩罚项。一般是在目标函数(经验风险)中加上一个正则化项$
(w) $即</p>
<p><span class="math display">\[\begin{equation}
J(w) = -\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) +
(1-y_i)log(1-h_w(x_i))] + \lambda \Phi(w)
\label{eq:reg}
\end{equation}\]</span></p>
<p>而这个正则化项一般会采用L1范数或者L2范数。其形式分别为$ (w)=||x||_1
<span class="math inline">\(和\)</span> (w)=||x||_2 $。</p>
<p>首先针对L1范数$ (w)=|x| <span
class="math inline">\(，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化当\)</span>
w_j &gt; 0 <span class="math inline">\(是取1，当\)</span> w_j &lt; 0
$时取-1.</p>
<p>从而导致的参数<span
class="math inline">\(w_j\)</span>减去了学习率与公式的乘积，因此当$ w_j
&gt; 0 <span class="math inline">\(的时候，\)</span> w_j<span
class="math inline">\(会减去一个正数，导致\)</span> w_j <span
class="math inline">\(减小，而当\)</span> w_j &lt; 0 <span
class="math inline">\(的时候，\)</span> w_j<span
class="math inline">\(会减去一个负数，导致\)</span> w_j<span
class="math inline">\(又变大，因此这个正则项会导致参数\)</span>
w_j$取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为
Lasso regularization。</p>
<p>然后针对L2范数<span class="math inline">\(\phi(w) =
\sum_{j=1}^{n}w_j^2\)</span>，同样对它求导，得到梯度变化为<span
class="math inline">\(\frac{\partial \Phi(w)}{\partial w_j} =
2w_j\)</span>(一般会用<span
class="math inline">\(\frac{\lambda}{2}\)</span>来把这个系数2给消掉)。同样的更新之后使得$
w_j$的值不会变得特别大。在机器学习中也将L2正则称为weight
decay，在回归问题中，关于L2正则的回归还被称为Ridge
Regression岭回归。weight
decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。</p>
<p><strong>需要注意的是</strong>，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而公式<span
class="math inline">\(\eqref{eq:reg}\)</span>中的<span
class="math inline">\(\lambda\)</span>也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，<span
class="math inline">\(\lambda\)</span>越大，对参数值惩罚越大，泛化能力越好。</p>
<h1 id="机器学习实战代码">《机器学习实战》代码</h1>
<p>梯度上升法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span>(<span class="params">dataMatIn, classLabels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;梯度上升法&quot;&quot;&quot;</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.1</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    weights = ones((n, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):</span><br><span class="line">        a = dataMatrix * weights</span><br><span class="line">        h = sigmoid(dataMatrix * weights)  <span class="comment"># 100*3 3*1</span></span><br><span class="line">        error = (labelMat - h)</span><br><span class="line">        weights = weights + alpha / m * dataMatrix.transpose() * error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>随机梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span>(<span class="params">dataMatrix, classLabels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机梯度上升法，但是迭代次数不够，且可能存在局部波动现象&quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        h = sigmoid(<span class="built_in">sum</span>(dataMatrix[i] * weights))</span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span>(<span class="params">dataMatrix, classLabels, numIter=<span class="number">150</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;改进的随机梯度上升法&quot;&quot;&quot;</span></span><br><span class="line">    m, n = dataMatrix.shape</span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(numIter):</span><br><span class="line">        dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            alpha = <span class="number">4</span> / (<span class="number">1.0</span> + j + i) + <span class="number">0.01</span> <span class="comment"># alpha在每次迭代时都进行了调整</span></span><br><span class="line">            randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, <span class="built_in">len</span>(dataIndex))) <span class="comment"># 随机选取样本数据</span></span><br><span class="line">            h = sigmoid(<span class="built_in">sum</span>(dataMatrix[randIndex] * weights))</span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            <span class="keyword">del</span> (dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<h1 id="问题">问题</h1>
<ol type="1">
<li><p>LR为什么使用最大似然函数作为损失函数，而不是用MSE？</p>
<p>选用MSE作为损失函数时，求导形式为<span
class="math inline">\(\frac{\partial C}{\partial w}=(\hat{y} -
y)\sigma&#39;(z)x\)</span>，这个梯度是和sigmoid导数有关的，当模型的输出接近0或者1时，<span
class="math inline">\(\sigma&#39;(z)\)</span>就会非常小，造成梯度消失的问题。</p></li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a
href="http://blog.csdn.net/dongtingzhizi/article/details/15962797">【机器学习笔记1】Logistic回归总结</a>
<a
href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html">【机器学习算法系列之二】浅析Logistic
Regression</a> <a
href="http://blog.csdn.net/itplus/article/details/21896453">牛顿法与拟牛顿法学习笔记（一）牛顿法</a></p>
</blockquote>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title>SMOTE算法</title>
    <url>/2021/06/22/SMOTE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>SMOTE（Synthetic Minority Oversampling
Technique），合成少数类过采样技术．它是基于随机过采样算法的一种改进方案。</p>
<span id="more"></span>
<h1 id="算法思想">算法思想</h1>
<p>SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。</p>
<h1 id="算法流程">算法流程</h1>
<ol type="1">
<li><p>对于少数类中每一个样本<span
class="math inline">\(x\)</span>，以欧氏距离为标准计算它到少数类样本集中所有的样本距离，得到<span
class="math inline">\(k\)</span>近邻</p></li>
<li><p>根据样本不平衡比例设置一个采样比例以确定采样倍率<span
class="math inline">\(N\)</span>，对于每一个少数类样本<span
class="math inline">\(x\)</span>，从其<span
class="math inline">\(k\)</span>近邻中随机选择若干个样本，假设选择的近邻为<span
class="math inline">\(x_n\)</span>。</p></li>
<li><p>对于每一个随机选出的近邻<span
class="math inline">\(x_n\)</span>，分别与原样本按照如下的公式构建新的样本:
<span class="math display">\[
x_{new} = x + rand(0,1)\times (\tilde{x}-x)
\]</span></p></li>
</ol>
<p><img src="smote_sample.png" /></p>
<h1 id="算法伪代码">算法伪代码</h1>
<p><img src="smote.png" /></p>
<h1 id="算法缺陷">算法缺陷</h1>
<ol type="1">
<li><span class="math inline">\(k\)</span>邻近的选择：如何选择<span
class="math inline">\(k\)</span>才能使算法最优是未知的，需要反复的测试。</li>
<li>分布边缘化为题：SMOTE无法克服非平衡数据集的数据分布问题，如果一个负类样本处在负类样本集的分布边缘,则由此负类样本和相邻样本产生的“人造”样本也会处在这个边缘,且会越来越边缘化,从而模糊了正类样本和负类样本的边界,而且使边界变得越来越模糊。这种边界模糊性,虽然使数据集的平衡性得到了改善,但加大了分类算法进行分类的难度．</li>
</ol>
<h1 id="引用">引用</h1>
<blockquote>
<ol type="1">
<li><a href="https://www.jianshu.com/p/13fc0f7f5565">SMOTE算法</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>样本</tag>
      </tags>
  </entry>
  <entry>
    <title>最大似然估计（MLE） &amp; 最大后验概率估计（MAP）</title>
    <url>/2021/06/28/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89-%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1%EF%BC%88MAP%EF%BC%89/</url>
    <content><![CDATA[<p><strong>概率</strong>用于在已知一些参数的情况下，预测接下来的观测所得到的结果；</p>
<p><strong>似然性</strong>则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。</p>
<span id="more"></span>
<blockquote>
<p>概率：参数 + 观测 --&gt; 结果； 似然：观测 + 结果 --&gt; 参数；</p>
<p>参数可以理解为描述事件性质的未知数，知道这些参数，就能完全描述一个事件；
可以用 CTR
预估来解释这个过程，最开始我们只有观测（pv）和结果（clk，是否点击），我们假设模型未
LR，那么通过最大似然估计可以得到 LR
的相关参数，最后在线上使用的时候，可以直接根据模型（LR）+
观测数据（pv）来预估结果（点击的可能性）；</p>
<p>其实就是一个似然估计参数，然后概率预估结果；</p>
</blockquote>
<h1
id="最大似然估计maximum-likelihood-estimation-mle-or-极大似然估计">最大似然估计（Maximum
Likelihood Estimation, MLE） or 极大似然估计</h1>
<p>最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计理想地认为，对于极少的样本观测，我们观测到的样本很可能就是发生概率最大的。</p>
<p>假设<span
class="math inline">\(x_1,x_2,...,x_n\)</span>为独立同分布采样， <span
class="math inline">\(\theta\)</span>为模型参数，<span
class="math inline">\(f\)</span>为已知的模型。遵循上述的独立同分布假设，产生上述采样的概率可以表示为：
<span class="math display">\[
f(x_1,x_2,...x_n)=f(x_1|\theta)\times f(x_2|\theta)\times ... \times
f(x_n|\theta)
\]</span> 此时，<span
class="math inline">\(x_1,x_2,...,x_n\)</span>已知，而模型参数<span
class="math inline">\(\theta\)</span>未知，因此似然可以定义为 <span
class="math display">\[
L(\theta|x_1,x_2,...,x_n):=f(x_1,x_2,...,x_n|\theta)=\prod_{i=1}^{n}f(x_i\theta)
\]</span></p>
<blockquote>
<p>注：上面如果写等式其实是有误解的，最大似然估计是不考虑先验概率的，只是根据观测直接预估模型的参数。可以结合贝叶斯公式，如果去除贝叶斯公式中的先验概率，那么上面两个等式是可以认为是相等的；</p>
</blockquote>
<p>由于小数连乘操作可能造成下溢的问题，所以一般会对似然两边同时取对数进行计算，得到如下的形式：
<span class="math display">\[
logL(\theta|x_1,x_2,...,x_n)=\sum_{i=1}^{n}logf(x_i|\theta)
\]</span> 对参数<span
class="math inline">\(\theta\)</span>的最大似然估计为 <span
class="math display">\[
\hat{\theta}_{mle} =
argmax_\{\theta\in\Theta\}\hat{l}(\theta|x_1,x_2,...,x_n)
\]</span> 其中，<span
class="math inline">\(\hat{l}=\frac{1}{n}logL\)</span>为平均对数似然（其实不求平均也没问题，因为之后是对参数
<span class="math inline">\(\theta\)</span> 求导，和<span
class="math inline">\(n\)</span>无关)</p>
<p>之后求参数导，导数等于 0，求参数即可；</p>
<h1
id="最大后验概率估计maximum-a-posteriori-estimationmap">最大后验概率估计（Maximum
a Posteriori estimation，MAP）</h1>
<p>最大似然估计是求<span class="math inline">\(\theta\)</span>
使得似然函数<span
class="math inline">\(P(x_0|\theta)\)</span>最大，最大后验估计是求<span
class="math inline">\(\theta\)</span>使得函数<span
class="math inline">\(P(x_n|\theta)P(\theta)\)</span>最大，<span
class="math inline">\(\theta\)</span>自己出现的先验概率也最大（其实就是考虑了参数的先验概率）</p>
<p>MAP 其实是在最大化： <span class="math display">\[
P(\theta|x_n)=\frac{P(x_n|\theta)P(\theta)}{P(x_0)}
\]</span> 观测数据 <span class="math inline">\(x_0\)</span>
是确定的（比如抛硬币，出现：反正正正正反正正正反，把实验做 1000
次，“反正正正正反正正正反”出现了<span
class="math inline">\(n\)</span>次，那么 <span
class="math inline">\(P(x_0)=n/1000\)</span>
，总之，这是一个可以由数据集得到的值），因此可以把分母去掉；</p>
<p>最大化后验概率<span
class="math inline">\(P(\theta|x_0)\)</span>的意义也很明确，<span
class="math inline">\(x_0\)</span>已经出现了，要求<span
class="math inline">\(\theta\)</span>取得什么值是的 <span
class="math inline">\(P(x_n|\theta)\)</span>最大；</p>
<h1 id="最大似然估计-vs-最大后验概率估计">最大似然估计 vs
最大后验概率估计</h1>
<p>最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；</p>
<p>为什么先验概率加进去有用？</p>
<p>比如抛硬币，先验概率认为，正面朝上概率的最大可能是
0.5；但是在实际抛硬币的过程中，可能会出现抛 10 次 7
次正面的情况，这种情况下，最大似然估计会认为正面朝上的概率最大可能是
0.7，最大后验概率估计会综合考虑实验和先验概率，因此认为正面朝上的概率最大可能是
0.5-0.7 之间的一个值，相当于先验概率对其做了一个校正；</p>
<p>最大似然估计可以认为是频率学派的观点，最大后验概率估计可以认为是贝叶斯学派的观点；</p>
<p><strong>后验概率 := 似然 * 先验概率</strong></p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a href="https://zhuanlan.zhihu.com/p/46737512">最大似然估计（MLE）
&amp; 最大后验概率估计（MAP）</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT算法原理</title>
    <url>/2021/06/28/GBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>GBDT 的全称是 Gradient Boosting Decision
Tree，梯度提升树，想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient
Boosting 和Decision Tree。</p>
<span id="more"></span>
<h1 id="decision-treecart回归树">Decision Tree：CART回归树</h1>
<p>GBDT使用的决策树通通都是都是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是<strong>梯度值</strong>，是连续值所以要用回归树。</p>
<p>对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。</p>
<p>在<strong>分类树</strong>中最佳划分点的判别标准是熵或者基尼系数，都是用<strong>纯度</strong>来衡量的，但是在<strong>回归树</strong>中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是<strong>平方误差</strong>，它能很好的评判拟合程度。</p>
<p><strong>回归树生成算法:</strong></p>
<p><strong>输入:</strong> 训练数据集$ D$</p>
<p><strong>输出:</strong> 回归树 <span
class="math inline">\(f(x)\)</span></p>
<p>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<ol type="1">
<li><p>选择最优切分变量$ j$与切分点 <span
class="math inline">\(s\)</span>， 求解:</p>
<p><span class="math display">\[
\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j, s)}
(y_i-   c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j, s)} (y_i-c_2)^2]
\]</span></p>
<p>遍历变量<span class="math inline">\(j\)</span>，对固定的切分变量<span
class="math inline">\(j\)</span>扫描切分点 <span
class="math inline">\(s\)</span>，选择使得上式达到最小值的对 <span
class="math inline">\((j,s)\)</span>.</p>
<p><strong>简要解释一下上述公式</strong>：中括号里面的公式是求出每个特征变量在哪一个划分点时损失函数最小，最外面的
<span class="math inline">\(min\)</span>
是在所有特征值，求得使损失函数全局最小的特征及其切分点<span
class="math inline">\((j^*, s^*)\)</span>;</p></li>
<li><p>用选定的对 <span class="math inline">\((j,s)\)</span>
划分区域并决定相应的输出值：</p>
<p><span class="math display">\[
R_1(j, s)=\{x|x^{(j)}\leq s\},R_2(j, s)=\{x|x^{(j)} &gt; s\}
\]</span></p>
<p><span class="math display">\[
\hat {c_m}=\frac{1}{N}\sum_{x_1 \in R_m(j, s)}y_i, x \in R_m, m=1,2
\]</span></p>
<p>划分区域的输出值就是将该区域的所有样本的<strong>输出值求平均</strong>。</p></li>
<li><p>继续对两个子区域调用步骤（1）和（2），直至满足停止条件。</p></li>
<li><p>将输入空间划分为<span class="math inline">\(M\)</span>个区域<span
class="math inline">\(R_1,R_2…R_M\)</span>，得到决策树</p></li>
</ol>
<p><span class="math display">\[
f(x)=\sum_{m=1}^M \hat{c_m}I(x \in R_m)
\]</span></p>
<h1 id="gradient-boosting拟合负梯度">Gradient Boosting：拟合负梯度</h1>
<p>梯度提升树（Grandient Boosting）是提升树（Boosting
Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。</p>
<p><strong>提升树算法:</strong></p>
<ol type="1">
<li><p>初始化$ f0(x)=0$</p></li>
<li><p>对<span class="math inline">\(m=1,2…M\)</span></p>
<ol type="1">
<li><p>计算残差 <span class="math display">\[
r_{mi}=y_i-f_{m-1}(x_i), i=1, 2,...,M
\]</span></p></li>
<li><p>拟合残差$ r_{mi}$ 学习一个回归树，得到 <span
class="math inline">\(h_m(x)\)</span></p></li>
<li><p>更新 <span
class="math inline">\(f_m(x)=f_{m-1}(x)+h_m(x)\)</span></p></li>
</ol></li>
<li><p>得到回归树 <span class="math display">\[
f_{M}(x)=\sum_{m=1}^Mh_m(x)
\]</span></p></li>
</ol>
<p>上面伪代码中的<strong>残差</strong>是什么？</p>
<p>在提升树算法中，假设我们前一轮迭代得到的强学习器是</p>
<p><span class="math display">\[
f_{t-1}(x)
\]</span></p>
<p>损失函数是</p>
<p><span class="math display">\[
L(y, f_{t-1}(x))
\]</span> 我们本轮迭代的目标是找到一个弱学习器</p>
<p><span class="math display">\[
h_{t}(x)
\]</span></p>
<p>当采用平方损失函数时</p>
<p><span class="math display">\[
\begin{aligned}
    &amp; L(y, f_{t-1}(x)+h_t(x)) \\
    &amp; = (y - f_{t-1}(x) - h_t(x))^2 \\
    &amp; =(r - h_t(x))^2 \\
\end{aligned}
\]</span></p>
<p>这里，</p>
<p><span class="math display">\[
r = y - f_{t-1}(x)
\]</span>
是当前模型拟合数据的<strong>残差（residual）</strong>。所以，对于提升树来说只需要简单地拟合当前模型的残差。</p>
<p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p>
<p>第<span class="math inline">\(t\)</span>轮的第<span
class="math inline">\(i\)</span>个样本的损失函数的负梯度为： <span
class="math display">\[
-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]\_{f(x)=f_{t-1}(x)}
\]</span> 此时不同的损失函数将会得到不同的负梯度，如果选择平方损失:
<span class="math display">\[
L(y, f(x_i))=\frac{1}{2}(y-f(x_i))^2
\]</span> 负梯度为 <span class="math display">\[
-[\frac{\partial {L(y, f(x_i))}}{\partial
{f(x_i)}}]\_{f(x)=f_{t-1}(x)}=-[\frac{\partial
\frac{1}{2}(y-f(x_i))^2}{\partial {f(x_i)}}]\_{f(x)=f_{t-1}(x)}=y-f(x_i)
\]</span>
此时我们发现GBDT的<strong>负梯度就是残差</strong>，所以说对于回归问题，我们要拟合的就是残差。</p>
<blockquote>
<p><strong>gbdt的残差为什么用负梯度代替？</strong></p>
<ol type="1">
<li><p>通过一阶泰勒展开证明负梯度方向是下降最快的方向 <span
class="math display">\[
f(\theta_{k+1}) \approx
f(\theta_k)+\frac{\partial{f(\theta_k)}}{\partial{\theta_k}}(\theta_{k+1}-\theta_k)
\]</span></p></li>
<li><p>在GB中，对损失函数展开： <span class="math display">\[
L(y,F_m(x))\approx L(y, F_{m-1}(x))+\frac{\partial{L(y,
F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}(F_m(x)-F_{m-1}(x))
\]</span> 即有： <span class="math display">\[
L(y,F_m(x))\approx L(y, F_{m-1}(x))+\frac{\partial{L(y,
F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}T_m(x)
\]</span> 则在优化<span class="math inline">\(L(y,F(x))\)</span>时，
<span class="math display">\[
F_m(x)=F_{m-1}(x)-\eta \frac{\partial{L(y, F_{m-1}(x))}}{\partial{
F_{m-1}(x)}}
\]</span> 即，<span class="math inline">\(T_m(x)=-\eta
\frac{\partial{L(y, F_{m-1}(x))}}{\partial{
F_{m-1}(x)}}\)</span></p></li>
</ol>
<p>所以需要当前的弱学习器来学习负梯度，这里和GBDT中差了一个 <span
class="math inline">\(\eta\)</span>.</p>
<ol start="3" type="1">
<li><p>在1和2中都是随机梯度下降，但是不同的是：</p>
<ol type="1">
<li>在参数空间中优化，每次迭代得到参数的增量，这个增量就是负梯度乘上学习率；</li>
<li>在函数空间中优化，每次得到增量函数，这个函数会去拟合负梯度，在GBDT中就是一个个决策树。</li>
</ol></li>
</ol>
<p>要得到最终结果，只需要把初始值或者初始的函数加上每次的增量。所以1的优化过程是(假设迭代了M次)：</p>
<p>无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，<strong>残差只是特例</strong>。</p>
</blockquote>
<h1 id="gbdt算法原理">GBDT算法原理</h1>
<p>上面两节分别将Decision Tree和Gradient
Boosting介绍完了，下面将这两部分组合在一起就是我们的GBDT了。</p>
<p><strong>GBDT算法：</strong></p>
<ol type="1">
<li><p>初始化弱学习器 <span class="math display">\[
f_0(x)=\arg \min_{c}\sum_{i=1}^{N}L(y_i, c)
\]</span> 当为平方损失时，<span
class="math inline">\(f_0(x)=\frac{\sum_{i=1}^N
y_i}{N}\)</span></p></li>
<li><p>对<span class="math inline">\(m=1,2,…,M\)</span>有：</p>
<ol type="a">
<li>对每个样本<span
class="math inline">\(i=1,2,…,N\)</span>，计算负梯度，即残差</li>
</ol>
<p><span class="math display">\[
r_{im}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial
f(x_i)}]\_{f(x)=f_{m-1}(x)}
\]</span></p>
<ol start="2" type="a">
<li><p>将上步得到的残差作为样本新的真实值，并将数据<span
class="math inline">\((x_i, x_im), i=1,
2,…,N\)</span>作为下棵树的训练数据，得到一颗新的回归树$
f_m(x)$，其对应的叶子节点区域为 <span class="math inline">\(R_jm, j=1,
2,…,J\)</span>。其中J为回归树<span
class="math inline">\(t\)</span>的叶子节点的个数。</p></li>
<li><p>对叶子区域$ j=1,2,..J$计算最佳拟合值 <span
class="math display">\[
\gamma_{jm}=\arg \min_{\gamma}\sum_{x_i \in R_{jm}}L(y_i,
f_{m-1}(x_i)+\gamma) (对 \gamma求导并令导数为0即可求得)
\]</span></p></li>
<li><p>更新强学习器 <span class="math display">\[
f_m(x)=f_{m-1}(x)+\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})
\]</span></p></li>
</ol></li>
<li><p>得到最终学习器 <span class="math display">\[
f(x)=f_M{x}=f_{0}(x)+\sum_{m=1}^{M}\sum_{j=1}^{J}\gamma_{jm}I(x \in
R_{jm})
\]</span></p></li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://ranmaosong.github.io/2019/04/27/ML-GBDT/">GBDT算法原理以及实例理解</a></li>
<li><a
href="https://www.zhihu.com/question/63560633/answer/581670747">gbdt的残差为什么用负梯度代替？</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
        <tag>cart</tag>
        <tag>boosting tree</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习损失函数</title>
    <url>/2021/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>损失函数旨在表示出logit和label的差异程度，不同的损失函数有不同的表示意义，也就是在最小化损失函数过程中，logit逼近label的方式不同，得到的结果可能也不同。</p>
<span id="more"></span>
<p>损失函数（loss function）是用来<strong>估量模型的预测值<span
class="math inline">\(f(x)\)</span>与真实值<span
class="math inline">\(Y\)</span>的不一致程度</strong>，它是一个非负实值函数,通常使用<span
class="math inline">\(L(Y,
f(x))\)</span>来表示，损失函数越小，模型的鲁棒性就越好。损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：
<span class="math display">\[
\theta^*=arg  min_{\theta}{\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i,\theta))+\lambda\Phi(\theta)}
\]</span></p>
<h1 id="分类损失">分类损失</h1>
<h2 id="entropy">Entropy</h2>
<h2 id="cross-entropy">Cross Entropy</h2>
<h2 id="k-l-divergence">K-L Divergence</h2>
<h2 id="dice-loss">Dice Loss</h2>
<h2 id="focal-loss">Focal Loss</h2>
<h2 id="tversky-loss">Tversky Loss</h2>
<h1 id="回归损失">回归损失</h1>
<h2 id="l1-loss-mean-absolute-error-mae">L1 Loss (Mean Absolute Error
MAE)</h2>
<p>它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。
<span class="math display">\[
L = \sum_{i=1}^{n}{|Y_i - f(x_i)|}
\]</span> <strong>优点：</strong>
收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。</p>
<p><strong>缺点：</strong>
对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。</p>
<h2 id="l2-lossmean-squred-error-mse">L2 Loss(Mean Squred Error
MSE)</h2>
<p>它衡量的是预测值与真实1值之间距离的平方和，作用范围同为0到正无穷。
<span class="math display">\[
L=\sum_{i=1}^{n}{(Y_i-f(x_i))^2}
\]</span>
均方误差可用于线性回归的一个原因是：我们假设观测中包含噪声，其中噪声服从正态分布：
<span class="math display">\[
y= \boldsymbol{w}^T \boldsymbol{x} + b +\epsilon
\]</span> 其中， <span class="math inline">\(\epsilon ~
N(0,\sigma^2)\)</span>。</p>
<p>通过给定的<span
class="math inline">\(\boldsymbol{x}\)</span>观测到特定<span
class="math inline">\(y\)</span>的似然（likelihood）： <span
class="math display">\[
P(y|\boldsymbol{x})=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(
-\frac{1}{2\sigma^2}(y-\boldsymbol{w}^T \boldsymbol{x} - b)^2 \right)
\]</span> 根据最大似然估计法，参数<span
class="math inline">\(\boldsymbol{w}\)</span>和<span
class="math inline">\(b\)</span>的最优值是使整个数据集的似然最大的值：
<span class="math display">\[
P(\boldsymbol{y}|\boldsymbol{X})=\prod_{i=1}^{n}p(y^{(i)}|\boldsymbol{x}^{(i)})
\]</span></p>
<p>最小化负对数似然函数有： <span class="math display">\[
-log(P(\boldsymbol{y}|\boldsymbol{X}))=\sum_{i=1}^{n}\frac{1}{2}log(2\pi\sigma^2)
+ \frac{1}{2\sigma^2}\left(y^{(i)}-\boldsymbol{w}^T\boldsymbol{x}^{i}-b
\right)^2
\]</span> 而<span
class="math inline">\(\sigma\)</span>是某个固定的常数，于是上述公式第二部分与均方差误差一致。因此，在高斯噪声的假设下，最小化均方差等价于对线性模型的极大似然估计。</p>
<h2 id="smooth-l1-loss">Smooth L1 Loss</h2>
<h2 id="iou-loss">IoU Loss</h2>
<h2 id="glou-loss">GloU Loss</h2>
<h2 id="dlou-loss">DloU Loss</h2>
<h2 id="clou-loss">CloU Loss</h2>
<h2 id="f-eiou-loss">F-EIoU Loss</h2>
<h2 id="cdlou-loss">CDloU Loss</h2>
<h1 id="logloss对数损失函数">LogLoss对数损失函数</h1>
<p>log损失函数的标准形式 <span class="math display">\[
L(Y,P(Y|X)) = - logP(Y|X)
\]</span></p>
<p>损失函数<span class="math inline">\(L(Y,
P(Y|X))\)</span>表达的是样本<span
class="math inline">\(X\)</span>在分类<span
class="math inline">\(Y\)</span>的情况下，使概率<span
class="math inline">\(P(Y|X)\)</span>达到最大值（换言之，<strong>就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。</strong></p>
<p>取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。</p>
<p>todo:</p>
<p><span class="math display">\[
max(L) --&gt; min(-L)
\]</span></p>
<p>各种损失函数的适用场景</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p>1.<a
href="https://www.cnblogs.com/guoyaohua/p/9217206.html">一文读懂机器学习常用损失函数（Loss
Function）</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title>样本不均衡问题</title>
    <url>/2021/06/21/%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>类别不平衡问题指的是数据集中各个类别的样本数量极不均衡。</p>
<span id="more"></span>
<h1 id="定义">定义</h1>
<p>通常把样本类别比例超过3:1的数据成为不平衡数据。</p>
<h1 id="影响">影响</h1>
<p>多数数据样本带有的信息量比少数样本信息量大，会使得我们的分类模型存在很严重的偏向性。</p>
<p><strong>直观的例子</strong>：根据1000个正样本和1000个负样本正确训练出了一个精确率90%，召回率90%的分类器，且通过实验验证没有欠采样过采样的问题。直到有一天，数据发生了一点变化，还是原来的数据类型和特征，只是每天新数据中正负样本变成了100个正样本，10000个负样本。注意，先前精确率90%的另一种表达是负样本有10%的概率被误检为正样本。模型不变，现在误检的负样本数10000
* 0.1=1000个，正样本被检出100 *
0.9（召回）=90个，这个时候召回率不变仍为90%，但是新的精确率=90 /
(1000+90)=8.26%</p>
<h1 id="解决方法">解决方法</h1>
<ol type="1">
<li><strong>扩大数据集</strong>：更多的数据往往意味着更多的信息。</li>
<li><strong>数据重采样</strong>：
<ol type="1">
<li>过采样：对小类数据进行采样，增加小类数据样本量。<strong>随机过采样容易产生模型过拟合的问题，即使得模型学习到的信息过于特别(Specific)而不够泛化(General)</strong></li>
<li>欠采样：对大类数据进行采样，减少大类数据样本量。</li>
</ol></li>
<li><strong>人工构造样本</strong>：<a href="/2021/06/22/SMOTE%E7%AE%97%E6%B3%95/" title="SMOTE算法">SMOTE算法</a>（Synthetic Minority
Oversampling
Technique），基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。</li>
</ol>
<h1 id="个人思考">个人思考</h1>
<p>对于不同的业务场景，模型的目标不一样，如癌症识别业务适合高召回的模型，而恶意识别适合高准确的模型。</p>
<h1 id="引用">引用</h1>
<blockquote>
<ol type="1">
<li><a
href="https://blog.csdn.net/songhk0209/article/details/71484469">解决样本不平衡问题的奇技淫巧
汇总</a></li>
<li><a
href="https://zhuanlan.zhihu.com/p/56882616">炼丹笔记一：样本不平衡问题</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>样本</tag>
      </tags>
  </entry>
  <entry>
    <title>树模型原理与区别</title>
    <url>/2021/06/28/%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E4%B8%8E%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>RF,GBDT,XGBoost,lightGBM都属于集成学习（Ensemble
Learning），集成学习的目的是通过结合多个基学习器的预测结果来改善基本学习器的泛化能力和鲁棒性。</p>
<span id="more"></span>
<h1 id="随机森林randomforest">随机森林（RandomForest）</h1>
<p><strong>原理：</strong></p>
<p>Random Forest（随机森林）是Bagging的扩展变体，它在以决策树
为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择。</p>
<p>因此可以概括RF包括四个部分：</p>
<p>1、随机选择样本（放回抽样）；</p>
<p>2、随机选择特征属性；</p>
<p>3、构建决策树；</p>
<p>4、随机森林投票（平均）， 因此防止过拟合能力更强，降低方差。</p>
<p><strong>优点：</strong></p>
<ol type="1">
<li>随机森林算法能解决分类与回归两种类型的问题，表现良好，由于是集成学习，方差和偏差都比较低，泛化性能优越；</li>
<li>随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出特征的重要性程度，这是一个非常实用的功能。</li>
<li>可以应对缺失数据；</li>
<li>当存在分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法；</li>
<li>高度并行化，易于分布式实现</li>
<li>由于是树模型 ，不需要归一化即可之间使用</li>
</ol>
<p><strong>缺点：</strong></p>
<p>随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。</p>
<h1 id="gbdt-gradient-boosting-decision-tree">GBDT (Gradient Boosting
Decision Tree)</h1>
<p><strong>原理：</strong></p>
<p>GradientBoosting算法关键是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。</p>
<p>GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是<strong>CART回归树</strong>，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树)
因为Gradient Boosting
需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。</p>
<p><strong>优点：</strong></p>
<ol type="1">
<li>它能灵活的处理各种类型的数据；</li>
<li>在相对较少的调参时间下，预测的准确度较高。</li>
</ol>
<p><strong>缺点：</strong></p>
<p>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</p>
<h1 id="xgboost">XGBoost</h1>
<p>XGBoost与GBDT的区别： 在了解了XGBoost原理后容易理解二者的不同</p>
<ol type="1">
<li><p><strong>损失函数的改变：</strong>（导数和正则项的认识）</p>
<p>传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；</p>
<p>传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；</p>
<p>XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；</p></li>
<li><p><strong>工具的优化</strong>：（趋势值和并行的认识）</p>
<p>shrinkage（缩减），相当于学习速率（XGBoost中的eta）。</p></li>
<li><p><strong>列抽样</strong>：XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过
拟合，还能减少计算；</p></li>
<li><p><strong>对缺失值的处理</strong>：对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；</p></li>
<li><p><strong>XGBoost工具支持并行</strong></p>
<p>注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。</p>
<p>XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代
中重复地使用这个结构，大大减小计算量。</p>
<p>这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p></li>
</ol>
<p><strong>缺点</strong></p>
<ol type="1">
<li>level-wise
建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。</li>
<li>预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)</li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://zhuanlan.zhihu.com/p/62207593">RandomForest、GBDT、XGBoost、lightGBM
原理与区别</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/142413825">机器学习 |
XGBoost详解</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>XGBoost</tag>
        <tag>GBDT</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title>概率分布函数和概率密度函数</title>
    <url>/2021/06/26/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E5%92%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>如何理解概率分布函数和概率密度函数的问题。</p>
<span id="more"></span>
<h1 id="离散型">离散型</h1>
<h2 id="变量定义">变量定义</h2>
<p><strong>离散型随机变量：</strong>如果随机变量的值可以都可以逐个列举出来，则为离散型随机变量。</p>
<h2 id="概率函数">概率函数</h2>
<p><strong>概率函数</strong>，就是用函数的形式来表达概率。 <span
class="math display">\[
p_i = P\\{X=x_i\\}
(i=1,2,3,4,5,6)
\]</span></p>
<h2 id="概率分布">概率分布</h2>
<p><strong>概率分布</strong>，用于表述随机变量取值的概率规律。</p>
<figure>
<img src="离散型概率分布.png" alt="离散型概率分布" />
<figcaption aria-hidden="true">离散型概率分布</figcaption>
</figure>
<h2 id="概率分布函数">概率分布函数</h2>
<p>又称累<strong>积概率函数</strong> <span class="math display">\[
F(x)=P(X\le x) = \sum_{x_k \le x}p_k
\]</span></p>
<h1 id="连续型">连续型</h1>
<h2 id="变量定义-1">变量定义</h2>
<p><strong>连续性随机变量：</strong>如果随机变量X的取值无法逐个列举则为连续型变量。</p>
<h2 id="概率密度函数">概率密度函数</h2>
<p>对标离散型变量的概率函数： <span class="math display">\[
P(a \le X \le b) = F(b) - F(a) = \int ^b_a(x)dx
\]</span></p>
<h2 id="概率分布函数-1">概率分布函数</h2>
<p>概率分布函数为概率密度函数的积分 <span class="math display">\[
F(x)=P(X \le x) = \int_{-\infty}^x(x)dx
\]</span> <img src="概率密度函数.png"
alt="概率分布函数与概率密度函数" /></p>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程</title>
    <url>/2021/06/28/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！</p>
<span id="more"></span>
<h1 id="数据描述">数据描述</h1>
<ul>
<li>统计值：max, min, mean, std等</li>
<li>集中趋势</li>
<li>分布形状</li>
</ul>
<h1 id="特征处理">特征处理</h1>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="缺失值处理">缺失值处理</h3>
<ol type="1">
<li>缺失值删除
<ul>
<li>删除样本</li>
<li>删除特征</li>
</ul></li>
<li>缺失值填充
<ul>
<li>固定值填充，如0，999，-999</li>
<li>均值填充</li>
<li>众数填充</li>
<li>上下数据填充</li>
<li>插值法填充</li>
<li>KNN填充</li>
<li>random forest填充</li>
<li>不填充：
LightGBM和XGBoost都能对NaN数据进行学习，不需要处理缺失值</li>
</ul></li>
</ol>
<h3 id="异常值处理">异常值处理</h3>
<ol type="1">
<li>基于统计的异常点检测算法
例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。</li>
<li>基于距离的异常点检测算法
主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离(曼哈顿距离)、欧氏距离和马氏距离等方法。</li>
<li>基于密度的异常点检测算法
考察当前点周围密度，可以发现局部异常点。</li>
</ol>
<h2 id="特征转换">特征转换</h2>
<h3 id="连续型特征">连续型特征</h3>
<ol type="1">
<li><p><strong>函数转换</strong>：有时我们的模型的假设条件是要求自变量或因变量服从某特殊分布（如正太分布），或者说自变量或因变量服从该分布时，模型的表现较好。</p></li>
<li><p><strong>特征缩放</strong>：某些模型（像岭回归）要求你必须将特征值缩放到相同的范围值内。通过缩放可以避免某些特征比其他特征获得大小非常悬殊的权重值。</p></li>
<li><p><strong>无量纲化</strong>：无量纲化使不同规格的数据转换到同一规格。</p>
<ul>
<li><p><strong>标准化</strong>：</p>
<ul>
<li>均值方差法</li>
<li>z-score标准化</li>
<li>StandardScaler标准化</li>
</ul></li>
<li><p><strong>归一化</strong>：</p>
<ul>
<li>最大最小归一化</li>
<li>对数函数转化（log）</li>
<li>反余切转化</li>
</ul></li>
<li><p><strong>区间缩放法</strong>：区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放。
<span class="math display">\[
x^{\prime}=\frac{x-Min}{Max-Min}
\]</span></p></li>
</ul></li>
<li><p><strong>二值化（定量特征）</strong>：特征的二值化处理是将数值型数据输出为布尔类型。其核心在于设定一个阈值，当样本书籍大于该阈值时，输出为1，小于等于该阈值时输出为0。</p></li>
<li><p><strong>离散化分箱处理</strong>：将数值型属性转换成类别型更有意义，同时将一定范围内的数值划分成确定的块，使算法减少噪声的干扰，避免过拟合。</p>
<ul>
<li><p>等值划分</p></li>
<li><p>等频划分</p></li>
</ul></li>
</ol>
<h3 id="离散化特征">离散化特征</h3>
<ol type="1">
<li><p><strong>数值化处理</strong>：将类别属性转换成一个标量，最有效的场景应该就是二分类的情况。这种情况下，并不需要排序，并且你可以将属性的值理解成属于类别1或类别2的概率。
多分类问题：选取多分类，编码到[0，classnum)。</p>
<p>该方法局限性较大：</p>
<ul>
<li>不适用于建立预测具体数值的模型，比如线性回归，只能用于分类，</li>
<li>即使用于分类，也有一些模型不适合，</li>
<li>可能结果的精度不如one-hot编码。</li>
</ul></li>
<li><p><strong>哑编码</strong>：</p>
<ul>
<li><p>独热编码（one-hot)：数据集中的每个实例，只有一个是1（其他的为0）</p>
<ul>
<li>优点：简单，且保证无共线性。</li>
<li>缺点：太稀（稀疏矩阵）</li>
</ul></li>
<li><p>顺序性哑变量：将一个变量的k个值生成k个哑变量，保护了特征的顺序关系。</p>
<table>
<thead>
<tr class="header">
<th>status</th>
<th>向量表示</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>bad</td>
<td>(1,0,0)</td>
</tr>
<tr class="even">
<td>normal</td>
<td>(1,1,0)</td>
</tr>
<tr class="odd">
<td>good</td>
<td>(1,1,1)</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ol>
<h1 id="特征选择">特征选择</h1>
<h2 id="特征检验">特征检验</h2>
<h3 id="单变量">单变量</h3>
<ol type="1">
<li>正态性检验</li>
<li>显著性分析</li>
</ol>
<h3 id="多变量">多变量</h3>
<ol type="1">
<li>一致性检验</li>
<li>多重共线性</li>
</ol>
<h2 id="特征选择-1">特征选择</h2>
<h3 id="filter过滤法">Filter：过滤法</h3>
<p>按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。过滤式特征选择的评价标准分为四种，即距离度量、信息度量、关联度度量以及一致性度量。</p>
<p><strong>优点：</strong>算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。</p>
<p><strong>缺点</strong>：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。</p>
<ol type="1">
<li><p><strong>方差选择法</strong></p>
<p>计算各个特征的方差，选择方差大于阈值的特征 。</p></li>
<li><p><strong>相关系数法</strong></p>
<p>计算各个特征对目标值的相关系数以及相关系数的P值。 <span
class="math display">\[
\rho_{\boldsymbol{x},\boldsymbol{y}}=\frac{\text{cov}(\boldsymbol{x},\boldsymbol{y})}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}}=\frac{E[(\boldsymbol{x}-\mu_\boldsymbol{x},\boldsymbol{y}-\mu_\boldsymbol{y})]}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}}
\]</span> 使用条件：</p>
<ol type="1">
<li>两个变量间有线性关系；</li>
<li>变量是连续变量；</li>
<li>变量均符合正态分布，且二元分布也符合正态分布；</li>
<li>两变量独立；</li>
<li>两变量的方差不为 0；</li>
</ol></li>
<li><p><strong>互信息法</strong></p>
<p>互信息（mutual
information）是用来评价<strong>一个事件的出现对于另一个事件的出现所贡献的信息量。</strong>
<span class="math display">\[
I(X;Y) = \sum\limits_{y \in \mathcal{Y}}\sum\limits_{x \in \mathcal{X}}
p(x,y) \,\text{log}\left(\frac{p(x,y)}{p(x)p(y)}\right)
\]</span> 而如果 𝑥 和 𝑦 是相互独立的随机变量，则 𝑝(𝑥,𝑦)=𝑝(𝑥)𝑝(𝑦)
，那么上式为 0。因此若
𝐼(𝑋;𝑌)越大，则表示两个变量相关性越大，于是就可以用互信息来筛选特征。</p></li>
<li><p><strong>卡方检验（Chi-Square）</strong></p>
<p>卡方检验恰好可以进行<strong>独立性检验</strong>，所以其适用于特征选择。</p>
<blockquote>
<p>卡方分布，其定义如下：</p>
<p>设随机变量<span class="math inline">\(x_1, x_2 ... x_n \,,\quad
\text{i.i.d} \sim N(0,1)\)</span>，即独立同分布于标准正态分布，那么这
𝑛个随机变量的平方和： <span class="math display">\[
X = \sum\limits_{i=1}^n x_i^2
\]</span> 构成一个新的随机变量，其服从自由度为 𝑛 的卡方分布 ( 𝜒2 分布)
，记为 <span class="math inline">\(X \sim \chi^2_n\)</span>。</p>
</blockquote>
<p>计算检验统计量 𝜒2 ( 𝜒2χ表示卡方值) ，𝜒2
越大，表示观测值和理论值相差越大，当 𝜒2
大于某一个临界值时，就能获得统计显著性的结论。 <span
class="math display">\[
\chi^2 = \sum\frac{(观测频数 - 期望频数)^2}{期望频数}= \sum_{i=1}^{r}
\sum_{j=1}^{c} {(O_{i,j} - E_{i,j})^2 \over E_{i,j}}
\]</span></p></li>
</ol>
<h3 id="wrapper包装法">Wrapper：包装法</h3>
<p>根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</p>
<h3 id="embedded嵌入法">Embedded：嵌入法</h3>
<p>先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://zhuanlan.zhihu.com/p/111296130">深度了解特征工程</a></li>
<li><a
href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">常用的特征选择方法之
Pearson 相关系数</a></li>
<li><a
href="https://www.cnblogs.com/massquantity/p/10486904.html">[特征选择：
卡方检验、F
检验和互信息](https://www.cnblogs.com/massquantity/p/10486904.html)</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>社区发现-Fast Unfolding算法</title>
    <url>/2021/06/28/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0-Fast-Unfolding%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>在社交网络中，有些用户之间联系较为紧密，而另外一些用户之间的关系则较为稀疏。在网络中，我们可以将联系较为紧密的部分用户看成一个社区，在这个社区内部，用户之间联系紧密，而在两个社区之间，联系较为稀疏。</p>
<span id="more"></span>
<h1 id="社区划分的评价标准">社区划分的评价标准</h1>
<p>利用算法将整个网络划分成多个社区之后，需要一个评价指标来衡量这个划分结果的好坏。fast
unfolding算法采用的是<strong>模块度（Modularity）</strong>Q值来衡量。</p>
<p><strong>模块度</strong>可以定义为：社区内部的总边数和网络中总边数的比例减去一个期望值，该期望值是将网络设定为随机网络时同样的社区分配所形成的社区内部的总边数和网络中总边数的比例的大小。
<span class="math display">\[
Q = \frac{1}{2m}\sum_{vw}\left[A_{vw}-\frac{k_v
k_w}{2m}\right]\delta(c_v,c_w)
\]</span></p>
<p>其中，<span
class="math inline">\(A_{vw}\)</span>为网络中邻接矩阵中的一个元素：</p>
<p><span class="math display">\[
A_{vw}=
\begin{cases}
1&amp; 点v和w是相连的\\\\
0&amp; 其它
\end{cases}
\]</span></p>
<p><strong>社区内部的边数和网络的总边数的比例：</strong></p>
<p><span class="math inline">\(m\)</span>为整个网络中的边数 <span
class="math display">\[
m=\frac{1}{2}\sum_{vw}A_{vw}
\]</span></p>
<p><span class="math inline">\(c_v\)</span>表示点<span
class="math inline">\(v\)</span>所属的社区，当<span
class="math inline">\(i\)</span>,<span
class="math inline">\(j\)</span>存在于同一个社区中时，<span
class="math inline">\(\delta(i,j) = 1\)</span>，否则为0。</p>
<p>于是，社区内部的边数和网络的总边数的比例为： <span
class="math display">\[
\frac{\sum_{vw}A_{vw}\delta(c_v,c_w)}{\sum_{vw}A_{vw}} =
\frac{1}{2m}\sum_{vw}A_{vw}\delta(c_v,c_w)
\]</span></p>
<p><strong>随机网络的总边数和网络中总边数的比例：</strong></p>
<p>定义<span class="math inline">\(k_v\)</span>表示点<span
class="math inline">\(v\)</span>的度，即 <span class="math display">\[
k_v = \sum_w A_{vw}
\]</span></p>
<p>则将网络设定成随机网络，并进行相同的社区分配操作形成的社区内部的总边数和网络中总边数的比例的大小为<span
class="math inline">\(\frac{k_vk_w}{2m}\)</span>。</p>
<blockquote>
<p>网络代表所有与原网络有一样的度序列的网络平均。下图说明了如何保持节点的度序列来随机化网络连边。</p>
<figure>
<img src="fast_unfolding0.jpg" alt="随机化网络" />
<figcaption aria-hidden="true">随机化网络</figcaption>
</figure>
<ul>
<li>将网络的边都断成两段。度为k的节点有<span
class="math inline">\(k\)</span>个“半边”</li>
<li>每条“半边”都随机寻找其他的“半边”配成一个整边</li>
<li>容易发现，这样得到的新的网络跟原始的网络有同样的度序列</li>
<li>节点<span class="math inline">\(i\)</span>有<span
class="math inline">\(k_i\)</span>个“半边”，每个半边恰好连接的是节点<span
class="math inline">\(j\)</span>的“半边”的概率为<span
class="math inline">\(\frac{k_j}{2m}\)</span></li>
<li>平均来说，新的网络中，节点i与节点j的期望连边数为<span
class="math inline">\(\frac{k_ik_j}{2m}\)</span>。</li>
</ul>
</blockquote>
<p><strong>模块度变形</strong>：</p>
<p>定义<span class="math inline">\(e_{ij}\)</span>为社区<span
class="math inline">\(i\)</span>与社区<span
class="math inline">\(j\)</span>之间的边数占网络中所有边数的占比，即
<span class="math display">\[
e_{ij} = \frac{1}{2m} \sum_{vw}A_{vw}\delta(c_v,i)\delta(c_w,j)
\]</span></p>
<p>定义<span class="math inline">\(a_i\)</span>为连接到社区<span
class="math inline">\(i\)</span>的边数占网络中所有边数的占比，即</p>
<p><span class="math display">\[
a\_i = \frac{1}{2m} \sum_{v}k_{v}\delta(c_v,i)
\]</span></p>
<p>同时，由于<span
class="math inline">\(\delta(c\_v,c\_w)=\sum\_i\delta(c\_v,i)\delta(c\_w,i)\)</span>.
则模块度的计算可以简化为：</p>
<p><span class="math display">\[
\begin{eqnarray\*}
Q &amp; = &amp; \frac{1}{2m}\sum_{vw}\left[A_{vw}-\frac{k_v
k_w}{2m}\right]\sum\_i\delta(c\_v,i)\delta(c\_w,i) \\\\
&amp; = &amp; \sum\_i
\left[\frac{1}{2m}\sum\_{vw}A\_{vw}\delta(c\_v,i)\delta(c\_w,i)-\frac{1}{2m}\sum\_v
k\_v\delta(c\_v,i)\frac{1}{2m}\sum\_w k\_w\delta(c\_w,i)\right]\\\\
&amp; = &amp; \sum\_i (e\_{ii}-a\_i^2)
\end{eqnarray\*}
\]</span></p>
<h1 id="fast-unfolding算法">Fast Unfolding算法</h1>
<p>在社区发现问题中，以前的研究人员提出了许多的方法，例如标签传播算法（Label
Propagation Algorithm）、Fast
Unfolding等。考虑到现有数据的规模和算法的复杂度等因素，本文选用的是fast
unfolding。</p>
<p>Fast
Unfolding算法的主要目标是不断划分社区使得划分后的整个网络的模块度不断增大。算法主要包括两个过程，过程示例如下。</p>
<figure>
<img src="fast_unfolding.png" alt="fast unfolding示意图" />
<figcaption aria-hidden="true">fast unfolding示意图</figcaption>
</figure>
<ol type="1">
<li><p><strong>Modularity
Optimization</strong>，这一过程主要讲节点与邻近的社区进行合并，使得网络的模块度不断变大。</p>
<p>定义<span class="math inline">\(\sum\_{in}\)</span>为社区<span
class="math inline">\(C\)</span>内所有边的权重和，<span
class="math inline">\(\sum\_{tot}\)</span>为与社区<span
class="math inline">\(C\)</span>内的点连接的边的权重和，<span
class="math inline">\(k\_i\)</span>为所有连接到节点<span
class="math inline">\(i\)</span>上的边的权重和，<span
class="math inline">\(k\_{i,in}\)</span>为节点<span
class="math inline">\(i\)</span>与社区<span
class="math inline">\(C\)</span>内的节点连接的边的权重和，<span
class="math inline">\(m\)</span>是网络中所有边的权重和。</p>
<p>则将节点<span class="math inline">\(i\)</span>划分到社区<span
class="math inline">\(C\)</span>中产生的模块度的变化<span
class="math inline">\(\Delta Q\)</span>可用下式计算 <span
class="math display">\[
\begin{eqnarray\*}
\Delta Q &amp; = &amp; \left[\frac{\sum\_{in} + k\_{i,in}}{2m} -
\left(\frac{\sum\_{tot}+k\_i}{2m}\right)^2\right]-\left[\frac{\sum\_{in}}{2m}
- \left(\frac{\sum\_{tot}}{2m}\right)^2 -
\left(\frac{k\_i}{2m}\right)^2\right]\\\\
&amp; = &amp; \frac{k\_{i,in}}{2m} - \frac{k\_i\sum\_{tot}}{2m^2}
\end{eqnarray\*}
\]</span></p>
<p>根据上式，我们只需要知道社区中与该节点连接的边的权重之和，以及社区中的点连接的边的权重和就可以计算模块度的变化量。</p></li>
<li><p><strong>Commnunity
Aggregation</strong>这一过程将第一步中的社区汇聚成一个点，重构网络结果。</p>
<p>这一步中，将原来的两个社区之间的边的<strong>权重和</strong>作为新的节点之间的权重，将社区内的权重和作为新节点上的<strong>环向边</strong>的权重。</p>
<p>可以看到，做Graph folding的意义在于调整Modularity
Optimization的基本单位。这么做的主要原因是</p>
<ul>
<li>基于邻居的Modularity
Optimization优化在后期变化非常缓慢，需要通过调整变量的粒度来加速算法的收敛。</li>
<li>Graph folding还使得算法的视野更远，即每一步Modularity
Optimization将涉及到更多的非邻居节点。</li>
</ul></li>
</ol>
<p>fast unfolding算法将重复迭代以上过程，直至网络的结构不变（<span
class="math inline">\(\Delta Q\)</span>小于某个阈值）。</p>
<h1 id="fast-unfolding算法的应用">Fast unfolding算法的应用</h1>
<ol type="1">
<li>寻找黑浓度较高的社区，判定成黑社区</li>
<li>黑浓度较低的社区，判定成灰规则</li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a
href="http://blog.csdn.net/wangyibo0201/article/details/52048248">模块度Q——复杂网络社区划分评价标准</a>
<a
href="http://ece-research.unm.edu/ifis/papers/community-moore.pdf">Finding
community structure in very large networks</a> <a
href="https://arxiv.org/pdf/0803.0476.pdf">Fast unfolding of communities
in large networks</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>社区发现</tag>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>ABCNN</title>
    <url>/2021/07/08/ABCNN/</url>
    <content><![CDATA[<p>ABCNN是一种基于卷积神经网络CNN和Attention机制的算法，应用于文本分类等业务场景中。</p>
<span id="more"></span>
<h1 id="主要思路">主要思路</h1>
<h2 id="bcnn">BCNN</h2>
<p><strong>BCNN结构</strong>：使用基本CNN网络（无attention机制），每次处理一对句子，输出层解决sentence
pair task.</p>
<figure>
<img src="bcnn.png" alt="bcnn" />
<figcaption aria-hidden="true">bcnn</figcaption>
</figure>
<p>假设输入两个句子 <span class="math inline">\(s_0\)</span> 和 <span
class="math inline">\(s_1\)</span>，句子长度分别为 <span
class="math inline">\(l_0\)</span> 和 <span
class="math inline">\(l_1\)</span>。输入的长度取 <span
class="math inline">\(s = max(l_0,l_1)\)</span> ,短的补0。</p>
<h3 id="input-layer">Input layer</h3>
<p>每个句子被填充为相同长度<span
class="math inline">\(s\)</span>，使用词向量映射每个句子为矩阵$
d_0s$维矩阵。</p>
<h3 id="convolution-layer">Convolution layer</h3>
<p>卷积核取<span class="math inline">\(d_0 \times
w\)</span>,沿词方向做卷积，左右做padding，分别加长度$w-<span
class="math inline">\(1 补0。则卷积后得到特征长度为\)</span>s + 2 (w-1)
- w + 1 = s+w-1<span class="math inline">\(。叠加\)</span>d_1$
个卷积核，最终得到新的词向量，维度为<span class="math inline">\(d_1
\times (s+w-1)\)</span> 。卷积核的高须为词向量的维度。</p>
<p>设词向量分别为<span
class="math inline">\(v_1,v_2,...,v_s\)</span>。记<span
class="math inline">\(c_i =
[v_{i-w+1},v_{i-w+2},...,v_i]\)</span>，<span
class="math inline">\(c_i\)</span>的维度为<span
class="math inline">\(d_0\times w\)</span>。<span
class="math inline">\(p_i\)</span>为卷积<span
class="math inline">\(c_i\)</span>后的特征，则有：</p>
<p><span class="math display">\[
p=tanh(Wc_i + b)
\]</span></p>
<p>其中，<span
class="math inline">\(W\)</span>为卷积核集合，即为卷积权重，其维度为<span
class="math inline">\(d_1\times d_0\times w\)</span>，<span
class="math inline">\(b_i\)</span>为对应的偏置。</p>
<h3 id="pooling-layer">Pooling layer</h3>
<p>沿词的方向，步长为<span class="math inline">\(step\)</span>,长度为
$wp $ ,取需要池化的向量<span class="math inline">\(p_i\)</span>,维度为
<span class="math inline">\(d_0\times
w\)</span>,经过池化函数后，维度变为$d_0
$。池化函数作用的是词向量沿着词维度的特征集合。池化函数可取均值，最大值等。</p>
<p><strong>all-ap</strong>：对所有特征列做column-wise的pooling操作，得到
<span class="math inline">\(s \times 1\)</span>维度的输出向量</p>
<p><strong>w-ap</strong>：卷积核长度设置为<span
class="math inline">\(w\)</span>，对窗口<span
class="math inline">\(w\)</span>内的特征列做column-wise的polling操作，得到<span
class="math inline">\(s+w-1\)</span>的输出向量。</p>
<h3 id="output-layer">Output layer</h3>
<p>就是对整个特征层做池化。输出特征向量后，把两个句子的特征向量拼接起来得到特征。</p>
<h2 id="abcnn">ABCNN</h2>
<p>ABCNN（Attention-Based
BCNN）有3种结构，ABCNN-1，ABCNN-2，ABCNN-3。</p>
<h3 id="abcnn-1">ABCNN-1</h3>
<figure>
<img src="abcnn_1.png" alt="ABCNN-1" />
<figcaption aria-hidden="true">ABCNN-1</figcaption>
</figure>
<p>结构中增加了一个抽象层级，就是在原有单词级上增加了一个短语级的抽象。单词级的抽象文中重新命名为unit,作为低级别的表示，短语级的作为更高一级的表示。图中那个红色的与BCNN网络中的输入是一样的，是句子的词向量矩阵，两个句子。第一个句子5个单词，第二个句子7个单词。</p>
<p>蓝色的为短语级高一级的词向量表示。蓝色表示是由Attention Matrix
A和红色词向量计算生成</p>
<p>Attention Matrix <span
class="math inline">\(A\)</span>是由左右两个句子的情况生成。<span
class="math inline">\(A\)</span>中的 <span
class="math inline">\(i\)</span> 列值是由左边句子（五个单词）中第 <span
class="math inline">\(i\)</span>
个单词（unit）的向量与右边句子的Attention值分布<span
class="math inline">\(j\)</span>行值是由右边句子中第<span
class="math inline">\(j\)</span>个单词的向量与左边句子的Attention值分布</p>
<p>注意力矩阵<span
class="math inline">\(A\)</span>定义句子间词的关系，即： <span
class="math display">\[
A_{i,j}=score(F_{0,r}[:,i],F_{1,r}[:,j])
\]</span> 其中，<span
class="math inline">\(F_{i,r[:,k]}\)</span>定义为第<span
class="math inline">\(i\)</span>个句子的第<span
class="math inline">\(k\)</span>个词向量。 <span class="math display">\[
score(x,y)=\frac{1}{1+|x-y|}
\]</span> 生成注意力矩阵后，利用<span
class="math inline">\(F_{0,a}=W_0A^T，F_{1,a}=W_1A\)</span>
得到句子对应的注意力特征矩阵。叠加到句子的特征矩阵中，进行卷积。</p>
<h3 id="abcnn-2">ABCNN-2</h3>
<figure>
<img src="abcnn_2.png" alt="ABCNN-2" />
<figcaption aria-hidden="true">ABCNN-2</figcaption>
</figure>
<p>ABCNN-2架构是在以初始词向量形式输入并经过卷积后的输出的向量表示中（两个句子分别变成了7col和9col），计算出Attention
Matrix
A，计算方法还是计算两个句子对应单词向量的欧式距离，生成向量矩阵，方法同ABCNN-1的架构。</p>
<p>第二步：计算卷积向量权重，给每个单词计算一个权重值。左边句子（7col）的每个单词对应的Attention权重是由Matrix
A中列向量求和的值作为权重值，col-wise
sum，右边句子中每个单词的权重值是Matrix
A中行向量求和值作为权重值，row-wise sum. <span class="math display">\[
a_{0,j}=\sum A[j,:]t
\]</span> 句子原始输入是词向量矩阵，左边的是<span
class="math inline">\(5*d\)</span>，右边是<span
class="math inline">\(7*d\)</span>，<span
class="math inline">\(w\)</span>是3 <span class="math display">\[
F_{i,r}^c \in R^{d\times(s_i+w-1)}
\]</span> 将卷积输出的特征矩阵，基于Attention权重值，做池化。<span
class="math inline">\(i\)</span>取值为0,1，<span
class="math inline">\(j\)</span>取值为句子长度。 <span
class="math display">\[
F_{i,r}^p[:,j]=\sum_{k=j:j+2}a_{i,k}F_{i,r}^c[:,k], j = 1...s_i
\]</span> <strong>ABCNN-1和ABCNN-2比较:</strong></p>
<ol type="1">
<li>ABCNN-1中Attention是间接的影响卷积，ABCNN-2中通过Attention权重直接影响池化。</li>
<li>ABCNN-1需要两个权重矩阵需要学习，并且输入特征矩阵要两次，
相比ABCNN-2网络需要更多的参数学习，容易过拟合。</li>
<li>ABCNN-2执行更大粒度的池化，如果在卷积层输入的是词粒度的，那么ABCNN-2在池化时就已经是短语粒度的（经过卷积了），池化时的w和卷积的w保持一致。</li>
</ol>
<h3 id="abcnn-3">ABCNN-3</h3>
<figure>
<img src="abcnn_3.png" alt="ABCNN-3" />
<figcaption aria-hidden="true">ABCNN-3</figcaption>
</figure>
<p>将ABCNN-1和ABCNN-2结合，作为架构，这样保留word
level信息，也增加了phrase level的信息。更加高层次的特征抽象。</p>
<h1 id="工程网络图">工程网络图</h1>
<h2 id="bcnn-1">BCNN</h2>
<pre class="mermaid">graph BT 

subgraph Input Layer
X1("x1 [None, 300, 40]")
X2("x2 [None, 300, 40]")

X1 --> X1_expand("x1_expand [None,300, 40,1]")
X2 --> X2_expand("x2_expand [None,300, 40,1]")
end

X1_expand --all_pool--> left_ap_0("left_ap_0 [None, 300]")
X2_expand --all_pool--> right_ap_0("right_ap_0 [None, 300]")

subgraph CNN Layer 1
X1_expand --"padding (3,3)"--> left_pad_1("left_pad [None, 300, 46, 1]")
X2_expand --"padding (3,3)"--> right_pad_1("right_pad [None, 300, 46, 1]")

left_pad_1 --"convolution (300, 4, 50)"--> left_conv_trans_1("left_conv_trans [None, 50, 43, 1]")
right_pad_1 --"convolution (300, 4, 50)"--> right_conv_trans_1("right_conv_trans [None, 50, 43, 1]")

left_conv_trans_1 --"w_pool (1, 4)"--> left_wp_1("left_wp [None, 50, 40, 1]")
left_conv_trans_1 --"all_pool (1, 43)"--> left_ap_1("left_ap [None, 50]")


right_conv_trans_1 --"w_pool (1, 4)"--> right_wp_1("left_wp [None, 50, 40, 1]")
right_conv_trans_1 --"all_pool (1, 43)"--> right_ap_1("left_ap [None, 50]")
end

subgraph CNN Layer 2
left_wp_1 --"padding (3,3)" --> left_pad_2("left_pad [None, 50, 46, 1]")
right_wp_1 --"padding (3,3)" --> right_pad_2("right_pad [None, 50, 46, 1]")

left_pad_2 --"convolution (50, 4, 50)"--> left_conv_trans_2("left_conv_trans [None, 50, 43, 1]")
right_pad_2 --"convolution (50, 4, 50)"--> right_conv_trans_2("right_conv_trans [None, 50, 43, 1]")

left_conv_trans_2 -- "w_pool (1,4)" --> left_wp_2("left_wp [None, 50, 40, 1]")
left_conv_trans_2 -- "all_pool (1, 43)" --> left_ap_2("left_ap [None, 50]")


right_conv_trans_2 -- "w_pool (1,4)" --> right_wp_2("right_wp [None, 50, 40, 1]")
right_conv_trans_2 -- "all_pool (1, 43)" --> right_ap_2("right_ap [None, 50]")
end

left_ap_0 --cosine -------> sims_0("sims_0 [None, ]")
right_ap_0 --cosine -------> sims_0


left_ap_1 --cosine -----> sims_1("sims_1 [None, ]")
right_ap_1 --cosine -----> sims_1


left_ap_2 --cosine --> sims_2("sims_2 [None, ]")
right_ap_2 --cosine --> sims_2

subgraph Output Layer

sims_0 --stack--> stack_sim("stack_sim [None, 3]")
sims_1 --stack--> stack_sim
sims_2 --stack--> stack_sim

features("features [None, 4]") --concat--> output_features("output_features [None, 7]")
stack_sim --concat--> output_features

output_features --"full connected" --> output("output [None, 2]")
output --softmax--> softmax("softmax [None, ]")

end</pre>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a href="https://aclanthology.org/Q16-1019.pdf">文献：ABCNN:
Attention-Based Convolutional Neural Network for Modeling Sentence
Pairs</a></li>
<li><a
href="https://zhuanlan.zhihu.com/p/50160263">注意力机制之ABCNN</a></li>
<li><a
href="https://www.jianshu.com/p/bb366027978a">文献阅读笔记：ABCNN:
Attention-Based Convolutional Neural Network for Modeling Sentence
Pairs</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>文本匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention详解</title>
    <url>/2021/07/23/Attention%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>Attention机制模仿生物观察行为的内部过程，将内部经验和外部对齐，从而增加部分区域的观察精细度。</p>
<span id="more"></span>
<h1 id="attention机制">Attention机制</h1>
<p>在一般的Encoder-Decoder框架中，模型会将所有输入的<span
class="math inline">\(X\)</span>都转化成语义表示<span
class="math inline">\(C\)</span>，这将导致Decoder出来的每个字都是同权的考虑了输入中的所有词。例如<code>Tom chase Jerry</code>目标翻译的结果是：<code>汤姆追逐杰瑞</code>。在未考虑注意力机制的模型中，<code>汤姆</code>这个词的翻译收到<code>Tom</code>、<code>chase</code>和<code>Jerry</code>三个词同权重的影响。但实际上，<code>汤姆</code>这个词的翻译应该受到<code>Tom</code>这个词的影响最大！</p>
<p>在带有Attention机制的Encoder-Decoder模型需要从序列中学习到每一个元素的重要成都，然后按照重要程度将元素合并。因此，注意力机制可以看作是
Encoder 和 Decoder 之间的接口，它向 Decoder 提供来自每个 Encoder
隐藏状态的信息。通过该设置，模型能够选择性地关注输入序列的有用部分，从而学习它们之间的“对齐”。这就表明，在
Encoder 将输入的序列元素进行编码时，得到的不在是一个固定的语义编码 C
，而是存在多个语义编码，且不同的语义编码由不同的序列元素以不同的权重参数组合而成。一个简单地体现
Attention 机制运行的示意图如下：</p>
<figure>
<img src="encoder-decoder.png" alt="Encoder-Decoder" />
<figcaption aria-hidden="true">Encoder-Decoder</figcaption>
</figure>
<p>在 Attention 机制下，语义编码 <span class="math inline">\(C\)</span>
就不在是输入序列 <span class="math inline">\(X\)</span>
的直接编码了，而是各个元素按其重要程度加权求和得到的，即 <span
class="math display">\[
C_i=\sum^{T_x}_{j=0}{a_{ij}f(x_j)}
\]</span> 其中，<span class="math inline">\(i\)</span>表示时刻，<span
class="math inline">\(j\)</span> 表示序列中的第 <span
class="math inline">\(j\)</span> 个元素， <span
class="math inline">\(T_x\)</span> 表示序列的长度， <span
class="math inline">\(f(⋅)\)</span>表示对元素 <span
class="math inline">\(x_j\)</span>x的编码。<span
class="math inline">\(a_{ij}\)</span>可以看作是一个概率，反映了元素
<span class="math inline">\(h_j\)</span> 对 <span
class="math inline">\(C_i\)</span> 的重要性，可以使用 softmax 来表示：
<span class="math display">\[
a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}
\]</span> 这里<span class="math inline">\(e_{ij}\)</span>
正是反映了待编码的元素和其它元素之间的匹配度，当匹配度越高时，说明该元素对其的影响越大，则
<span class="math inline">\(a_{ij}\)</span> 的值也就越大。</p>
<p>因此，得出<span class="math inline">\(a_{ij}\)</span>
的过程如下图：</p>
<figure>
<img src="attention_matrix.png" alt="Attention Matrix" />
<figcaption aria-hidden="true">Attention Matrix</figcaption>
</figure>
<p>其中，<span class="math inline">\(h_i\)</span>表示 Encoder
的转换函数，<span class="math inline">\(F(h_j,H_i)\)</span>
表示预测与目标的匹配打分函数。将以上过程串联起来，则注意力模型的结构如下图所示：</p>
<figure>
<img src="attention_architecture.png" alt="Attention Architecture" />
<figcaption aria-hidden="true">Attention Architecture</figcaption>
</figure>
<h1 id="attention原理">Attention原理</h1>
<p>Attention 机制的一个重点就是获得 attention
value，即机器翻译中的语义编码 <span
class="math inline">\(C_i\)</span>。在上一节中我们知道该值是通过输入元素按照不同的权重参数组合而成的，所以我们可以将其定义为一个
attention 函数，比较主流的 attention
函数的机制是采用键值对查询的方式，其工作实质如下图所示：</p>
<figure>
<img src="attention_qkv.png" alt="Attention QKV" />
<figcaption aria-hidden="true">Attention QKV</figcaption>
</figure>
<p>在自然语言任务中，往往 Key 和 Value
是相同的。需要注意的是，计算出来的 attention value
是一个向量，代表序列元素 <span class="math inline">\(x_j\)</span>
的编码向量，包含了元素 <span class="math inline">\(x_j\)</span>
的上下文关系，即同时包含全局联系和局部联系。全局联系很好理解，因为在计算时考虑了该元素与其他所有元素的相似度计算；而局部联系则是因为在对元素
<span class="math inline">\(x_j\)</span>
进行编码时，重点考虑与其相似度较高的局部元素，尤其是其本身。</p>
<p><strong>Step 1：准备隐藏状态</strong></p>
<p>首先准备第一个 Decoder 的隐藏层状态（红色）和所有可用的 Encoder
隐藏层状态（绿色）。在示例中，有 4 个 Encoder 隐藏状态和 1 个 Decoder
隐藏状态。</p>
<figure>
<img src="attention1.gif" alt="Attention1" />
<figcaption aria-hidden="true">Attention1</figcaption>
</figure>
<p><strong>Step 2：得到每一个 Encoder 隐藏状态的得分</strong></p>
<p>分值（score）由 <code>score</code> 函数来获得，最简单的方法是直接用
Decoder 隐藏状态和 Encoder 中的每一个隐藏状态进行点积。</p>
<figure>
<img src="attention2.gif" alt="Attention2" />
<figcaption aria-hidden="true">Attention2</figcaption>
</figure>
<p><strong>Step 3：将所有得分送入 softmax 层</strong></p>
<p>该部分实质上就是对得到的所有分值进行归一化，这样 <code>softmax</code>
之后得到的所有分数相加为
1。而且能够使得原本分值越高的隐藏状态，其对应的概率也越大，从而抑制那些无效或者噪音信息。</p>
<figure>
<img src="attention3.gif" alt="Attention3" />
<figcaption aria-hidden="true">Attention3</figcaption>
</figure>
<p><strong>Step 4：用每个 Encoder 的隐藏状态乘以 softmax
之后的得分</strong></p>
<p>通过将每个编码器的隐藏状态与其softmax之后的分数(标量)相乘，我们得到
对齐向量 或标注向量。这正是对齐产生的机制。</p>
<figure>
<img src="attention4.gif" alt="Attention4" />
<figcaption aria-hidden="true">Attention4</figcaption>
</figure>
<p><strong>Step 5：将所有对齐的向量进行累加</strong></p>
<p>对对齐向量进行求和，生成 <em>上下文向量</em>
。上下文向量是前一步的对齐向量的聚合信息。</p>
<figure>
<img src="attention5.gif" alt="Attention5" />
<figcaption aria-hidden="true">Attention5</figcaption>
</figure>
<p><strong>Step 6：把上下文向量送到 Decoder 中</strong></p>
<p>通过将上下文向量和 Decoder
的上一个隐藏状态一起送入当前的隐藏状态，从而得到解码后的输出。</p>
<figure>
<img src="attention6.gif" alt="Attention6" />
<figcaption aria-hidden="true">Attention6</figcaption>
</figure>
<p>最终得到完整的注意力层结构如下图所示：</p>
<figure>
<img src="attention.png" alt="Attention" />
<figcaption aria-hidden="true">Attention</figcaption>
</figure>
<h1 id="其他理解">其他理解</h1>
<p>Q就是词的查询向量，K是“被查”向量，V是内容向量。</p>
<p>简单来说一句话：Q是最适合查找目标的，K是最适合接收查找的，V就是内容，这三者不一定要一致，所以网络这么设置了三个向量，然后学习出最适合的Q,
K, V，以此增强网络的能力。</p>
<p>主要要理解Q，K的意义，可以<strong>类比搜索</strong>的过程：</p>
<p>假设我们想查一篇文章，我们不会直接把文章的内容打上去，而是会在搜索框输入该文章的<strong>关键字</strong>，如果我们搜不到，我们往往会再换一个关键字，直到搜到为止，那么可以让我们搜到的关键字就是<strong>最适合查找目标文章的关键字</strong>。这个<strong>最适合查找目标文章的关键字就是Q。</strong></p>
<p>那么搜索引擎拿到我们输入的关键字Q之后，就会把Q和库里面的文章对比，当然搜索引擎为了节省资源加快对比速度，提前把库里面的文章进行了处理提取了<strong>关键信息</strong>，关键信息有很多，那么那个关键信息能够使得搜索命中率高，那个就是<strong>最适合接收查找的关键信息，</strong>这个<strong>最适合接收查找的关键信息就是K</strong>。</p>
<p>使用Q和K计算了相似度之后得到score，这就是相似度评分，之后有了相似度评分，就可以把内容V加权回去了。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&amp;mid=2247485860&amp;idx=1&amp;sn=e926a739784090b3779711164217b968&amp;chksm=c06981f9f71e08efb5f57441444f71a09f1d27fc667af656a5ad1173e32ad394201d02195a3a&amp;mpshare=1&amp;scene=1&amp;srcid=0618HMAYi4gzzwWfedLoOuSD&amp;key=cb6098335ab487a8ec84c95399379f16f975d33ce91588d73ecf857c54b543666b5927e231ad3a9b17bff0c20fff20fc49c262912dca050dee9465801de8a4cdc79e3d8f4fbc058345331fb691bcbacb&amp;ascene=1&amp;uin=MTE3NTM4MTY0NA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=ikhBXxX7PL%2Fal9hbIGXbRFA96ei74EF%2BcP8KdbP6UcV6mIpOfPWzVuju%2Bqw86q5r">动画图解Attention机制，让你一看就明白</a></li>
<li><a
href="https://www.cnblogs.com/ydcode/p/11038064.html">浅谈Attention机制的理解</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>ESIM</title>
    <url>/2021/07/08/ESIM/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>模型</tag>
        <tag>文本匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM原理</title>
    <url>/2021/07/28/LSTM%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://www.cnblogs.com/stephen-goodboy/p/12773466.html">RNN和LSTM模型详解</a></li>
<li><a
href="https://zhuanlan.zhihu.com/p/109519044">为什么LSTM会减缓梯度消失？</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>lstm</tag>
      </tags>
  </entry>
  <entry>
    <title>Elastic Search</title>
    <url>/2021/07/02/Elastic-Search/</url>
    <content><![CDATA[<p>Elasticsearch（简称ES）是一个分布式、可扩展、实时的搜索与数据分析引擎。</p>
<span id="more"></span>
<p>todo:</p>
<ol type="1">
<li>索引过程</li>
<li>检索过程</li>
<li>分数怎么计算的</li>
</ol>
<h1 id="传统数据库的问题">传统数据库的问题</h1>
<p>随着访问量的上升，几乎大部分使用 MySQL
架构的网站在数据库上都开始出现了性能问题。</p>
<p><strong>读写分离</strong>
由于数据库的写入压力增加，读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从复制技术来达到读写分离，以提高读写性能和读库的可扩展性。Mysql
的 master-slave 模式成为这个时候的网站标配了。</p>
<p><strong>分表分库</strong>
开始流行使用分表分库来缓解写压力和数据增长的扩展问题。这个时候，分表分库成了一个热门技术，也是业界讨论的热门技术问题。</p>
<p><strong>MySQL 的扩展性瓶颈</strong> 大数据量高并发环境下的 MySQL
应用开发越来越复杂，也越来越具有技术挑战性。分表分库的规则把握都是需要经验的。分库分表的子库到一定阶段又面临扩展问题。还有就是需求的变更，可能又需要一种新的分库方式。</p>
<h1 id="搜索引擎原理">搜索引擎原理</h1>
<p>一次检索大致可分为四步：</p>
<ol type="1">
<li><p><strong>查询分析</strong>
正常情况下用户输入正确的查询，比如搜索“里约奥运会”这个关键词，用户输入正确完成一次搜索，但是搜索通常都是全开放的，任何的用户输入都是有可能的，很大一部分还是非常口语化和个性化的，有时候还会存在拼写错误，用户不小心把“淘宝”打成“涛宝”，这时候需要用自然语言处理技术来做拼写纠错等处理，以正确理解用户需求。</p></li>
<li><p><strong>分词技术</strong>
这一步利用自然语言处理技术将用户输入的查询语句进行分词，如标准分词会把“lucene全文检索框架”分成
lucene | 全 | 文｜检｜索｜框｜架｜， IK分词会分成：
lucene｜全文｜检索｜框架｜，还有简单分词等多种分词方法。</p></li>
<li><p><strong>关键词检索</strong>
提交关键词后在倒排索引库中进行匹配，倒排索引就是关键词和文档之间的对应关系，就像给文档贴上标签。比如在文档集中含有
"lucene" 关键词的有文档1 、文档 6、文档9，含有 "全文检索"
关键词的有文档1 、文档6 那么做与运算，同时含有 "lucene" 和 "全文检索"
的文档就是文档1和文档6，在实际的搜索中会有更复杂的文档匹配模型。</p></li>
<li><p><strong>搜索排序</strong>
对多个相关文档进行相关度计算、排序，返回给用户检索结果。</p></li>
</ol>
<h1 id="倒排索引">倒排索引</h1>
<p>倒排索引，也常被称为反向索引，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射，它是文档检索系统中最常用的数据结构。</p>
<p>下面我们通过具体实例深入理解倒排索引，通过简单文档以小见大，体验倒排索引的建过程。</p>
<table>
<thead>
<tr class="header">
<th>文档ID</th>
<th>文档内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>人工智能成为互联网大会焦点</td>
</tr>
<tr class="even">
<td>2</td>
<td>谷歌推出开源人工智能系统工具</td>
</tr>
<tr class="odd">
<td>3</td>
<td>互联网的未来在人工智能</td>
</tr>
<tr class="even">
<td>4</td>
<td>谷歌开源机器学习工具</td>
</tr>
</tbody>
</table>
<p>对于文档内容，先要经过词条化处理。与英文不同的是，英文通过空格分隔单词，中文的词与词之间没有明确的分隔符号，经过分词系统进行中文分词以后把矩阵切分成一个个的词条。</p>
<table>
<thead>
<tr class="header">
<th>词项</th>
<th>文档频率</th>
<th>倒排记录表</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>人工</td>
<td>3</td>
<td>1,2,3</td>
</tr>
<tr class="even">
<td>智能</td>
<td>3</td>
<td>1,2,3</td>
</tr>
<tr class="odd">
<td>成为</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>互联网</td>
<td>2</td>
<td>1,3</td>
</tr>
<tr class="odd">
<td>大会</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>焦点</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>谷歌</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr class="even">
<td>推出</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="odd">
<td>开源</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr class="even">
<td>系统</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="odd">
<td>工具</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr class="even">
<td>的</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="odd">
<td>未来</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>在</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="odd">
<td>机器</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="even">
<td>学习</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>我们需要对单词进行排序，像 B+ 树一样，可以在页里实现二分查找。</p>
<figure>
<img src="InvertIndex.png" alt="inverted index" />
<figcaption aria-hidden="true">inverted index</figcaption>
</figure>
<p>Lucene 的倒排索引，增加了最左边的一层「字典树」term
index，它不存储所有的单词，只存储单词前缀，通过字典树找到单词所在的块，也就是单词的大概位置，再在块里二分查找，找到对应的单词，再找到单词对应的文档列表。</p>
<p>Lucene
的实现会要更加复杂，针对不同的数据结构采用不同的字典索引，使用了FST模型、BKDTree等结构。</p>
<p>真实的倒排记录也并非一个链表，而是采用了SkipList、BitSet等结构。</p>
<h1 id="elasticsearc索引">ElasticSearc索引</h1>
<h2 id="索引的不变性"><strong>索引的不变性</strong></h2>
<p>由于倒排索引的结构特性，在索引建立完成后对其进行修改将会非常复杂。再加上几层索引嵌套，更让索引的更新变成了几乎不可能的动作。
所以索性设计成不可改变的：倒排索引被写入磁盘后是不可改变的，它永远不会修改。</p>
<p><strong>优点</strong>：</p>
<ol type="1">
<li>不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。</li>
<li>一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。</li>
<li>其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。</li>
<li>写入单个大的倒排索引允许数据压缩，减少磁盘 I/O 和
需要被缓存到内存的索引的使用量。</li>
</ol>
<p><strong>缺点：</strong></p>
<p>主要事实是它是不可变的，你不能修改它。如果你需要让一个新的文档
可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。</p>
<h2 id="动态更新索引">动态更新索引</h2>
<p>怎样在保留不变性的前提下实现倒排索引的更新？答案是:
<strong>用更多的索引</strong>。</p>
<p>Elasticsearch 基于 Lucene, 引入了 <em>按段搜索</em> 的概念。 每一
<em>段</em> 本身都是一个倒排索引， 但 <em>索引</em> 在 Lucene
中除表示所有 <em>段</em> 的集合外， 还增加了 <em>提交点</em> 的概念 —
一个列出了所有已知段的文件，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段。</p>
<p>在 lucene 中查询是基于 segment。每个 segment 可以看做是一个独立的
subindex，在建立索引的过程中，lucene 会不断的 flush
内存中的数据持久化形成新的 segment。多个 segment 也会不断的被 merge
成一个大的 segment，在老的 segment
还有查询在读取的时候，不会被删除，没有被读取且被 merge 的 segement
会被删除。</p>
<figure>
<img src="index_segment.png" alt="Index Segment" />
<figcaption aria-hidden="true">Index Segment</figcaption>
</figure>
<p><strong>索引更新过程</strong>：</p>
<ol type="1">
<li><p>数据先写入内存buffer，在写入buffer的同时将数据写入translog日志文件，注意：此时数据还没有被成功es索引记录，因此无法搜索到对应数据；</p></li>
<li><p>如果buffer快满了或者到一定时间，es就会将buffer数据refresh到一个新的segment
file中，但是此时数据不是直接进入segment file的磁盘文件，而是先进入os
cache的。这个过程就是<strong>refresh</strong>。一旦buffer中的数据被refresh操作，刷入os
cache中，就代表这个数据就可以被搜索到了。</p>
<p>每隔1秒钟，es将buffer中的数据写入一个新的segment
file，因此每秒钟会产生一个新的磁盘文件segment file，这个segment
file中就存储最近1秒内buffer中写入的数据。</p>
<p>这就是为什么es被称为准实时（NRT，near
real-time）：因为写入的数据默认每隔1秒refresh一次，也就是数据每隔一秒才能被
es 搜索到，之后才能被看到，所以称为准实时。</p>
<p>只要数据被输入os
cache中，buffer就会被清空，并且数据在translog日志文件里面持久化到磁盘了一份，此时就可以让这个segment
file的数据对外提供搜索了。</p></li>
<li><p>重复1~2步骤，新的数据不断进入buffer和translog，不断将buffer数据写入一个又一个新的segment
file中去，每次refresh完，buffer就会被清空，同时translog保留一份日志数据。随着这个过程推进，translog文件会不断变大。当translog文件达到一定程度时，就会执行commit操作。</p></li>
<li><p>commit操作发生第一步，就是将buffer中现有数据refresh到os
cache中去，清空buffer。将一个 commit point 写入磁盘文件，里面标识着这个
commit point 对应的所有 segment file，同时强行将 os cache
中目前所有的数据都 fsync
到磁盘文件中去。将现有的translog清空，然后再次重启启用一个translog，此时commit操作完成。</p></li>
</ol>
<h1 id="文档的更新与删除">文档的更新与删除</h1>
<h2 id="删除">删除</h2>
<p>段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。</p>
<p>磁盘上的每个segment都有一个.del文件与它相关联。当发送删除请求时，该文档未被真正删除，而是在.del文件中标记为已删除。此文档可能仍然能被搜索到，但会从结果中过滤掉。当segment合并时，在.del文件中标记为已删除的文档不会被包括在新的segment中，也就是说merge的时候会真正删除被删除的文档。</p>
<h2 id="更新">更新</h2>
<p>创建新文档时，Elasticsearch将为该文档分配一个版本号。对文档的每次更改都会产生一个新的版本号。当执行更新时，旧版本在.del文件中被标记为已删除，并且新版本在新的segment中写入索引。旧版本可能仍然与搜索查询匹配，但是从结果中将其过滤掉。</p>
<h1 id="elasticsearch集群原理">ElasticSearch集群原理</h1>
<figure>
<img src="es_cluster.png" alt="es-cluster" />
<figcaption aria-hidden="true">es-cluster</figcaption>
</figure>
<p><strong>Document</strong>：文档，指一行数据；</p>
<p><strong>Index</strong>：索引，是多个document的集合（和sql数据库的表对应)；</p>
<p><strong>Shard</strong>：分片，当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的</p>
<p><strong>Replia</strong>：副本，为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。</p>
<p><strong>Node</strong>：节点，形成集群的每个服务器称为节点，一个节点可以包含多个shard</p>
<p><strong>Cluster</strong>：集群，ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。</p>
<h2 id="集群节点角色">集群节点角色</h2>
<figure>
<img src="cluster_role.png" alt="cluster role" />
<figcaption aria-hidden="true">cluster role</figcaption>
</figure>
<p>ES集群的服务器主要分为以下三种角色：</p>
<ol type="1">
<li><p><strong>master节点</strong>：负责保存和更新集群的一些元数据信息，之后同步到所有节点，所以每个节点都需要保存全量的元数据信息，包括集群的配置信息、集群的节点信息、模板template设置、索引以及对应的设置、mapping、分词器和别名、索引关联到的分片以及分配到的节点等配置</p>
<blockquote>
<p><strong>master选举</strong></p>
<p><strong>选举策略</strong></p>
<p>如果集群中存在master，认可该master，加入集群，如果集群中不存在master，从具有master资格的节点中选id最小的节点作为master</p>
<p><strong>选举时机</strong></p>
<p>集群启动：后台启动线程去ping集群中的节点，按照上述策略从具有master资格的节点中选举出master</p>
<p>现有的master离开集群：后台一直有一个线程定时ping
master节点，超过一定次数没有ping成功之后，重新进行master的选举</p>
<p><strong>避免脑裂</strong></p>
<p>脑裂问题是采用master-slave模式的分布式集群普遍需要关注的问题，脑裂一旦出现，会导致集群的状态出现不一致，导致数据错误甚至丢失。</p>
<p>ES避免脑裂的策略：过半原则，可以在ES的集群配置中添加一下配置，避免脑裂的发生</p>
</blockquote></li>
<li><p><strong>data节点</strong>：负责数据存储和查询</p></li>
<li><p><strong>coordinator节点</strong>：路由索引请求、聚合搜索结果集、分发批量索引请求</p></li>
</ol>
<h2 id="路由机制">路由机制</h2>
<p>当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch
如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片
1 还是分片 2 中呢？</p>
<p>首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：</p>
<p>shard = hash(routing) % number_of_primary_shards</p>
<p>routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。
routing 通过 hash 函数生成一个数字，然后这个数字再除以
number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到
number_of_primary_shards-1
之间的余数，就是我们所寻求的文档所在分片的位置。</p>
<p>这就解释了为什么我们要在创建索引的时候就确定好主分片的数量
并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。</p>
<h2 id="新建索引删除文档">新建、索引、删除文档</h2>
<p>新建、索引和删除请求都是写操作，
必须在主分片上面完成之后才能被复制到相关的副本分片。</p>
<figure>
<img src="document_operation.png" alt="document operation" />
<figcaption aria-hidden="true">document operation</figcaption>
</figure>
<p>以下是在主副分片和任何副本分片上面
成功新建，索引和删除文档所需要的步骤顺序：</p>
<ol type="1">
<li>客户端向 Node 1 发送新建、索引或者删除请求。</li>
<li>节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node
3，因为分片 0 的主分片目前被分配在 Node 3 上。</li>
<li>Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1
和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3
将向协调节点报告成功，协调节点向客户端报告成功。</li>
</ol>
<h2 id="查询文档">查询文档</h2>
<p>可以从主分片或者从其它任意副本分片检索文档</p>
<figure>
<img src="document_query.png" alt="doucment query" />
<figcaption aria-hidden="true">doucment query</figcaption>
</figure>
<p>以下是从主分片或者副本分片检索文档的步骤顺序：</p>
<ol type="1">
<li>客户端向 Node 1 发送获取请求。</li>
<li>节点使用文档的 _id 来确定文档属于分片 0 。分片 0
的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2
。</li>
<li>Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。</li>
</ol>
<p>在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。</p>
<p>在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。
在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。
一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。</p>
<h2 id="更新文档">更新文档</h2>
<figure>
<img src="document_update.png" alt="document update" />
<figcaption aria-hidden="true">document update</figcaption>
</figure>
<p>以下是部分更新一个文档的步骤：</p>
<ol type="1">
<li>客户端向 Node 1 发送更新请求。</li>
<li>它将请求转发到主分片所在的 Node 3 。</li>
<li>Node 3 从主分片检索文档，修改 _source 字段中的 JSON
，并且尝试重新索引主分片的文档。
如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict
次后放弃。</li>
<li>如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和
Node 2 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， Node 3
向协调节点也返回成功，协调节点向客户端返回成功。</li>
</ol>
<h2 id="分布式检索">分布式检索</h2>
<p>搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档:
这些文档有可能在集群的任何分片上。一个搜索请求必须询问我们关注的索引（index
or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档。</p>
<p>但是找到所有的匹配文档仅仅完成事情的一半。在 search 接口返回一个 page
结果之前，多分片中的结果必须组合成单个排序列表。
为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch 。</p>
<h3 id="查询阶段"><strong>查询阶段</strong></h3>
<p>在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。
每个分片在本地执行搜索并构建一个匹配文档的优先队列。</p>
<figure>
<img src="query.png" alt="query" />
<figcaption aria-hidden="true">query</figcaption>
</figure>
<p>查询阶段包含以下三个步骤:</p>
<ol type="1">
<li>客户端发送一个 <code>search</code> 请求到 <code>Node 3</code> ，
<code>Node 3</code> 会创建一个大小为 <code>from + size</code>
的空优先队列。</li>
<li><code>Node 3</code>
将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为
<code>from + size</code> 的本地有序优先队列中。</li>
<li>每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是
<code>Node 3</code>
，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li>
</ol>
<p>当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。
这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。</p>
<p>协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。</p>
<h3 id="取回阶段"><strong>取回阶段</strong></h3>
<p>查询阶段标识哪些文档满足搜索请求，但是我们仍然需要取回这些文档，这是取回阶段的任务。</p>
<figure>
<img src="fetch.png" alt="fetch" />
<figcaption aria-hidden="true">fetch</figcaption>
</figure>
<p>分布式阶段由以下步骤构成：</p>
<ol type="1">
<li>协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET
请求。</li>
<li>每个分片加载并 丰富
文档，如果有需要的话，接着返回文档给协调节点。</li>
<li>一旦所有的文档都被取回了，协调节点返回结果给客户端。</li>
</ol>
<h1 id="常见问题">常见问题</h1>
<h2 id="分片的设定">分片的设定</h2>
<p>分片数过小，数据写入形成瓶颈，无法水平拓展</p>
<p>分片数过多，每个分片都是一个lucene的索引，分片过多将会占用过多资源</p>
<p>如何计算分片数</p>
<p>需要注意分片数量最好设置为节点数的整数倍，保证每一个主机的负载是差不多一样的，特别的，如果是一个主机部署多个实例的情况，更要注意这一点，否则可能遇到其他主机负载正常，就某个主机负载特别高的情况。</p>
<p>一般我们根据每天的数据量来计算分片，保持每个分片的大小在 50G
以下比较合理。如果还不能满足要求，那么可能需要在索引层面通过拆分更多的索引或者通过别名
+ 按小时 创建索引的方式来实现了。</p>
<h2 id="相关度评分">相关度评分</h2>
<p>Lucene（或 Elasticsearch）使用 <em>布尔模型（Boolean model）</em>
查找匹配文档，并用一个名为 <em>实用评分函数（practical scoring
function）</em>的公式来计算相关度。这个公式借鉴了<em>词频/逆向文档频率（term
frequency/inverse document frequency）</em> 和 <em>向量空间模型（vector
space
model）</em>，同时也加入了一些现代的新特性，如协调因子（coordination
factor），字段长度归一化（field length
normalization），以及词或查询语句权重提升。</p>
<h3 id="布尔模型">布尔模型</h3>
<p><em>布尔模型（Boolean Model）</em> 只是在查询中使用 <code>AND</code>
、 <code>OR</code> 和 <code>NOT</code>
（与、或和非）这样的条件来查找匹配的文档，以下查询：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">full AND text AND search AND (elasticsearch OR lucene)</span><br></pre></td></tr></table></figure>
<p>会将所有包括词 <code>full</code> 、 <code>text</code> 和
<code>search</code> ，以及 <code>elasticsearch</code> 或
<code>lucene</code> 的文档作为结果集。</p>
<p>这个过程简单且快速，它将所有可能不匹配的文档排除在外。</p>
<h3 id="词频逆向文档频率tfidf">词频/逆向文档频率（TF/IDF）</h3>
<p>当匹配到一组文档后，需要根据相关度排序这些文档，不是所有的文档都包含所有词，有些词比其他的词更重要。一个文档的相关度评分部分取决于每个查询词在文档中的
<em>权重</em> 。</p>
<p>词的权重由三个因素决定，有兴趣可以了解下面的公式，但并不要求记住。</p>
<h4 id="词频">词频</h4>
<p>词在文档中出现的频度是多少？频度越高，权重 <em>越高</em> 。 5
次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tf(t in d) = √frequency </span><br></pre></td></tr></table></figure>
<blockquote>
<p>词 <code>t</code> 在文档 <code>d</code> 的词频（ <code>tf</code>
）是该词在文档中出现次数的平方根。</p>
</blockquote>
<h4 id="逆向文档频率">逆向文档频率</h4>
<p>词在集合所有文档里出现的频率是多少？频次越高，权重 <em>越低</em>
。常用词如 <code>and</code> 或 <code>the</code>
对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如
<code>elastic</code> 或 <code>hippopotamus</code>
可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">idf(t) = 1 + log ( numDocs / (docFreq + 1)) </span><br></pre></td></tr></table></figure>
<blockquote>
<p>词 <code>t</code> 的逆向文档频率（ <code>idf</code>
）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。</p>
</blockquote>
<h4 id="字段长度归一值">字段长度归一值</h4>
<p>字段的长度是多少？字段越短，字段的权重 <em>越高</em>
。如果词出现在类似标题 <code>title</code> 这样的字段，要比它出现在内容
<code>body</code>
这样的字段中的相关度更高。字段长度的归一值公式如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">norm(d) = 1 / √numTerms </span><br></pre></td></tr></table></figure>
<blockquote>
<p>字段长度归一值（ <code>norm</code> ）是字段中词数平方根的倒数。</p>
</blockquote>
<p>字段长度的归一值对全文搜索非常重要，许多其他字段不需要有归一值。无论文档是否包括这个字段，索引中每个文档的每个
<code>string</code> 字段都大约占用 1 个 byte 的空间。对于
<code>not_analyzed</code> 字符串字段的归一值默认是禁用的，而对于
<code>analyzed</code> 字段也可以通过修改字段映射禁用归一值：</p>
<p>对于有些应用场景如日志，归一值不是很有用，要关心的只是字段是否包含特殊的错误码或者特定的浏览器唯一标识符。字段的长度对结果没有影响，禁用归一值可以节省大量内存空间。</p>
<h4 id="结合使用">结合使用</h4>
<p>以下三个因素——词频（term frequency）、逆向文档频率（inverse document
frequency）和字段长度归一值（field-length
norm）——是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的
<em>权重</em> 。</p>
<p>当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector
space model）。</p>
<h3 id="向量空间模型">向量空间模型</h3>
<p>在向量空间模型里，向量空间模型里的每个数字都代表一个词的
<em>权重</em> ，与 词频/逆向文档频率（term frequency/inverse document
frequency） 计算方式类似。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://www.jianshu.com/p/52b92f1a9c47">理解ElasticSearch工作原理</a></li>
<li><a
href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/scoring-theory.html">相关度评分背后的理论</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>后台开发</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>es</tag>
        <tag>倒排索引</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解</title>
    <url>/2021/07/23/Transformer%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://www.cnblogs.com/mantch/p/11591937.html">Transformer各层网络结构详解！</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>bert原理</title>
    <url>/2021/07/23/bert%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder
<strong>R</strong>epresentations from
<strong>T</strong>ransformers）是一种<strong>预训练模型</strong>，旨在通过考虑所有层中的<strong>双侧上下文</strong>信息来得到<strong>深度的双向表示</strong>。该表示连接上一层输出层后，仅需微调训练就可以在很多
NLP 任务中取得惊人的效果。</p>
<span id="more"></span>
<h1 id="bert结构">Bert结构</h1>
<h2 id="bert的输入">Bert的输入</h2>
<p>在BERT中，输入的向量是由三种不同的embedding求和而成，分别是：</p>
<ol type="1">
<li>token embedding：单词本身的向量表示</li>
<li>segment embedding：用于区分两个句子的向量表示</li>
<li>position embedding：单词位置信息的编码表示</li>
</ol>
<figure>
<img src="bert_input.png" alt="bert input" />
<figcaption aria-hidden="true">bert input</figcaption>
</figure>
<h3 id="token-embedding">Token embedding</h3>
<p>token embedding
层是要将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示。</p>
<figure>
<img src="token_embedding.png" alt="token embedding" />
<figcaption aria-hidden="true">token embedding</figcaption>
</figure>
<p>输入文本在送入token embeddings
层之前要先进行tokenization处理。此外，两个特殊的token会被插入到tokenization的结果的开头
([CLS])和结尾 ([SEP]) 。它们视为后面的分类任务和划分句子对服务的。</p>
<p>Token Embeddings 层会将每一个wordpiece
token转换成768维的向量。这样，例子中的6个token就被转换成了一个(6, 768)
的矩阵或者是(1, 6, 768)的张量（如果考虑batch_size的话）。</p>
<h3 id="segment-embedding">Segment embedding</h3>
<p>BERT
能够处理对输入句子对的分类任务。这类任务就像判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入到模型中。那BERT如何去区分一个句子对中的两个句子呢？答案就是segment
embeddings.</p>
<figure>
<img src="segment_embedding.png" alt="segment embedding" />
<figcaption aria-hidden="true">segment embedding</figcaption>
</figure>
<p>Segment Embeddings
层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token,
后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment
embedding就是全0。</p>
<h3 id="position-embeddings">Position Embeddings</h3>
<p>Transformers无法编码输入的序列的顺序性，加入position
embeddings会让BERT理解输入句子的位置信息。</p>
<p><strong>如何设计positional embedding？</strong></p>
<ol type="1">
<li>为每个时间步（单词在句子中的位置）输出唯一的编码</li>
<li>即便句子长度不一，句子中两个时间步之间的距离应该是“恒定”的</li>
<li>模型可以轻易泛化到更长的句子上</li>
<li>PE必须是确定的</li>
</ol>
<p>论文中采用的positional
embedding：<strong>偶数位置，使用正弦编码，在奇数位置，使用余弦编码</strong>
<span class="math display">\[
PE(pos,2i)=sin(\frac{pos}{10000^{2i/d_{model}}})
\]</span></p>
<p><span class="math display">\[
PE(pos,2i+1)=cos(\frac{pos}{10000^{2i/d_{model}}})
\]</span></p>
<p>BERT能够处理最长512个token的输入序列。论文作者通过让BERT在各个位置上学习一个向量表示来将序列顺序的信息编码进来。这意味着Position
Embeddings layer 实际上就是一个大小为 (512, 768)
的lookup表，表的第一行是代表第一个序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子“Hello
world” 和“Hi there”, “Hello” 和“Hi”会由完全相同的position
embeddings，因为他们都是句子的第一个词。同理，“world”
和“there”也会有相同的position embedding。</p>
<h2 id="网络结构">网络结构</h2>
<p>BERT的主要结构是transformer，一个BERT预训练模型的基础结构是标准transformer结构的encoder部分，一个标准transformer结构如下图所示，其中右边的部分就是BERT中使用的encoder部分。</p>
<figure>
<img src="bert_architecture.png" alt="bert architecture" />
<figcaption aria-hidden="true">bert architecture</figcaption>
</figure>
<p>一个transformer的encoder单元由一个multi-head-Attention + Layer
Normalization + feedforword + Layer Normalization
叠加产生，BERT的每一层由一个这样的encoder单元构成。在比较大的BERT模型中，有24层encoder，每层中有16个Attention，词向量的维度是1024。在比较小的BERT模型中，有12层encoder，每层有12个Attention，词向量维度是768。在所有情况下，将feed-forward/filter
的大小设置为 4H（H为词向量的维度），即H = 768时为3072，H =
1024时为4096。</p>
<h3 id="multi-headed-attention">Multi-Headed Attention</h3>
<h4 id="self-attention">Self Attention</h4>
<ol type="1">
<li><p><strong>self-attention出现的原因</strong></p>
<ul>
<li><p>为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中<strong>并行加速计算</strong>的问题</p></li>
<li><p>由于每个目标词是直接与句子中所有词分别计算相关度(attention)的，所以解决了传统的RNN模型中长距离依赖的问题，通过attention，可以将两个距离较远的词之间的距离拉近为1直接计算词的相关度，而传统的RNN模型中，随着距离的增加，词之间的相关度会被削弱。</p></li>
</ul></li>
<li><p><strong>单个self-attention 的计算过程</strong></p>
<p>self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路</p>
<ol type="1">
<li><p>首先，self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64。</p>
<figure>
<img src="self_attention_1.png" alt="self attention1" />
<figcaption aria-hidden="true">self attention1</figcaption>
</figure></li>
<li><p>计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点成，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2。</p>
<figure>
<img src="self_attention_2.png" alt="self attention2" />
<figcaption aria-hidden="true">self attention2</figcaption>
</figure></li>
<li><p>接下来，把点成的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大。</p>
<figure>
<img src="self_attention_3.png" alt="self attention3" />
<figcaption aria-hidden="true">self attention3</figcaption>
</figure></li>
<li><p>下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attention在当前节点的值。</p>
<figure>
<img src="self_attention_4.png" alt="self attention4" />
<figcaption aria-hidden="true">self attention4</figcaption>
</figure></li>
</ol>
<p>在实际的应用场景，为了提高计算速度，我们采用的是矩阵的方式，直接计算出Query,
Key, Value的矩阵，然后把embedding的值与三个矩阵直接相乘，把得到的新矩阵
Q 与 K 相乘，乘以一个常数，做softmax操作，最后乘上 V 矩阵。</p>
<p><strong>这种通过 query 和 key 的相似性程度来确定 value
的权重分布的方法被称为scaled dot-product attention。</strong></p>
<figure>
<img src="self_attention_matrix1.png" alt="self attention matrix 1" />
<figcaption aria-hidden="true">self attention matrix 1</figcaption>
</figure>
<figure>
<img src="self_attention_matrix2.png" alt="self attention matrix 2" />
<figcaption aria-hidden="true">self attention matrix 2</figcaption>
</figure></li>
</ol>
<blockquote>
<p>self
attention中Q、K、V都是通过一个线性变换得到，其维度可自定义，但一般定义成<span
class="math inline">\(embedding\_size \times
embedding\_size/head\)</span>​</p>
</blockquote>
<h4 id="multi-headed-attention-1">Multi-Headed Attention</h4>
<p>multi-headed
attention机制理解起来很简单，<strong>就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组</strong>，所以最后得到的结果是8个矩阵。</p>
<figure>
<img src="multi_headed_attention1.png" alt="multi-headed attention1" />
<figcaption aria-hidden="true">multi-headed attention1</figcaption>
</figure>
<figure>
<img src="multi_headed_attention2.png" alt="multi-headed attention2" />
<figcaption aria-hidden="true">multi-headed attention2</figcaption>
</figure>
<p>Multi-Head
Self-Attention<strong>将多个不同单头的</strong>Self-Attention输出<strong>Concat</strong>成一条，然后再经过一个全连接层降维输出，如下图所示，右边的部分即为一个multi-head
attention的计算过程，其中的h指的是attention的个数，即上面例子中的n。</p>
<figure>
<img src="multi_headed_attention3.jpg" alt="multi headed attention" />
<figcaption aria-hidden="true">multi headed attention</figcaption>
</figure>
<blockquote>
<p>经过concat和全连接层降维后，multi-headed
attention输出的向量维度与输入的向量维度一致</p>
</blockquote>
<h3 id="add-norm">Add &amp; Norm</h3>
<h4 id="add">Add</h4>
<p>Add是对得到的<span
class="math inline">\(X_{Attention}\)</span>以及X做一个相加。 <span
class="math display">\[
X_{Attention} = X_{Attention} + X
\]</span> <img src="skip_connect.jpg" alt="skip connect" /></p>
<p>Add的目的和ResNet的跳跃连接目的一样，使用<strong>残差</strong>，相当于每次更新时，导数项上加了一个恒等项1，即使原来的导数很小，这时误差仍然可以有效的反向传播，可以<strong>减少梯度消失和梯度爆炸的问题</strong>。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<p><strong>Normalization的目的是将数据送入激活函数之前进行归一化，避免输入数据落在激活函数的饱和区（两端）。</strong></p>
<p>但是Self-Attention为什么选择Layer Normalization而不是Batch
Normalization？</p>
<p><strong>Bach Normalization与Layer Normalization的区别：</strong></p>
<p>假设我们有10行3列的数据，即我们的batchsize =
10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。</p>
<p>那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种<strong>“列缩放”</strong>。</p>
<p>而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种<strong>“行缩放”</strong>。</p>
<p>在NLP领域中，如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的<strong>第一个</strong>词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是<strong>针对每个位置</strong>进行缩放，这<strong>不符合NLP的规律</strong>。而LN则是针对一句话进行缩放的，且<strong>LN一般用在第三维度</strong>，如[batchsize,
seq_len,
dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p>
<figure>
<img src="layer_normalization.png" alt="layer normalization" />
<figcaption aria-hidden="true">layer normalization</figcaption>
</figure>
<h3 id="feed-forword-layer">Feed Forword Layer</h3>
<p>这里就是将Multi-Head
Attention得到的提炼好的向量再投影到一个更大的空间（论文里将空间放大了4倍）在那个大空间里可以更方便地提取需要的信息（使用Relu激活函数），最后再投影回token向量原来的空间。
<span class="math display">\[
FFN(x)=ReLu(xW_1+b_1)W_2+b_2
\]</span></p>
<h1 id="模型训练">模型训练</h1>
<h2 id="训练任务">训练任务</h2>
<h3 id="masked-language-model">Masked language Model</h3>
<p>随机掩盖掉一些单词，然后通过上下文预测该单词。BERT中有15%的wordpiece
token会被随机掩盖，这15%的token中80%用<code>[MASK]</code>这个token来代替，10%用随机的一个词来替换，10%保持这个词不变。这种设计使得模型具有捕捉上下文关系的能力，同时能够有利于token-level
tasks例如序列标注。</p>
<figure>
<img src="mask_lm.png" alt="mask lm" />
<figcaption aria-hidden="true">mask lm</figcaption>
</figure>
<ol type="1">
<li><p>为什么选中的15%的wordpiece token不能全部
用<code>[MASK]</code>代替，而要用 10% 的 random token 和 10% 的原
token？</p>
<p><code>[MASK]</code>是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果
<code>[MASK]</code>以外的部分全部都用原
token，模型会学到『如果当前词是<code>[MASK]</code>，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在
finetune
阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。</p>
<p>以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token
的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在
finetune
阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。</p></li>
<li><p>最后怎么利用[MASK] token做的预测？</p>
<p>最终的损失函数只计算被mask掉的token的，每个句子里
<code>[MASK]</code>的个数是不定的。实际代码实现是每个句子有一个 maximum
number of predictions，取所有<code>[MASK]</code>的位置以及一些 PADDING
位置的向量拿出来做预测（总共凑成 maximum number of predictions
这么多个预测，是定长的），然后再用掩码把 PADDING
盖掉，只计算<code>[MASK]</code>部分的损失。</p></li>
</ol>
<h3 id="next-sentence-prediction">Next Sentence Prediction</h3>
<p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
<figure>
<img src="next_sentence_prediction.png"
alt="Next Sentence Prediction" />
<figcaption aria-hidden="true">Next Sentence Prediction</figcaption>
</figure>
<h1 id="fine-tuning">Fine Tuning</h1>
<h2 id="句子情感分类">句子情感分类</h2>
<p>如果是做单个句子的情感分类。输入中添加[CLS]，输出在最开始的地方添加一个线性分类器即可。为什么在最开始就可以，因为BERT是基于Transformer，而Transformer是基于self-attention，在每个位置都会得到整个句子的信息，所以不需要放到最后面。</p>
<figure>
<img src="bert_case1.png" alt="Bert case 1" />
<figcaption aria-hidden="true">Bert case 1</figcaption>
</figure>
<h2 id="机器翻译">机器翻译</h2>
<p>做机器翻译时，在每个输出后添加一个线性分类器，输出每个词语对应的翻译。</p>
<figure>
<img src="bert_case2.png" alt="Bert case2" />
<figcaption aria-hidden="true">Bert case2</figcaption>
</figure>
<h2 id="同义句判断">同义句判断</h2>
<p>将相似对拼接一起输入到模型中，最后一层外接一个线性分类器。</p>
<figure>
<img src="bert_case3.png" alt="Bert case3" />
<figcaption aria-hidden="true">Bert case3</figcaption>
</figure>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><p><a
href="https://arxiv.org/pdf/1810.04805.pdf">原始论文</a></p></li>
<li><p><a
href="https://blog.csdn.net/u011412768/article/details/108015783">BERT原理和结构详解</a></p></li>
<li><p><a
href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a></p></li>
<li><p><a
href="https://www.cnblogs.com/d0main/p/10447853.html">【译】为什么BERT有3个嵌入层，它们都是如何实现的</a></p></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/359366717">Transformer 中的
positional embedding</a></p></li>
<li><p><a
href="https://zhuanlan.zhihu.com/p/42833949">【模型解读】resnet中的残差连接，你确定真的看懂了？</a></p></li>
<li><p><a href="http://www.cxyzjd.com/article/herosunly/94720139">李宏毅
BERT 学习笔记_herosunly的博客</a></p></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>segment embedding</tag>
        <tag>position embedding</tag>
        <tag>self attention</tag>
        <tag>残差连接</tag>
        <tag>layer normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>共线性特征</title>
    <url>/2021/07/20/%E5%85%B1%E7%BA%BF%E6%80%A7%E7%89%B9%E5%BE%81/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>特征工程</tag>
        <tag>共线性</tag>
      </tags>
  </entry>
  <entry>
    <title>word embedding</title>
    <url>/2021/07/06/word-embedding/</url>
    <content><![CDATA[<p>word embedding
是文本表示的一类方法，将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。</p>
<span id="more"></span>
<p>这种方法有几个明显的优势：</p>
<ol type="1">
<li>他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。</li>
<li>语意相似的词在向量空间上也会比较相近。</li>
<li>通用性很强，可以用在不同的任务中。</li>
</ol>
<h1 id="神经概率语言模型">神经概率语言模型</h1>
<p>神经网络语言模型的提出解决了N-gram模型当𝑛较大时会发生数据稀疏的问题。与N-gram语言模型相同，神经网络语言模型（NNLM）也是对𝑛元语言模型进行建模，估计<span
class="math inline">\(P(x_i|x_{x-n+1},x_{x-n+2},...,x_{i-1})\)</span>的概率，与统计语言模型不同的是，神经网络语言模型不通过计数的方法对𝑛元条件概率进行估计，而是直接通过一个神经网络对其建模求解。</p>
<p>语言模型的的<strong>训练样本</strong>：<span
class="math inline">\((Context(w),w)\)</span></p>
<figure>
<img src="语言模型.png" alt="语言模型" />
<figcaption aria-hidden="true">语言模型</figcaption>
</figure>
<p>神经网络语言模型的结构入上图所示，可以分为输入层、投影层、隐藏层和输出层：</p>
<ol type="1">
<li><strong>输入层</strong>：词<span
class="math inline">\(x\)</span>的上下文，如果用N-gram的方法就是词<span
class="math inline">\(x\)</span>的前<span
class="math inline">\(n−1\)</span>个词了。每一个词都作为一个长度为<span
class="math inline">\(V\)</span>的one-hot向量传入神经网络中。</li>
<li><strong>投影层</strong>:投影层也叫embedding层，在投影层中，存在一个look-up表<span
class="math inline">\(C\)</span>，<span
class="math inline">\(C\)</span>被表示成一个<span
class="math inline">\(V∗m\)</span>的自由参数矩阵，其中<span
class="math inline">\(V\)</span>是词典的大小，而<span
class="math inline">\(m\)</span>代表每个词投影后的维度。表<span
class="math inline">\(C\)</span>中每一行都作为一个词向量存在，这个词向量可以理解为每一个词的另一种分布式表示。每一个one-hot向量都经过表<span
class="math inline">\(C\)</span>的转化变成一个词向量。<span
class="math inline">\(n−1\)</span>个词向量首尾相接的拼起来，转化为<span
class="math inline">\((n−1)m\)</span>的列向量输入到下一层。</li>
<li><strong>隐藏层</strong>：隐藏层的作用是进行非线性变换。<span
class="math inline">\(𝑧=𝑡𝑎𝑛ℎ(𝑊𝑋+𝑏)\)</span></li>
<li><strong>输出层</strong>：用softmax进行概率计算，计算词表<span
class="math inline">\(V\)</span>的每个词的概率。<span
class="math inline">\(𝑃(𝑥_𝑖)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑧)\)</span></li>
</ol>
<p>函数<span
class="math inline">\(F(w,Context(w),\theta)\)</span>中待确定的参数代表以下两个方面：</p>
<ul>
<li><p>词向量：<span class="math inline">\(v(w) \in R^m, w \in
C\)</span>以及填充向量</p></li>
<li><p>神经网络的参数：权重参数<span
class="math inline">\(W、U\)</span>和偏置参数<span
class="math inline">\(p、q\)</span></p>
<p><span class="math inline">\(W \in R^{n_h \times (n-1)m}\)</span></p>
<p><span class="math inline">\(p \in R^{n_h}\)</span></p>
<p><span class="math inline">\(U \in R^{V \times n_h}\)</span></p>
<p><span class="math inline">\(q \in R^V\)</span></p></li>
</ul>
<p>词向量在神经概率语言模型中扮演的角色：训练时，它用来帮助构造目标函数的辅助参数，训练完成后，它只是语言模型的一个副产品。</p>
<h1 id="word2vec">Word2Vec</h1>
<p>word2vec中到的两个重要模型：</p>
<ul>
<li><strong>CBOW模型（Continuous Bag-of-Words
Model）：</strong>已知上下文<span
class="math inline">\(w_{t-2},w_{t-1},w_{t+1},w{t+2}\)</span>的前提下，预测当前词<span
class="math inline">\(w_t\)</span></li>
<li><strong>Skip-gram模型（Continuous Skip-gram
Model)：</strong>已知当前词<span
class="math inline">\(w_t\)</span>的前提下，预测上下文<span
class="math inline">\(w_{t-2},w_{t-1},w_{t+1},w_{t+2}\)</span></li>
</ul>
<figure>
<img src="word2vec.png" alt="word2vec" />
<figcaption aria-hidden="true">word2vec</figcaption>
</figure>
<h2 id="基于hierarchical-softmax的模型">基于Hierarchical
Softmax的模型</h2>
<ul>
<li>Huffman树</li>
</ul>
<h2 id="基于negative-sampling的模型">基于Negative Sampling的模型</h2>
<p>随机负采样</p>
<h1 id="glove">Glove</h1>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://www.cnblogs.com/huangyc/p/9861453.html">语言模型</a></li>
<li><a
href="https://blog.csdn.net/heyc861221/article/details/80126134">漫谈词向量</a></li>
<li><a
href="https://blog.csdn.net/qq_27586341/article/details/90146342">word2vec原理（一）：
词向量、CBOW与Skip-Gram模型基础</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>word embedding</tag>
        <tag>word2vec</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络（CNN）</title>
    <url>/2021/07/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/</url>
    <content><![CDATA[<p><strong>卷积神经网络（Convolutional Neural Network,
CNN）</strong>是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p>
<span id="more"></span>
<p>卷积神经网络主要结构有：卷积层、池化层、和全连接层。通过堆叠这些层结构形成一个卷积神经网络。将原始图像转化为类别得分，其中卷积层和全连接层拥有参数，激活层和池化层没有参数。参数更新通过反向传播实现。</p>
<figure>
<img src="cnn.png" alt="cnn" />
<figcaption aria-hidden="true">cnn</figcaption>
</figure>
<p>卷积神经网络通常包含以下几种层：</p>
<ul>
<li><p><strong>卷积层（Convolutional
layer）</strong>，卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是<strong>提取输入的不同特征</strong>，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p></li>
<li><p><strong>线性整流层（Rectified Linear Units layer, ReLU
layer）</strong>，这一层神经的活性化函数（Activation
function）使用线性整流（Rectified Linear Units, ReLU）。</p></li>
<li><p><strong>池化层（Pooling
layer）</strong>，通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。</p></li>
<li><p><strong>全连接层（ Fully-Connected layer）</strong>,
把所有局部特征结合变成全局特征，用来计算最后每一类的得分。</p>
<p>其中A1为上一层的输出，D1为用随机数生成的一组dropout向量，然后将其与保留概率prob做比较得到一个布尔向量，再将其与A1做乘积即可得到失活后的A1，按理说dropout到这里应该也就完成了，但最后还有一个将A1除以保留概率的操作。所以这里有个疑问，为什么在dropout之后还要做个rescale的除法？</p></li>
</ul>
<p>​ 其实，这种实现dropout的方法也叫<strong>Inverted
Dropout</strong>，是一种经典的dropout实现方法。先不说Inverted
Dropout，我们来看正常dropout应该是怎样的：当我们使用了dropout后，在模型训练阶段只有占比为p部分的神经元参与了训练，那么在预测阶段得到的结果会比实际平均要大1/p，所以在测试阶段我们需要将输出结果乘以p来保持输出规模不变。这种原始的dropout实现方式也叫<strong>Vanilla
Dropout</strong>。Vanilla操作有一个重大缺陷，那就是预测过程需要根据训练阶段所使用的dropout策略做调整，比较麻烦，所以一般情况下都不会使用这种方法。</p>
<p>​ 既如此，相必大家也知道了，我们目前用的都是Inverted
Dropout方法，为了能够在神经网络训练完成后安安心心的做预测，我们可以把全部心思都放在训练阶段，所有的设置都在训练阶段完成。<strong>所以为了保证神经网络在丢弃掉一些神经元之后总体信号强度不变和预测结果稳定，也有一种说法叫保证Bernoulli二项分布的数学期望不变，我们在Inverted
Dropout方法中对dropout之后的做了除以p的rescale操作。</strong></p>
<p>​ 反向传播时同理，梯度计算时需要除以保留概率：</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li>https://blog.csdn.net/liangchunjiang/article/details/79030681</li>
<li>https://easyai.tech/ai-definition/cnn/</li>
<li>https://juejin.cn/post/6920928949576925191</li>
<li>https://cloud.tencent.com/developer/article/1745012</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2021/07/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>推荐系统本质上就是一个信息过滤系统，通常分为：召回、排序、重排序这3个环节，每个环节逐层过滤，最终从海量的物料库中筛选出几十个用户可能感兴趣的物品推荐给用户。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>数据倾斜</title>
    <url>/2021/07/19/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
    <content><![CDATA[<p>简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢。</p>
<span id="more"></span>
<h1 id="数据倾斜">数据倾斜</h1>
<p>数据倾斜一般有两种情况：</p>
<ul>
<li><strong>变量值很少：</strong>
单个变量值的占比极大，常见的字段如性别、学历、年龄等。</li>
<li><strong>变量值很多：</strong>
单个变量值的占比极小，常见的字段如收入、订单金额之类的。</li>
</ul>
<h1 id="常用优化方法">常用优化方法</h1>
<ol type="1">
<li><strong>增加jvm内存：</strong>这适用于变量值非常少的情况，这种情况下，往往只能通过硬件的手段来进行调优；</li>
<li><strong>增加reduce的个数：</strong>这适用于变量值非常多的情况，这种情况下最容易造成的结果就是大量相同key被partition到一个分区，从而一个reduce执行了大量的工作；</li>
<li><strong>重新设计key：</strong>在map阶段时给key加上一个随机数，有了随机数的key就不会被大量的分配到同一节点(小几率)，待到reduce后再把随机数去掉即可</li>
<li><strong>使用combiner合并：</strong>combinner是在map阶段，reduce之前的一个中间阶段，在这个阶段可以选择性的把大量的相同key数据先进行一个合并，可以看做是local
reduce，然后再交给reduce来处理，减轻了map端向reduce端发送的数据量(减轻了网络带宽)，也减轻了map端和reduce端中间的shuffle阶段的数据拉取数量(本地化磁盘IO速率)；（hive.map.aggr=true）</li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://cloud.tencent.com/developer/article/1519028">一文带你搞清楚什么是“数据倾斜”</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>map reduce</tag>
      </tags>
  </entry>
  <entry>
    <title>智能客服系统</title>
    <url>/2021/07/01/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>智能客服机器人慢慢成为了很多企业售后环节的标配产品，它能在一定程度上减轻客服人员的压力。</p>
<span id="more"></span>
<pre class="mermaid">graph TB
O[用户问题] --> A[文本预处理]
A --> B{中控管理}
B --> C[QA机器人]
B --> D[KBQA问答]
B --> E[闲聊机器人]
B --> F[任务机器人]

C --> C1(精准匹配)
C1 --> C2{意图匹配}
C2 --关键词匹配--> C31(会话开始/会话结束)
C2 --关键词匹配--> C32(政治词/黄暴词)
C2 --关键词匹配--> C33(人工客服)
C2 --> C24(QA问题)
C31--> C41(特定回复)
C32 --> C42(特定回复)
C33 --> C43(接入人工坐席)
C24 -->  C44(规则匹配)
C44 --> C51(Elastic Search粗排)
C51 --> C62("Embedding + VSM计算相似性")
C51 --> C63(XGBoost)
C51 --> C64("表达式(Bert)")
C51 --> C65("交互式式(Bert/ABCNN)")

D ---> D1(匹配商品属性)
D1 ---> D2(获取商品属性值)
D2 ---> D3(生成回复)

E ----> E1(Elastic Search粗排)
E1 ----> E2(Embedding+VSM计算相似性)

F --> F1(natural language understanding自然语言理解)
F1 --意图识别/槽位提取---> F2(dialogue state tracking对话状态跟踪器)
subgraph Dialogue Management
F2 --维护/更新对话状态--> F3(dialgoue policy learning对话策略)
end
F3 --决定对话策略 ---> F4(natural language generation自然语言生成)</pre>
<h1 id="文本预处理">文本预处理</h1>
<ul>
<li>原句</li>
<li>去除空格、标点符号等</li>
<li>简繁转化</li>
<li>同义词改写</li>
</ul>
<h1 id="qa机器人">QA机器人</h1>
<h2 id="elasticsearch粗排">ElasticSearch粗排</h2>
<p>ES库中保存有每个用户自己的知识库，因精排比较用户query和知识库中相似问比较耗时，为提高检索性能，因此添加es检索模块，先将与query较相似的相似问检索出来，将精排匹配次数限制在一定数量级。</p>
<p>详细见文章：<a href="/2021/07/02/Elastic-Search/" title="Elastic Search">Elastic Search</a></p>
<h1 id="闲聊机器人">闲聊机器人</h1>
<h1 id="任务机器人">任务机器人</h1>
]]></content>
      <categories>
        <category>business</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nlp</tag>
        <tag>dialogue</tag>
        <tag>taskbot</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习评价指标</title>
    <url>/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<p>本文将详细介绍机器学习分类任务的常用评价指标：准确率（Accuracy）、精确率（Precision）、召回率（Recall）、P-R曲线（Precision-Recall
Curve）、F1 Score、混淆矩阵（Confuse Matrix）、ROC、AUC。</p>
<span id="more"></span>
<h1 id="混淆矩阵">混淆矩阵</h1>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 43%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>预测结果</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>真实类别</strong></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr class="even">
<td>正例</td>
<td>真阳性（True Positive） TP</td>
<td>假阴性（Ffalse Negative）FN</td>
</tr>
<tr class="odd">
<td>负例</td>
<td>假阳性（False Positive） FP</td>
<td>真阴性（True Negative）TN</td>
</tr>
</tbody>
</table>
<h1 id="准确率accuracy">准确率（Accuracy）</h1>
<p>准确率是分类问题中最为原始的评价指标，准确率的定义是<strong>预测正确的结果占总样本的百分比</strong>，其公式如下：
<span class="math display">\[
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\]</span></p>
<h1 id="精确率precision">精确率（Precision）</h1>
<p><strong>精准率</strong>（Precision）又叫<strong>查准率</strong>，它是<strong>针对预测结果</strong>而言的，它的含义是<strong>在所有被预测为正的样本中实际为正的样本的概率</strong>：
<span class="math display">\[
Precision = \frac{TP}{TP+FP}
\]</span></p>
<h1 id="召回率recall">召回率（Recall）</h1>
<p><strong>召回率</strong>（Recall）又叫<strong>查全率</strong>，它是<strong>针对原样本</strong>而言的，它的含义是<strong>在实际为正的样本中被预测为正样本的概率</strong>：
<span class="math display">\[
Recall = \frac{TP}{TP+FN}
\]</span></p>
<h1 id="f1-score">F1-Score</h1>
<p>在不同的应用场景下，我们的关注点不同，例如，在预测股票的时候，我们更关心精准率，即我们预测升的那些股票里，真的升了有多少，因为那些我们预测升的股票都是我们投钱的。而在预测病患的场景下，我们更关注召回率，即真的患病的那些人里我们预测错了情况应该越少越好。</p>
<p>精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，召回率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样准确率就很低了。</p>
<p>在实际工程中，我们往往需要结合两个指标的结果，去寻找一个平衡点，使综合性能最大化。</p>
<p>在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的<strong>加权调和平均</strong>，即：
<span class="math display">\[
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}}
\cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)
\]</span></p>
<p><span class="math display">\[
F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times
R}{\left(\beta^{2} \times P\right)+R}
\]</span></p>
<p>特别地，当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。
<span class="math display">\[
F1=\frac{2 \times P \times R}{P+R} = \frac{2 \times TP}{样例总数+TP-TN}
\]</span></p>
<h1 id="rocauc曲线">ROC/AUC曲线</h1>
<h2 id="灵敏度特异度真阳率假阳率">灵敏度、特异度、真阳率、假阳率</h2>
<p><span class="math display">\[
Sensitivity = \frac{TP}{TP+FN}
\]</span></p>
<p><span class="math display">\[
Specificity=\frac{TN}{FP+TN}
\]</span></p>
<ul>
<li>其实我们可以发现灵敏度和召回率是一模一样的，只是名字换了而已。</li>
<li>由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1-
特异度），而不是特异度。</li>
</ul>
<p><span class="math display">\[
真阳率（TPR）=Sensitivity = \frac{TP}{TP+FN}
\]</span></p>
<p><span class="math display">\[
假阳率(FPR) = 1 - Specificity = \frac{FP}{FP+TN}
\]</span></p>
<p><strong>TPR 和 FPR 分别是基于实际表现 1 和 0
出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。</strong></p>
<p>正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90%
是正样本，10% 是负样本。我们知道用准确率是有水分的，但是用 TPR 和 FPR
不一样。这里，TPR 只关注 90% 正样本中有多少是被真正覆盖的，而与那 10%
毫无关系，同理，FPR 只关注 10% 负样本中有多少是被错误覆盖的，也与那 90%
毫无关系，所以可以看出：<strong>如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用
TPR 和 FPR 作为 ROC/AUC 的指标的原因。</strong></p>
<h2 id="roc曲线">ROC曲线</h2>
<p><strong>ROC（Receiver Operating
Characteristic）曲线</strong>，又称接受者操作特征曲线。ROC曲线中的主要两个指标就是<strong>真正率TPR</strong>和<strong>假正率FPR</strong>，上面已经解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。</p>
<figure>
<img src="roc.png" alt="ROC" />
<figcaption aria-hidden="true">ROC</figcaption>
</figure>
<h3 id="阈值问题">阈值问题</h3>
<p>ROC曲线是通过<strong>遍历所有阈值</strong>来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。</p>
<figure>
<img src="roc_gate.gif" alt="roc gate" />
<figcaption aria-hidden="true">roc gate</figcaption>
</figure>
<p>我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。</p>
<h3 id="判断模型性能">判断模型性能</h3>
<p>那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是<strong>TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。</strong>参考如下动态图进行理解。</p>
<figure>
<img src="roc_tell.gif" alt="roc tell" />
<figcaption aria-hidden="true">roc tell</figcaption>
</figure>
<p>即：<strong>进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。</strong></p>
<h2 id="auc曲线">AUC曲线</h2>
<p>AUC(Area Under Curve)又称为曲线下面积，是处于ROC
Curve下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC
=
1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC
=
0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p>1.<a
href="https://www.cnblogs.com/guoyaohua/p/classification-metrics.html">【机器学习】一文读懂分类算法常用评价指标</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习中的梯度消失、梯度爆炸问题</title>
    <url>/2021/07/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://zhuanlan.zhihu.com/p/33006526">详解深度学习中的梯度消失、爆炸原因及其解决方法</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络 Neural Networks</title>
    <url>/2021/07/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Neural-Networks/</url>
    <content><![CDATA[<p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。</p>
<span id="more"></span>
<h1 id="神经元">神经元</h1>
<figure>
<img src="SingleNeuron.png" alt="神经元" />
<figcaption aria-hidden="true">神经元</figcaption>
</figure>
<p>这个“神经元”是一个以 <span class="math inline">\(x_1,x_2,x_3\)</span>
及截距 $ +1 $ 为输入值的运算单元，其输出为 $ h_{W,b}(x) = f(W^Tx) =
f(<em>{i=1}^3 W</em>{i}x_i +b)$ ，其中函数 $ f : $
被称为“激活函数”。在本教程中，我们选用sigmoid函数作为激活函数 $ f() $:
<span class="math display">\[
f(z) = \frac{1}{1+\exp(-z)}.
\]</span></p>
<h1 id="神经网络模型">神经网络模型</h1>
<p>所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络：</p>
<figure>
<img src="400px-Network331.png" alt="神经网络模型" />
<figcaption aria-hidden="true">神经网络模型</figcaption>
</figure>
<p>我们使用 $ w^l_{jk} $ 表示从 <span
class="math inline">\((l−1)^{th}\)</span> 层的 $k^{th} $个神经元到
$l^{th} $ 层的 $j^{th} $ 个神经元的链接上的权重。使用 <span
class="math inline">\(b^l_j\)</span> 表示在 <span
class="math inline">\(l^{th}\)</span> 层第 $j^{th} $
个神经元的偏置，中间量 $ z^l w^l a<sup>{l-1}+b</sup>l$ ，使用 <span
class="math inline">\(a^l_j\)</span> 表示 <span
class="math inline">\(l^{th}\)</span> 层第 <span
class="math inline">\(j^{th}\)</span> 个神经元的激活值。</p>
<p><span class="math inline">\(l^{th}\)</span> 层的第 <span
class="math inline">\(j^{th}\)</span> 个神经元的激活值 <span
class="math inline">\(a^l_j\)</span> 就和 <span
class="math inline">\(l-1^{th}\)</span>
层的激活值通过方程关联起来了。</p>
<p><span class="math display">\[
\begin{eqnarray}
a^{l}\_j = \sigma\left( \sum\_k w^{l}\_{jk} a^{l-1}\_k + b^l\_j \right)
\label{eq:fp}\tag{fp}
\end{eqnarray}
\]</span></p>
<p>对方程<span class="math inline">\(\eqref{eq:fp}\)</span>
就可以写成下面这种美妙而简洁的向量形式了</p>
<p><span class="math display">\[
\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l)
  \label{eq:mfp}\tag{mfp}
\end{eqnarray}
\]</span></p>
<h1 id="反向传播">反向传播</h1>
<p>反向传播的目标是计算代价函数 <span class="math inline">\(C\)</span>
分别关于 <span class="math inline">\(w\)</span> 和 <span
class="math inline">\(b\)</span> 的偏导数 <span
class="math inline">\(\frac{∂C}{∂w}\)</span> 和 <span
class="math inline">\(\frac{∂C}{∂b}\)</span>
。反向传播其实是对权重和偏置变化影响代价函数过程的理解。最终极的含义其实就是计算偏导数
<span class="math inline">\(\frac{\partial C}{\partial
w_{jk}^l}\)</span> 和<span class="math inline">\(\frac{\partial
C}{\partial
b_j^l}\)</span>。但是为了计算这些值，我们首先引入一个中间量， <span
class="math inline">\(\delta_j^l\)</span> ，这个我们称为在 <span
class="math inline">\(l^{th}\)</span> 层第 <span
class="math inline">\(j^{th}\)</span>
个神经元上的<strong>误差</strong>。</p>
<p>对于<span class="math inline">\(l\)</span>层的第 <span
class="math inline">\(j^{th}\)</span>
个神经元，当输入进来时，对神经元的带权输入增加很小的变化 <span
class="math inline">\(\Delta z_j^l\)</span> ，使得神经元输出由 $
(z_j^l)$ 变成 <span class="math inline">\(\sigma(z_j^l + \Delta
z_j^l)\)</span> 。这个变化会向网络后面的层进行传播，最终导致整个代价产生
<span class="math inline">\(\frac{\partial C}{\partial z_j^l} \Delta
z_j^l\)</span> 的改变。所以这里有一种启发式的认识， <span
class="math inline">\(\frac{\partial C}{\partial z_j^l}\)</span>
是神经元的误差的度量。</p>
<p>按照上面的描述，我们定义 <span class="math inline">\(l\)</span>
层的第 <span class="math inline">\(j^{th}\)</span> 个神经元上的误差
<span class="math inline">\(\delta_j^l\)</span> 为： <span
class="math display">\[
\begin{eqnarray}
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}
  \label{eq:error}\tag{error}
\end{eqnarray}
\]</span></p>
<h2 id="输出层误差的方程">输出层误差的方程</h2>
<p><strong>输出层误差的方程</strong>， <span
class="math inline">\(\delta^L\)</span> ： 每个元素定义如下： <span
class="math display">\[
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma&#39;(z^L_j)
  \label{eq:bp1}\tag{BP1}
\end{eqnarray}
\]</span></p>
<p>第一个项 <span class="math inline">\(\frac{\partial C}{\partial
a_j^L}\)</span> 表示代价随着 <span class="math inline">\(j^{th}\)</span>
输出激活值的变化而变化的速度。第二项 <span
class="math inline">\(\sigma&#39;(z^L_j)\)</span> 刻画了在 <span
class="math inline">\(z_j^L\)</span> 处激活函数 <span
class="math inline">\(\sigma\)</span> 变化的速度。</p>
<h2
id="使用下一层的误差-deltal1-来表示当前层的误差-deltal">使用下一层的误差
<span class="math inline">\(\delta^{l+1}\)</span> 来表示当前层的误差
<span class="math inline">\(\delta^{l}\)</span></h2>
<p><strong>使用下一层的误差 <span
class="math inline">\(\delta^{l+1}\)</span> 来表示当前层的误差 <span
class="math inline">\(\delta^{l}\)</span>：</strong>特别地， <span
class="math display">\[
\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma&#39;(z^l)
  \label{eq:bp2}\tag{BP2}
\end{eqnarray}
\]</span></p>
<p>其中<span class="math inline">\((w^{l+1})^T\)</span>是<span
class="math inline">\((l+1)^{\rm th}\)</span>层权重矩阵<span
class="math inline">\(w^{l+1}\)</span>的转置。假设我们知道<span
class="math inline">\(l+1^{\rm th}\)</span>层的误差<span
class="math inline">\(\delta^{l+1}\)</span>。当我们应用转置的权重矩阵<span
class="math inline">\((w^{l+1})^T\)</span>，我们可以凭直觉地把它看作是在沿着网络<strong>反向</strong>移动误差，给了我们度量在<span
class="math inline">\(l^{\rm th}\)</span>
层输出的误差方法。然后，我们进行 Hadamard 乘积运算 <span
class="math inline">\(\odot \sigma&#39;(z^l)\)</span> 。这会让误差通过
<span class="math inline">\(l\)</span>
层的激活函数反向传递回来并给出在第 <span
class="math inline">\(l\)</span> 层的带权输入的误差 <span
class="math inline">\(\delta\)</span> 。</p>
<p><strong>证明：</strong> 我们想要以<span
class="math inline">\(\delta^{l+1}_k = \partial C / \partial
z^{l+1}_k\)</span>的形式重写<span class="math inline">\(\delta^l_j =
\partial C / \partial z^l_j\)</span>。应用链式法则 <span
class="math display">\[
\begin{eqnarray}
\delta^l\_j &amp;=&amp; \frac{\partial C}{\partial z^l\_j}\\\\
           &amp;=&amp; \sum\_k \frac{\partial C}{\partial z^{l+1}\_k}
\frac{\partial z^{l+1}\_k}{\partial z^l\_j}\\\\
           &amp;=&amp; \sum\_k \frac{\partial z^{l+1}\_k}{\partial
z^l\_j} \delta^{l+1}\_k
\end{eqnarray}
\]</span></p>
<p>为了对最后一行的第一项求值，注意：</p>
<p><span class="math display">\[
\begin{eqnarray}
  z^{l+1}\_k = \sum\_j w^{l+1}\_{kj} a^l\_j +b^{l+1}\_k = \sum\_j
w^{l+1}\_{kj} \sigma(z^l\_j) +b^{l+1}\_k
\end{eqnarray}
\]</span></p>
<p>做微分，我们得到</p>
<p><span class="math display">\[
\begin{eqnarray}
  \frac{\partial z^{l+1}\_k}{\partial z^l\_j} = w^{l+1}\_{kj}
\sigma&#39;(z^l\_j)
\end{eqnarray}
\]</span></p>
<p>代入上式即有：</p>
<p><span class="math display">\[
\begin{eqnarray}
  \delta^l\_j = \sum\_k w^{l+1}\_{kj}  \delta^{l+1}\_k
\sigma&#39;(z^l\_j)
\end{eqnarray}
\]</span></p>
<h2
id="代价函数关于网络中任意偏置的改变率">代价函数关于网络中任意偏置的改变率</h2>
<p><strong>代价函数关于网络中任意偏置的改变率：</strong> 就是 <span
class="math display">\[
\begin{eqnarray}
  \frac{\partial C}{\partial b^l_j} = \delta^l_j
  \label{eq:bp3}\tag{BP3}
\end{eqnarray}
\]</span></p>
<p>这其实是，误差<span class="math inline">\(\delta^l_j\)</span>
和偏导数值 <span class="math inline">\(\partial C / \partial
b^l_j\)</span><strong>完全一致</strong>。</p>
<h2
id="代价函数关于任何一个权重的改变率">代价函数关于任何一个权重的改变率</h2>
<p><strong>代价函数关于任何一个权重的改变率：</strong> 特别地，</p>
<p><span class="math display">\[
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j
  \label{eq:bp4}\tag{BP4}
\end{eqnarray}
\]</span></p>
<h2 id="反向传播算法描述">反向传播算法描述</h2>
<ul>
<li><strong>输入<span class="math inline">\(x\)</span>：</strong>
为输入层设置对应的激活值<span class="math inline">\(a^1\)</span></li>
<li><strong>前向传播：</strong> 对每个<span
class="math inline">\(l=2,3,...,L\)</span>计算相应的<span
class="math inline">\(z^l = w^la^{l-1} + b^l\)</span> 和 <span
class="math inline">\(a^l = \sigma(z^l)\)</span></li>
<li><strong>输出层误差 <span class="math inline">\(\delta^L\)</span>
：</strong> 计算向量 <span class="math inline">\(\delta^L = \nabla_a C
\odot \sigma&#39;(z^L)\)</span></li>
<li><strong>反向误差传播：</strong> 对每个<span
class="math inline">\(l=L-1, L-2,...,2\)</span> ，计算<span
class="math inline">\(\delta^l = ((w^{l+1})^T\delta^{l+1})\odot
\sigma&#39;(z^l)\)</span></li>
<li><strong>输出：</strong> 代价函数的梯度由 <span
class="math inline">\(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k
\delta^l_j\)</span> 和 <span class="math inline">\(\frac{\partial
C}{\partial b_j^l} = \delta_j^l\)</span> 得出</li>
</ul>
<p>证明见<a
href="https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/chap2-5.html">四个基本方程的证明</a>。</p>
<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sizes</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won&#x27;t set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers.&quot;&quot;&quot;</span></span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, a</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="params"><span class="function">            test_data=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/<span class="built_in">len</span>(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/<span class="built_in">len</span>(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[-<span class="number">1</span>])</span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It&#x27;s a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, test_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network&#x27;s output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation.&quot;&quot;&quot;</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<p><a
href="https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/cover.html">神经网络与深度学习</a></p>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nn</tag>
      </tags>
  </entry>
  <entry>
    <title>ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction </title>
    <url>/2021/11/25/ASAP-A-Chinese-Review-Dataset-Towards-Aspect-Category-Sentiment-Analysis-and-Rating-Prediction/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention-based LSTM for Aspect-level Sentiment Classification</title>
    <url>/2021/11/25/Attention-based-LSTM-for-Aspect-level-Sentiment-Classification/</url>
    <content><![CDATA[<p>利用Aspect信息和Attention机制，解决Aspect-level的细粒度情感分析问题。</p>
<span id="more"></span>
<h1 id="主要贡献">主要贡献</h1>
<ol type="1">
<li>提出了基于注意力机制的LSTM模型。当关注不同的aspect时，模型可以关注句子中的不同部分。</li>
<li>设计了两种方式将aspect信息输入在模型中：
<ol type="1">
<li>对词进行向量表示时，将aspect embeddings附加到输入向量后面。</li>
<li>计算注意力权重时，将aspect
embeddings和句子的隐藏层输出做concat，一起计算注意力权重。</li>
</ol></li>
</ol>
<h1 id="模型设计">模型设计</h1>
<h2 id="模型结构图">模型结构图</h2>
<figure>
<img src="atae-lstm.png" alt="atae-lstm" />
<figcaption aria-hidden="true">atae-lstm</figcaption>
</figure>
<h2 id="输入向量">输入向量</h2>
<ol type="1">
<li><p>input embeddings：加载glove.6B.300.txt向量</p></li>
<li><p>aspect embeddings：aspect中字向量的加权平均</p></li>
<li><p>将input embeddings和aspect embeddings进行拼接，作为LSTM的输入
<span class="math display">\[
\begin{bmatrix}
x_t \\
x_{aspect}
\end{bmatrix}
\]</span></p></li>
</ol>
<h2 id="attention层">Attention层</h2>
<ol type="1">
<li><p>将LSTM输出向量与aspect向量进行拼接，并计算softmax，得到LSTM输出层的Attention系数
<span class="math display">\[
M = tanh
\begin{bmatrix}
W_hH \\
W_vv_a \otimes e_N
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\alpha=softmax(w^TM)
\]</span></p></li>
<li><p>将隐层向量与各相关系数进行点乘，得到attention向量 <span
class="math display">\[
r = H \alpha^{T}
\]</span></p></li>
</ol>
<h2 id="任务层">任务层</h2>
<ol type="1">
<li><p>将attention向量和隐层输出向量相加 <span class="math display">\[
h^*=tanh(W_pr+W_xh_N)
\]</span></p></li>
<li><p>最后接一个Dense层和一个softmax进行任务判别 <span
class="math display">\[
y = softmax(W_xh^*+b_s)
\]</span></p></li>
</ol>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a href="https://aclanthology.org/D16-1058.pdf">Attention-based LSTM
for Aspect-level Sentiment Classification</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>Dropout</title>
    <url>/2021/08/23/Dropout/</url>
    <content><![CDATA[<p>Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。</p>
<span id="more"></span>
<h1 id="dropout简介"><strong>Dropout简介</strong></h1>
<h2 id="dropout出现的原因"><strong>Dropout出现的原因</strong></h2>
<p>训练深度神经网络的时候，总是会遇到两大缺点:</p>
<ol type="1">
<li>容易过拟合</li>
<li>费时</li>
</ol>
<p>过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p>
<h2 id="什么是dropout">什么是Dropout</h2>
<p>Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p>
<p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图所示。</p>
<figure>
<img src="dropout.jpg" alt="dropout" />
<figcaption aria-hidden="true">dropout</figcaption>
</figure>
<h1 id="dropout训练和预测时的不同">Dropout训练和预测时的不同</h1>
<p><strong>在训练阶段：</strong></p>
<ol type="1">
<li>首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变</li>
<li>然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）</li>
<li>在p是神经元抛弃概率时，训练后神经元参数需要进行<span
class="math inline">\(\frac{1}{1-p}\)</span>缩放</li>
</ol>
<blockquote>
<p>当模型使用dropout layer时，训练的时候只有占比为<span
class="math inline">\(p\)</span>的隐层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大<span
class="math inline">\(\frac{1}{p}\)</span>，为了避免这种情况，就需要测试的时候将输出结果乘以
<span class="math inline">\(p\)</span>使下一层的输入规模保持不变。</p>
<p>而利用inverted
dropout，我们可以在训练的时候直接将dropout后留下的权重扩大<span
class="math inline">\(\frac{1}{p}\)</span>倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。</p>
</blockquote>
<p><strong>在测试阶段：</strong></p>
<p>直接利用所有训练好的神经元权重计算输出，不需要dropout</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>PyCharm开发问题</title>
    <url>/2021/11/25/PyCharm%E5%BC%80%E5%8F%91%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>记录Pycharm中常见开发问题，以便后续查用。</p>
<span id="more"></span>
<h1
id="解决pycharm不提示tensorflow的问题">解决Pycharm不提示TensorFlow的问题</h1>
<p>TensorFlow
2.0中，Keras为懒加载，因此在Pycharm中不能加载代码提示。</p>
<p>在<code>tensorflow/__init__.py</code>文件最后添加如下导入语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras <span class="keyword">as</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.util.lazy_loader <span class="keyword">import</span> LazyLoader</span><br><span class="line"><span class="comment"># pylint: disable=g-import-not-at-top</span></span><br><span class="line">keras = LazyLoader(<span class="string">&#x27;keras&#x27;</span>, <span class="built_in">globals</span>(), <span class="string">&#x27;tensorflow.keras&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> LazyLoader</span><br></pre></td></tr></table></figure>
<h1 id="pycharm-不能debug-tensorflow-keras的call函数">pycharm 不能debug
tensorflow keras的call函数</h1>
<p>增加@tf.autograph.experimental.do_not_convert</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.autograph.experimental.do_not_convert</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>saveModel时需要去除，否则会报<code>Missing support to serialize a method function without a named 'self' argument.</code>错误</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>pycharm</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</title>
    <url>/2021/11/11/SKEP-Sentiment-Knowledge-Enhanced-Pre-training-for-Sentiment-Analysis/</url>
    <content><![CDATA[<p>SKEP将不同类型的情感知识整合在一起，为各种情感分析任务提供统一的情感表示。</p>
<span id="more"></span>
<h1 id="模型解析">模型解析</h1>
<figure>
<img src="skep.png" alt="skep" />
<figcaption aria-hidden="true">skep</figcaption>
</figure>
<h2 id="unsupervised-sentiment-knowledge-mining">Unsupervised Sentiment
Knowledge Mining</h2>
<h3
id="情感词挖掘pmipointwise-mutual-information">情感词挖掘：PMI(Pointwise
Mutual Information)</h3>
<p>PMI方法仅依赖于少量的情感种子词，给出了每个种子词的词极性<span
class="math inline">\(WP(s)\)</span>。首先建立一个候选词对的集合，每个词对包含一个种子词，并满足预定义的词性模式。然后，一个词对的共现用PMI计算如下：
<span class="math display">\[
PMI(w_1,w_2) = log \frac{p(w_1,w_2)}{p(w_1)p(w_2)}
\]</span> 其中，<span
class="math inline">\(p(.)\)</span>表示通过计数估计的概率。</p>
<p>一个词的极性是由其PMI得分和所有正种子及负种子之间的差异决定的： <span
class="math display">\[
WP(w) = \sum_{WP(s)=+}{PMI(w,s)} - \sum_{WP(s)=-}{PMI(w,s)}
\]</span> 如果<span class="math inline">\(WP(w)&gt;0\)</span>，判定<span
class="math inline">\(w\)</span>为正向情感词，反之为负向情感词。</p>
<h3 id="方面情感对">方面情感对</h3>
<p>一个情感词与其最近的名词将被视为一个方面-情感对，最大距离被经验地限制为不超过3个token。</p>
<h2 id="sentiment-masking">Sentiment Masking</h2>
<p>Sentiment
Masking主要是构造掩盖情感信息的语料。此过程包含情感检测和混合情感掩盖两部分：</p>
<h3 id="sentiment-detection-with-knowledge">Sentiment Detection with
Knowledge</h3>
<ol type="1">
<li>如果输入序列中的一个词也出现在知识库G中，那么这个词就被视为情感词。</li>
<li>情感词与其最近的名词（距离3以内）作为aspect-sentiment
pair候选集，如果这样的
candidate也被发现在挖掘的知识G，那么它被认为是一个方面感情对。</li>
</ol>
<h3 id="hybrid-sentiment-masking">Hybrid Sentiment Masking</h3>
<p>由三种类型token生成：aspect-sentiment pairs， sentiment words，
common tokens.</p>
<ol type="1">
<li><p>Aspect-sentiment Pair Masking：随机选择最多2个aspect-sentiment
pair进行mask，这种masking方法提供了捕获aspect word和sentiment
word的组合的方法</p></li>
<li><p>Sentiment Word Masking：对于未屏蔽的sentiment
word，随机选取不超过10%的token进行mask。</p></li>
<li><p>Common Token
Masking：如果上两个步骤中的mask数量不足（低于10%），随机选取一定数量的common
tokens进行掩盖。</p></li>
</ol>
<h2 id="sentiment-pre-training-objectives">Sentiment Pre-training
Objectives</h2>
<p><span class="math inline">\(\widetilde{X}\)</span>为sentiment
masking之后产生的序列，模型需要恢复三个被替换的目标：</p>
<p><span class="math display">\[
L = L_{sw} + L_{wp} + L_{ap}
\]</span></p>
<h3 id="sentiment-wordsw-prediction-l_sw"><strong>Sentiment Word（SW）
prediction：</strong> <span class="math inline">\(L_{sw}\)</span></h3>
<p>利用 transformer encoder的output vector <span
class="math inline">\(\widetilde{x_i}\)</span>
来恢复被屏蔽的情感词，<span
class="math inline">\(\widetilde{x_i}\)</span>被输入到一个softmax层，该层在整个vocab上产生一个归一化的概率向量<span
class="math inline">\(\widetilde{y_i}\)</span>，因此<span
class="math inline">\(L_{sw}\)</span>就是最大化下面的损失： <span
class="math display">\[
\widehat{y}_i=softmax(\widetilde{x}_iW+b)
\]</span></p>
<p><span class="math display">\[
L_{sw}=-\sum^{i=n}_{i=1}{m_i\times y_i log{\widehat{y}_i}}
\]</span></p>
<p><span class="math inline">\(W\)</span>和<span
class="math inline">\(b\)</span>是输出层参数，当<span
class="math inline">\(i\)</span>-th位置的词语被屏蔽时，<span
class="math inline">\(m_i\)</span>=1，否则为0，<span
class="math inline">\(\widetilde{y_i}\)</span>为原始词<span
class="math inline">\(x_i\)</span>的独热编码表示。</p>
<h3 id="word-polaritywp-predictionl_wp"><strong>Word Polarity（WP）
prediction：</strong><span class="math inline">\(L_{wp}\)</span></h3>
<p><span class="math inline">\(L_{wp}\)</span>与<span
class="math inline">\(L_{sw}\)</span>类似，计算每个被mask掉的sentiment
token</p>
<h3 id="aspect-sentiment-pairapprediction-l_ap"><strong>Aspect-sentiment
Pair（AP）prediction：</strong> <span
class="math inline">\(L_{ap}\)</span></h3>
<p>论文中利用多标签分类对aspect-sentiment
pair进行预测，使用最终的[CLS]来表示整个序列，利用sigmoid激活函数同时输出多个token的预测结果：
<span class="math display">\[
\widehat{y}_a = sigmoid(\widetilde{x}_1W\_{ap} + b\_{ap})
\]</span></p>
<p><span class="math display">\[
L_{ap}=-\sum^{a=A}_{a=1}y_alog{\widehat{y}_a}
\]</span></p>
<p>其中，<span
class="math inline">\(x_1\)</span>表示[CLS]的输出向量，<span
class="math inline">\(A\)</span>为被掩盖的aspect-sentiment
pairs数量，<span
class="math inline">\(\widehat{y}_a\)</span>为预测的词语概率分布，<span
class="math inline">\(y_a\)</span>为目标aspect-sentiment
pair的独热编码表示（<span
class="math inline">\(y_a\)</span>中多个位置为1）</p>
<h2 id="fine-tuning-for-sentiment-analysis">Fine-tuning for Sentiment
Analysis</h2>
<p>我们验证了三个典型的情绪分析任务：句子级的情绪分类，方面级情绪分类和观点角色标记。</p>
<h3 id="sentence-level-sentiment-classification">Sentence-level
Sentiment Classification</h3>
<p>这个任务是分类句子的情感极性。用【CLS】表示输入语句的整体表示形式。外接一个分类器来对整句进行情感极性分类。</p>
<h3 id="aspect-level-sentiment-classification">Aspect-level Sentiment
Classification</h3>
<p>这个任务的目的是当给定一个上下文文本时，分析方面的细粒度情感。因此输入包含两个部分：aspect和上下文。</p>
<p>讲这两部分用[SEP]进行拼接，输入到transformer
encoder中，利用[CLS]向量判断是否属于aspect-sentiment pair。</p>
<h3 id="opinion-role-labeling">Opinion Role Labeling</h3>
<p>这个任务是从输入文本中检测细粒度的观点。这个任务被转换成序列标记，它使用BIOS方案进行标记，并添加一个CRF层来预测标签。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a href="https://arxiv.org/abs/2005.05635">SKPEP: Sentiment
Knowledge Enhanced Pre-training for Sentiment Analysis</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>SemEval-2022 Task 10: Structured Sentiment Analysis</title>
    <url>/2021/11/22/SemEval-2022-Task-10-Structured-Sentiment-Analysis/</url>
    <content><![CDATA[<p>2022年情感识别任务Task10：<a
href="https://competitions.codalab.org/competitions/33556">SemEval-2022
Task 10: Structured Sentiment Analysis</a></p>
<span id="more"></span>
<h1 id="任务描述">任务描述</h1>
<p>Structured Sentiment
Analysis主要是从文本中抽取结构化的情感图(Structured sentiment graph)。
即，从文本中抽取所有的观点元组<span
class="math inline">\(O=O_1,...,O_n\)</span>，每个观点<span
class="math inline">\(O_i\)</span>为一个四元组<span
class="math inline">\((h,t,e,p)\)</span></p>
<p>其中<span
class="math inline">\(h\)</span>为<strong>holder</strong>，<span
class="math inline">\(p\)</span>为<strong>polarity</strong>，<span
class="math inline">\(t\)</span>为<strong>target</strong>，<span
class="math inline">\(e\)</span>为<strong>sentiment
expression</strong>， holder通过sentiment
expression对target表达polarity。</p>
<figure>
<img src="multi_sent_graph.png" alt="multi_sent_graph" />
<figcaption aria-hidden="true">multi_sent_graph</figcaption>
</figure>
<h2 id="单语言预测">单语言预测</h2>
<p>在相同的语言上训练和预测结构化情感图，需上传7个预测文本</p>
<h2 id="跨语言预测">跨语言预测</h2>
<p>探索模型在跨语言上的繁华能力。在多语言上进行训练，在
<code>MultiBooked、Datasets (Catalan and Basque)</code> 和
<code>OpeNER Spanish</code> 数据集上进行测试。</p>
<h1 id="数据集">数据集</h1>
<table>
<thead>
<tr class="header">
<th>数据集</th>
<th>描述</th>
<th>训练集句子数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(NoReC_{Fine}\)</span></td>
<td>挪威语: 多领域的专业评论数据</td>
<td>8634</td>
</tr>
<tr class="even">
<td><span class="math inline">\(MultiB_{EU}\)</span></td>
<td>巴斯克語: 酒店评论数据</td>
<td>1064</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(MultiB_{CA}\)</span></td>
<td>加泰罗尼亚语:酒店评论数据</td>
<td>1174</td>
</tr>
<tr class="even">
<td><span class="math inline">\(MPQA\)</span></td>
<td>英语: 新闻语料</td>
<td>4500</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(DS_{Unis}\)</span></td>
<td>英语:网课和电子商务评论语料</td>
<td>2253</td>
</tr>
<tr class="even">
<td><span class="math inline">\(OpeNER\_es\)</span></td>
<td>西班牙语:</td>
<td>1439</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(OpeNER\_en\)</span></td>
<td>英语:</td>
<td>1745</td>
</tr>
</tbody>
</table>
<h1 id="评估方法">评估方法</h1>
<p><strong>Sentiment Graph F1</strong></p>
<p>对元组级别(holder, target, expression,
polarity)的记录进行对比，在polarity一致的情况下，在holder/target/expression三种元素上，利用预测元素和真实元素的字符重叠情况作为对应元素的准确情况，并取三个元素准确情况的平均值作为元组的准确情况。
<span class="math display">\[
tp_{holder} = \frac{intersect(holder_{predict},
holder_{true})}{holder_{predict}}
\]</span> <span class="math display">\[
tp_{target} = \frac{intersect(target_{predict},
target_{true})}{target_{predict}}
\]</span> <span class="math display">\[
tp_{expression} = \frac{intersect(expression_{predict},
expression_{true})}{expression_{predict}}
\]</span> <span class="math display">\[
tp = \frac{tp_{holder} + tp_{target} + tp_{expression}}{3}
\]</span></p>
<p>准确率<span class="math inline">\(P\)</span>:
所有预测准确的元素占预测元素的比例</p>
<p>召回率<span
class="math inline">\(R\)</span>：所有预测准确的元素占真实元素的比例</p>
<p>Sentiment Graph F1： <span class="math display">\[
F_1 = \frac{2\times P \times R}{P + R}
\]</span></p>
<h1 id="baseline">Baseline</h1>
<table>
<colgroup>
<col style="width: 42%" />
<col style="width: 11%" />
<col style="width: 4%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 4%" />
<col style="width: 7%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>darmstadt_unis</th>
<th>mpqa</th>
<th>multibooked_ca</th>
<th>multibooked_eu</th>
<th>norec</th>
<th>opener_en</th>
<th>opener_es</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pipeline sequence labeling + relation classification</td>
<td>0.072</td>
<td>0.011</td>
<td>0.289</td>
<td>0.405</td>
<td>0.201</td>
<td>0.254</td>
<td></td>
</tr>
<tr class="even">
<td>graph_parser</td>
<td>0</td>
<td>0</td>
<td>0.038</td>
<td>0.065</td>
<td>0.011</td>
<td>0.098</td>
<td>0.068</td>
</tr>
</tbody>
</table>
<h2 id="pipeline-sequence-labeling-relation-classification">Pipeline
Sequence labeling + relation classification</h2>
<p>任务分成两部分: 元素抽取(holder/target/expression) 、情感极性分类</p>
<ul>
<li>元素抽取：训练三个BiLSTM模型进行
holder、target、expression的抽取</li>
<li>情感极性分类：BiLSTM + max
pooling获取三个表示向量，将三个向量拼接后接一个线性分类器进行polarity预测。
<ul>
<li>全文向量表示</li>
<li>holder或者target的向量表示</li>
<li>expression的向量表示</li>
</ul></li>
</ul>
<h2 id="graph-parser">Graph Parser</h2>
<p>看做是一个双词依存图的预测任务( a bilexical dependency graph
prediction task）。</p>
<figure>
<img src="bilexical.png" alt="bilexical" />
<figcaption aria-hidden="true">bilexical</figcaption>
</figure>
<p>模型paper</p>
<ul>
<li><a href="https://aclanthology.org/2021.acl-long.263/">Structured
Sentiment Analysis as Dependency Graph Parsing</a>.</li>
<li><a href="https://aclanthology.org/2020.iwpt-1.3/">End-to-End
Negation Resolution as Graph Parsing</a>.</li>
</ul>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义模型的保存与加载</title>
    <url>/2021/11/28/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
    <content><![CDATA[<p>TensorFlow2中自定义Model/Layer的保存与加载。</p>
<span id="more"></span>
<p>保存/加载带有自定义层的模型或子类化模型分成两步：</p>
<ol type="1">
<li><p>您应该重写 <code>get_config</code> 和
<code>from_config</code>（可选）方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;将返回一个包含模型配置的 Python 字典。&#x27;&#x27;&#x27;</span></span><br><span class="line">    config = <span class="built_in">super</span>(Attention, self).get_config()</span><br><span class="line">    config.update(&#123;</span><br><span class="line">        <span class="string">&#x27;use_W&#x27;</span>: self.use_W,</span><br><span class="line">        <span class="string">&#x27;return_self_attend&#x27;</span>: self.return_self_attend,</span><br><span class="line">        <span class="string">&#x27;return_attend_weight&#x27;</span>: self.return_attend_weight</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure></li>
<li><p>您应该注册自定义对象，以便 Keras 能够感知它</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">custom_objects = &#123;<span class="string">&#x27;Attention&#x27;</span>: Attention&#125;</span><br><span class="line">self.model = tf.keras.models.load_model(self.config.checkpoint_file, custom_objects=custom_objects)</span><br><span class="line"><span class="built_in">print</span>(self.model.summary())</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>load_model</tag>
        <tag>save_model</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2021/12/01/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>激活函数为神经网络引入了非线性，增强了神经网络的表达能力。</p>
<span id="more"></span>
<h1 id="sigmoid">sigmoid</h1>
<p>函数表达式: <span class="math display">\[
f(z) = \frac{1}{1+e^{-z}}
\]</span> 函数曲线:</p>
<figure>
<img src="sigmoid.png" alt="sigmoid" />
<figcaption aria-hidden="true">sigmoid</figcaption>
</figure>
<p>优点:</p>
<ol type="1">
<li>输出范围是0到1，因此，对每个神经元的输出进行了归一化。</li>
<li>用于预测概率作为输出的模型</li>
<li>梯度平滑，不会出现【跳跃】的输出值。</li>
</ol>
<p>缺点:</p>
<ol type="1">
<li>梯度消失:当函数的输出不是0附近时，会降低权重更新效率。</li>
<li>输出总是为正，随着层数的增加，样本的分布会从0-1高斯分布偏移至sigmoid的饱和区域，导致反向传播很难进行，收敛速度较慢。而batch-normalization会把样本强行拉回0-1高斯分布</li>
</ol>
<h1 id="tanh">tanh</h1>
<p>函数表达式: <span class="math display">\[
f(x) = tanh(x) = \frac{2}{1+e^{-2x}}-1
\]</span> 函数曲线:</p>
<figure>
<img src="tanh.png" alt="tanh" />
<figcaption aria-hidden="true">tanh</figcaption>
</figure>
<p>与sigmoid函数的对比:</p>
<figure>
<img src="sigmoid_tanh.png" alt="tanh and sigmoid" />
<figcaption aria-hidden="true">tanh and sigmoid</figcaption>
</figure>
<p>优点:</p>
<ol type="1">
<li>tanh相较于sigmoid函数的优点在于:中心对称，均值为0，能将0-1高斯分布依然映射到0附近的分布，保持零均值特性，所以，收敛速度较sigmoid快一些。</li>
</ol>
<p>缺点:</p>
<ol type="1">
<li>梯度消失:当输入较大或较小时，梯度较小，不利于权重更新</li>
</ol>
<p>在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid
函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p>
<h1 id="relu">relu</h1>
<p>函数表达式: <span class="math display">\[
\sigma(x) = \
\begin{cases}
max(0,x)&amp;, x&gt;=0
\\
0 &amp;, x&lt;0
\end{cases}
\]</span> 函数曲线:</p>
<figure>
<img src="relu.png" alt="relu" />
<figcaption aria-hidden="true">relu</figcaption>
</figure>
<p>优点:</p>
<ol type="1">
<li>当输入为正时，不存在梯度饱和问题；</li>
<li>计算速度快，relu函数中只存在线性关系，因此计算速度比sigmoid和tanh快。</li>
</ol>
<p>缺点:</p>
<ol type="1">
<li>当输入为负时，relu完全失效，在反向传播过程中，如果输入是负数，则梯度完全为0。</li>
</ol>
<h1 id="softmax">softmax</h1>
<p>函数是用于多分类问题的激活函数，对于长度为 K 的任意实向量，Softmax
可以将其压缩为长度为 K，值在（0，1）范围内，并且向量中元素的总和为 1
的实向量。</p>
<p>函数表达式： <span class="math display">\[
f(x_i)=\frac{e^{x_i}}{\sum_{j=1}^{K}{e^{x_j}}}
\]</span></p>
<p>函数曲线:</p>
<figure>
<img src="softmax.png" alt="softmax" />
<figcaption aria-hidden="true">softmax</figcaption>
</figure>
<p>优点:</p>
<ol type="1">
<li>引入指数形式:能够将差距大的数值距离拉的更大</li>
</ol>
<p>缺点:</p>
<ol type="1">
<li>指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当
<span
class="math inline">\(x_i\)</span>值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。</li>
</ol>
<h1 id="gelu">gelu</h1>
<p>对于每一个输入 <span
class="math inline">\(x\)</span>，其服从于标准正态分布 <span
class="math inline">\(N(0, 1)\)</span>，它会乘上一个伯努利分布 <span
class="math inline">\(Bernoulli(Φ(x))\)</span>，其中<span
class="math inline">\(Φ(x) = P(X ≤ x)\)</span>。<strong>随着 x
的降低，它被归零的概率会升高</strong>。</p>
<p>函数表达式: <span class="math display">\[
GELU(x) = xP(X \le x) = xΦ(x)
\]</span> 函数曲线:</p>
<figure>
<img src="gelu.png" alt="gelu" />
<figcaption aria-hidden="true">gelu</figcaption>
</figure>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>activation function</tag>
      </tags>
  </entry>
  <entry>
    <title>高维矩阵乘法</title>
    <url>/2021/12/08/%E9%AB%98%E7%BB%B4%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/</url>
    <content><![CDATA[<p>NLP中经常为碰到高维矩阵运算，如Attention中的Q、K、V相乘，在此记录矩阵相乘的运算规则。</p>
<span id="more"></span>
<h1 id="高维矩阵可视化">高维矩阵可视化</h1>
<p><strong>一维:</strong>
首先shape=[4]的一维矩阵非常简单，可以用下图表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<figure>
<img src="yiwei.jpg" alt="一维矩阵" />
<figcaption aria-hidden="true">一维矩阵</figcaption>
</figure>
<p><strong>二维:</strong>shape=[2,3]的二维矩阵可视化如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
<figure>
<img src="erwei.jpg" alt="二维" />
<figcaption aria-hidden="true">二维</figcaption>
</figure>
<p>为方便展示三维矩阵，旋转角度如下:</p>
<figure>
<img src="erwei2.jpg" alt="二维" />
<figcaption aria-hidden="true">二维</figcaption>
</figure>
<p><strong>三维:</strong>一个shape=[2,2,3]的三维矩阵，可视化如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">  [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line"> </span><br><span class="line"> [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],</span><br><span class="line">  [<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]]</span><br></pre></td></tr></table></figure>
<figure>
<img src="sanwei.jpg" alt="三维" />
<figcaption aria-hidden="true">三维</figcaption>
</figure>
<p>切片展示如下:</p>
<figure>
<img src="sanwei2.jpg" alt="三维" />
<figcaption aria-hidden="true">三维</figcaption>
</figure>
<p><strong>四维:</strong>shape=[2,2,2,3]的四维矩阵可视化如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">  </span><br><span class="line">  [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],</span><br><span class="line">   [<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]],</span><br><span class="line">  </span><br><span class="line">  [[[<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>]],</span><br><span class="line">   </span><br><span class="line">  [[<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>],</span><br><span class="line">  [<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>]]]]</span><br></pre></td></tr></table></figure>
<figure>
<img src="siwei.jpg" alt="四维" />
<figcaption aria-hidden="true">四维</figcaption>
</figure>
<h1 id="高维矩阵运算">高维矩阵运算</h1>
<p>从上面的结论可以看出:<strong>所有大于二维的，最终都是以二维为基础堆叠在一起的！！</strong></p>
<p>所以在矩阵运算的时候，其实最后都可以转成我们常见的二维矩阵运算，遵循的原则是：<strong>在多维矩阵相乘中，需最后两维满足shape匹配原则，最后两维才是有数据的矩阵，前面的维度只是矩阵的排列而已！</strong></p>
<p><strong>相乘必须满足以下两个条件：</strong></p>
<ol type="1">
<li>两个n维数组的前n-2维必须完全相同。例如（3,2,4,2）（3,2,2,3）前两维必须完全一致；</li>
<li>最后两维必须满足二阶矩阵乘法要求。例如（3,2,4,2）（3,2,2,3）的后两维可视为（4,2）x（2,3）满足矩阵乘法。</li>
</ol>
<p>另，由于广播机制，第一维为1的，可以与第一维任何数相乘：</p>
<p>（3,2,4,2）*（1,2,2,3）——&gt;&gt;（3,2,4,3）</p>
<p>（1,2,4,2）*（3,2,2,3）——&gt;&gt;（3,2,4,3）</p>
<p>比如两个三维的矩阵相乘，分别为shape=[2,2,3]和shape=[2,3,2]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = </span><br><span class="line">[[[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>]</span><br><span class="line">  [ <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line">  [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]]</span><br><span class="line"></span><br><span class="line">b = </span><br><span class="line">[[[ <span class="number">1.</span>  <span class="number">2.</span>]</span><br><span class="line">  [ <span class="number">3.</span>  <span class="number">4.</span>]</span><br><span class="line">  [ <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>]</span><br><span class="line">  [ <span class="number">9.</span> <span class="number">10.</span>]</span><br><span class="line">  [<span class="number">11.</span> <span class="number">12.</span>]]]</span><br></pre></td></tr></table></figure>
<p>计算的时候把a的第一个shape=[2,3]的矩阵和b的第一个shape=[3,2]的矩阵相乘，得到的shape=[2,2]，即</p>
<figure>
<img src="matmul1.jpg" alt="matmul1" />
<figcaption aria-hidden="true">matmul1</figcaption>
</figure>
<p>同理，再把a，b个字的第二个shape=[2,3]的矩阵相乘，得到的shape=[2,2]。</p>
<figure>
<img src="matmul2.jpg" alt="matmul2" />
<figcaption aria-hidden="true">matmul2</figcaption>
</figure>
<p>最终把结果堆叠在一起，就是2个shape=[2,2]的矩阵堆叠在一起，结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line">  [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">220.</span> <span class="number">244.</span>]</span><br><span class="line">  [<span class="number">301.</span> <span class="number">334.</span>]]]</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<blockquote>
<ol type="1">
<li><a
href="https://zhuanlan.zhihu.com/p/337829793">【全面理解多维矩阵运算】多维（三维四维）矩阵向量运算-超强可视化</a></li>
<li><a
href="https://blog.csdn.net/weixin_45459911/article/details/107852351">高维数组相乘的运算规则</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>tensor</tag>
        <tag>numpy</tag>
        <tag>matmul</tag>
      </tags>
  </entry>
  <entry>
    <title>Unified Language Model Pre-training</title>
    <url>/2023/03/30/Unified-Language-Model-Pre-training/</url>
    <content><![CDATA[<figure>
<img src="UniLM.png" alt="UniLM" />
<figcaption aria-hidden="true">UniLM</figcaption>
</figure>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>unlm</tag>
      </tags>
  </entry>
  <entry>
    <title>cmder配置</title>
    <url>/2023/06/26/cmder%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录cmder的一些配置情况！</p>
<span id="more"></span>
<h1 id="配置环境变量">配置环境变量</h1>
<p>设置以下环境变量： -
<code>CMDER_ROOT=D:\ProgramData\cmder_mini</code> -
<code>PATH=%CMDER_ROOT%</code></p>
<h1 id="添加到右键菜单">添加到右键菜单</h1>
<p>以管理员身份打开cmd</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> %CMDER_ROOT%</span><br><span class="line">Cmder.exe /REGISTER ALL</span><br></pre></td></tr></table></figure>
<h1 id="解决中文乱码">解决中文乱码</h1>
<p>在设置中找到<code>startup -&gt; Environment</code>，追加以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> PATH=%ConEmuBaseDir%\Scripts;%PATH%</span><br><span class="line"><span class="built_in">set</span> LANG=zh_CN.UTF-8</span><br><span class="line"><span class="built_in">set</span> LC_ALL=zh_CN.utf8</span><br><span class="line">chcp utf-8</span><br></pre></td></tr></table></figure>
<h1 id="修改提示符符号">修改提示符符号</h1>
<p>Cmder 中的提示符符号默认为
λ，可能会导致某些bug，修改提示符时，需要修改
<code>%CMDER_ROOT%\config\cmder_prompt_config.lua</code>，搜索<code>λ</code>，替换成<code>$</code></p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">prompt_lambSymbol = <span class="string">&quot;$&quot;</span></span><br><span class="line">prompt_singleLine = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h1 id="添加自定义命令">添加自定义命令</h1>
<p>修改<code>%CMDER_ROOT%\config\user-aliases.cmd</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ll=ls -alF --show-control-chars</span><br></pre></td></tr></table></figure>
<h1 id="防止字体重叠">防止字体重叠</h1>
<p>去掉<code>Setting -&gt; Main -&gt; Fonts</code>中的<code>Monospace</code>勾选项</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>cmder</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>同时配置两个github账户</title>
    <url>/2023/06/26/%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AE%E4%B8%A4%E4%B8%AAgithub%E8%B4%A6%E6%88%B7/</url>
    <content><![CDATA[<p>配置两个github账号，使用时互不干扰</p>
<h1 id="生成两个ssh-key">生成两个SSH key</h1>
<p>生成两个key的具体命令： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t ed25519 -C <span class="string">&quot;one@gmail.com&quot;</span></span><br><span class="line">ssh-keygen -t ed25519 -C <span class="string">&quot;two@gmail.com&quot;</span></span><br></pre></td></tr></table></figure>
注意在输入文件名时将两个文件进行区分</p>
<h1 id="创建config文件并配置">创建config文件并配置</h1>
<p>继续在 ​​<code>.ssh</code>​​ 目录下创建 <code>config</code> 文件，在
<code>config</code> 文件中添加以下内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># one(one@gmail.com)</span></span><br><span class="line">Host one.github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa_one</span><br><span class="line">User one</span><br><span class="line">    </span><br><span class="line"><span class="comment"># two(two@gmail.com)</span></span><br><span class="line">Host two.github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa_two</span><br><span class="line">User two</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
</search>
